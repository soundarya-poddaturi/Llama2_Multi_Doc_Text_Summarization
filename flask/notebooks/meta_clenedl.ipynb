{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wptzjSY9Cq5Y",
        "outputId": "e58f2fe4-b3f5-4bb1-ab21-ae37fe9912ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW9XSdx57kFx",
        "outputId": "aa77eeda-2ff4-4ad3-9bab-bb9cb9d9cc37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login --token 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"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBnK4mR0C9yY",
        "outputId": "5501b219-3ca5-497c-d1ab-65f2c3702b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "1fd3f0531553439980b28c41ddb12bd6",
            "927a2735ea124d329c940928fe9036cd",
            "259068f072bd44ecadcbf9f5dd2caa38",
            "ecb2939ca02b496bac01bfed46f093e6",
            "0eac531a8d76444ea6080cf431c95c70",
            "592b84c8aee24de0ae9415a35bf4b2c3",
            "b1af8f61aea547c2bfd4409b6e6ca774",
            "b612e4df05d34f8a8777de1df96385e2",
            "f8e9b59c0852462b9189700249b7f53a",
            "bc565c6e32a445d2955aecdba3aadad8",
            "37f4c5b6a30f40d69d330f5981ca24ec",
            "995f8c3da9a44cd7bbc12552c8dfffda",
            "5ab4a0e86caa4b989e36afc4f58262f6",
            "bc3dc2eecafb4f4c82258db7483f2fd5",
            "5c24226a6f1f4f9a94f62f42abfccf5f",
            "5eabf59e514849aaa89e645ba831df1c",
            "f8e8d1e0b5854ffeabb9cc84f6e1f42e",
            "e117cce7b8a1414fb21c7bb0174ce546",
            "341e2e7960ca42978f36f7e6f7e2ba0e",
            "b83381e6efc34f35ae16743ea58621cc",
            "8700e39b542343ab8dec520f85851962",
            "4a8abe70e81b41b6b906d7f589c96e70",
            "c96d70c291874ca58794ae248e9ed5c2",
            "453a3077a6424e36b24bc5392c5bda55",
            "fa71edfddd2248ae911649a17d52440b",
            "166215b9186b48c7bc2e81633034eb07",
            "443b7a09cff64df1908b45aa7db424bd",
            "dc54cb4131be4e00a93da43cc2347e5d",
            "1b70cc83eb444f70b72383181702720b",
            "f9564b2cfe214f19a97aecab114ebb5a",
            "fef37280e7f74fcdbec5610d604a0d09",
            "41f934488f864bfc888dfd2e3dbaf63d",
            "401cd6c92be449b6a4ca9d3896a120bc",
            "3fd468616b23429dac88ad2364621eb6",
            "75ff2d0080954489acf1eb0f0bd15287",
            "e403799d08624226b7d4ffd4daff5872",
            "81834d69630d459cb63cb15e3df59c3b",
            "5334617c720a41848c29a2ca6a306267",
            "2436c5504d7a48ec9c956ee4f853d0c4",
            "54acc001f90a4396940365689f3073f4",
            "d90ee37a3be9444eaca328ca34660733",
            "fc30da70bac8455ca0763df819834119",
            "348fff0007cf4a4d9d79b4042280fc93",
            "4ba75d56b0b64550b45ddb887f1e7487",
            "fd927eb564434ee1876df34dd55d9155",
            "ff7a85e1262b4ae1a998c63db51467d9",
            "63373723871c4a779f828a072a8a7a1c",
            "1398db51216d4c0694e5c8f5605cf821",
            "c8bac774c5864d1babebaa4903acdd68",
            "0485883bf3734f76b6d74cfd394a839d",
            "7c9798b46b434698b110bdca48e0a679",
            "f6a53b9945064e9386cf4a9b43b6bde0",
            "72f06ba39b1b4646849799523603c35b",
            "2f1ac6bad7e2425cb91a86255bb9c973",
            "86f57bc710464b1f980e41525d3b7602",
            "f4534d0465de4078a5f9345295784661",
            "beb6c5a35bfe4ef5a130372c8b87ca6a",
            "36d796832ca14bc4ab65d5e6c6d7718a",
            "5a2be7be86b342329b06f945d2f0c16b",
            "69d5be3efd434cf3bc831379102525a5",
            "684d428d881142ae86d719d473f97079",
            "92a23b4920084407af1decd6ff0fc8a2",
            "1e1ca2e880ca48338f43b398d7b7fe75",
            "6863be341b224208911150937cf6d49b",
            "25809a66c2e6451bae6a41c0e79589ca",
            "4800262b66bf4211ba9dbe6b63a1fb85",
            "a63067e77e1f4b9085d5643324bf4188",
            "e8ad9c374727475996a71894a0ac1bcd",
            "6f2f10a55a9148b89c05247dd171d430",
            "00c6b66c662747d58915fcfbf73ecc28",
            "e389376124014ea991f961013e720342",
            "7d2eccb895bf423898000a429bb22829",
            "2c7b7026e9fa4b3ab605b9a69492b795",
            "4bd6322456434347a1fbf28c0eb449db",
            "bc33eaa3b3d64cd38280ac4a12236945",
            "65d30454942d407196207f7047d067a3",
            "ca2807f3b1b9479396ddc103fb9f0817",
            "afab86c6094043b6ad0caba746274a3f",
            "a75fbdfb58014986b5541370e1eeb341",
            "31d55b73c00346048c15b13b553aa004",
            "a94e33aa900042d39ec3f1ad1f31fd60",
            "440f71e8aba644718bbf5a9d36b2dfa9",
            "40d54a26f1184bd3bbf4a4610ce9c5f4",
            "b82ebd31ad50448f8ed9ee69790bbbea",
            "386d337704554263bf5a6e6c6ef9040b",
            "a881c3671f8e4da1872ded5855d259f5",
            "66ea718ce1b04801964b40b1c9ed5f67",
            "865d00ea545749cdaa75b8dc51ebce01",
            "7e495eb59998460da7c11313fd7308d6",
            "17b1f4e2ca844c8098976d89efc73ccb",
            "e42bdcbe90954392802f25a8ebe49488",
            "f17d7d344987445e9ea416f678081f01",
            "0e0838df15cf45c7833d673d29005d92",
            "0f406c11440947ab9d61ef69edcce4ae",
            "6079e41ab2f84152914b6705288758b1",
            "a9f0f228532e4d7c814f43b93150740b",
            "85e82f01bb4646049f18cfb40ae2f8c0",
            "d07513b491e340fe9f173dbed9bf109f",
            "8f564c9cb8c741ff867fd66a916c5d4c",
            "a4edbe7102604a33a314989daef49c3f",
            "a864278de4af450d8ba5f88e35bc4cff",
            "bfc2ae82f1e64aa1bf53e10a65ddecec",
            "5aaf0d68a737490b8580034fbb539e7a",
            "e659a998dc394d35bbf62c5220c9ae43",
            "d88260045e6a450e987247c2e82f63d9",
            "acc0e3b453454610be3b097e01b5a930",
            "ca9b4b60af6a49bd85f68379a2672535",
            "9b8ffe7803b0458dbbdc2ec1c57049d2",
            "08fc40c0f03243cab947a7cef7eff9bc",
            "bd54092c9f0c4878b58884079d0c590e",
            "0da90438c34340d9bc244432a263b775",
            "f5b04591614a45baa61c99fffeaee9ea",
            "1c5256ca7bdf49db9d319630deb8afe6",
            "f9c756dcb961421281b5fbda0dcf710c",
            "010d425aeacf48958321d247b012b9e5",
            "01f6627d81b34cc490be8715a87b6052",
            "c682b70874c641b0aeacc875d77f52c5",
            "08f543a57dc2485aa06b90d6c7916fd1",
            "4f8496d18f3b4d20bd1a506dc5286f1a",
            "ed2b2dc791264b959471f3611010efd4",
            "6e12c540d1eb483ba34688727431d2ff"
          ]
        },
        "id": "SNu9GNwlCr1R",
        "outputId": "18255f8b-7ea7-4916-b75c-d1e9bd9cb429"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fd3f0531553439980b28c41ddb12bd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "995f8c3da9a44cd7bbc12552c8dfffda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c96d70c291874ca58794ae248e9ed5c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fd468616b23429dac88ad2364621eb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd927eb564434ee1876df34dd55d9155",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f4534d0465de4078a5f9345295784661",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a63067e77e1f4b9085d5643324bf4188",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afab86c6094043b6ad0caba746274a3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e495eb59998460da7c11313fd7308d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4edbe7102604a33a314989daef49c3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0da90438c34340d9bc244432a263b775",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "model1 = \"\"\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "pipeline = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer, # text is first divided into the smallest units, and these\n",
        "                 # smallest units are called tokens.\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 512,#means that the model will generate a maximum of 512 new tokens as output, \n",
        "                #regardless of the length of the input text.\n",
        "                do_sample=True,#controlling the creativity of generated text\n",
        "                top_k=30, #Number of tokens to consider when sampling from the output distribution of \n",
        "                #the model. If top_k=10, the pipeline will only consider the top 10 most likely tokens when sampling. \n",
        "                num_return_sequences=1,\n",
        "                #num_return_sequences=10 10 different ways of same context is implemented\n",
        "                # use the end-of-sequence token (EOS) token from the tokenizer to indicate the end of a \n",
        "                #generated text sequence.\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LL7JGQ5iCzIy"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})\n",
        "#, a pipeline refers to a series of steps or processes that are applied sequentially to perform a specific task or generate desired outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OQT5L5DQMEaB"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rdEbOPmYSysU"
      },
      "outputs": [],
      "source": [
        "# text='''\n",
        "# \"computing\" refers to the process of using computers and computer systems to perform various\n",
        "# tasks, such as data processing, information storage, and solving complex problems .\n",
        "# this computing can be either done in serial way known as serial computing or in parallel wa y\n",
        "# known as parallel computing\n",
        "# serial computing:\n",
        "# serial computing refers to traditional computing where tasks are executed sequentially, one after\n",
        "# the other, using a single processor. in serial computing, each instruction or task must wait for the\n",
        "# previous one to complete before it can be executed. this approach limits the speed and efficiency\n",
        "# of processing, especially when dealing with complex or time -consuming tasks. computing\n",
        "# serial or\n",
        "# sequential parallel example of serial computing : consider a task of sorting a large dataset of numbers in ascending\n",
        "# order. in a serial computing environment, a single processor would go through the entire dataset,\n",
        "# comparing and rearranging numbers one pair at a time until the entire dataset is sorted. this process\n",
        "# occurs sequentially, and each comparison and rearrangement must wait for the previous one to\n",
        "# finish.\n",
        "# parallel computing\n",
        "# parallel computing is a type of computation in which multiple processors or computers work together\n",
        "# to solve a problem. instead of one single processor handling the entire task, parallel computing divides\n",
        "# the task into smaller sub -tasks that can be processed simultaneously. this simultaneous processing\n",
        "# can lead to significant improvements in computational speed and efficien cy.\n",
        "# example of parallel computing: using the same example of sorting a large dataset, parallel computing\n",
        "# would involve dividing the dataset into smaller chunks, and each chunk is sorted independently by a\n",
        "# separate processor. these processors work in parall el, sorting their respective chunks simultaneously.\n",
        "# once all processors have completed sorting their portions, the sorted chunks can be combined to\n",
        "# produce the final sorted dataset.\n",
        "# definition parallel computing is the practice of identifying and exposing parallelism in algorithms,\n",
        "# expressing this in our software, and understanding the costs, benefits, and limitations of the chosen\n",
        "# implementation.\n",
        "# benefits of parallel computing\n",
        "\n",
        "#  faster run time with more compute cores: parallelization involves dividing a task into\n",
        "# smaller sub -tasks that can be executed simultaneously, utilizing multiple cores to process the\n",
        "# data. this approach can significantly reduce the time required to complete the task, as each\n",
        "# core works on a separate portion of the problem concurrently.\n",
        "#  larger problem sizes with more compute nodes : with more nodes, you can break down your\n",
        "# problem into smaller pieces that each node can work on simultaneously, which is especially\n",
        "# beneficial for handling larger datasets and more complex simulations .\n",
        "#  energy efficiency by doing more with less: in the context of parallel computing, the concept\n",
        "# of \"doing more with less\" often revolves around optimizing energy efficiency while achieving\n",
        "# better computational performance.this can be achieved by making use of dynamic resource\n",
        "# allocation and workload consolidation to ensure that the number of processors used is\n",
        "# proportional to the workload. turn off or put to sleep any unused processors.\n",
        "# the energy consumption for your applicat ion can be estimated using the formula\n",
        "# p = (n processors) × (r watts/processors) × (t hours)\n",
        "# where p is the energy consumption, n is the number of processors, r is the thermal design\n",
        "# power, and t is the application run time.\n",
        "\n",
        "#  scalability : parallel computing can be easily scaled by adding more processors, which further\n",
        "# enhances performance. serial computing does not scale in this manner, as it relies on a single\n",
        "# processor.\n",
        "#  parallel computing can reduce costs: as technology advances, the cost of individual\n",
        "# processors and memory decreases. parallel computing systems can take advantage of these\n",
        "# cost reductions, making it more economical to build high -performance computing clusters or\n",
        "# data centers .\n",
        "# applications of parallel computing:\n",
        "#  scientific simulations: used in fields such as physics, chemistry, and engineering for\n",
        "# complex simulations.\n",
        "#  big data processing: parallel computing is crucial in processing vast amounts of data in\n",
        "# fields like data analytics and machine learning.\n",
        "#  weather forecasting: enables complex weather simulations and predictions.\n",
        "#  video and image processing: parallelism accelerates tasks like video rendering and\n",
        "# image recognition.\n",
        "#  financial modelling : used for risk analysis, option pricing, and other complex financial\n",
        "# calculations.\n",
        "# fundamental laws\n",
        "# fundamental laws in parallel computing, such as amdahl's law and gustafson's law, are essential\n",
        "# for understanding the limitations and possibilities of parallel processing. these laws provide valuable\n",
        "# insights into how the speedup of a parallel algorithm is affected by various factors .\n",
        "# what is speedup?\n",
        "# speedup in parallel computing refers to the performance improvement achieved by using multiple\n",
        "# processors or computing resources to solve a problem compared to using a single processor. it is a\n",
        "# measure of how much faster a parallel algorithm or system can complet e a task compared to a serial\n",
        "# (single -processor) implementation of the same task. speedup is a crucial metric for evaluating the\n",
        "# effectiveness of parallel computing systems.\n",
        "# the speedup ( s) can be calculated using the following formula:\n",
        "# s=tserial / tparallel\n",
        "# where:\n",
        "#  tserial is the execution time of the task using a single processor (serial execution time).\n",
        "# tparallel is the execution time of the task using multiple processors (parallel execution time).\n",
        "# a speedup value greater than 1 indicates that the parallel implementation is faster than the serial\n",
        "# implementation. ideally, in a perfectly parallelizable task, doubling the number of processors would ideally halve the execution time, resulting in a speedup of 2. however, achieving perfect linear\n",
        "# spee dup is rare in real -world scenarios due to factors such as communication overhead, load\n",
        "# balancing issues, and synchronization constraints between processors.\n",
        "\n",
        "# amdahl's law is a fundamental principle in parallel computing that expresses the potential\n",
        "# speedup of a parallel algorithm as a function of the proportion of the algorithm that can be\n",
        "# parallelized. it was formulated by gene amdahl in 1967 and is represented by the fol lowing\n",
        "# formula:\n",
        "# where:\n",
        "#  speedup is the improvement in performance achieved by parallelizing a computation\n",
        "# compared to executing it sequentially.\n",
        "#  p is the proportion of the algorithm that can be parallelized (a value between 0 and 1).\n",
        "#  s is the serial fraction\n",
        "#  n – no.of processors/nodes/cores\n",
        "# amdahl's law highlights the limitations of parallel computing. it states that the speedup of a\n",
        "# program using multiple processors in parallel computing is limited by the sequential fraction of\n",
        "# the program. in other words, if only a portion of a program can be parallelized (the rest being\n",
        "# inherently sequential), then no matter how many processors are added, there will always be a\n",
        "# limit to the speedup that can be achieved.\n",
        "# for example, if 90% of a program can be parall elized (p = 0.9) and the parallel portion runs on\n",
        "# 5 processors , the maximum speedup that can be achieved according to amdahl's law is:\n",
        "# speedup= 1/(0.1+(0.9/5))=3.57\n",
        "# in this case, even though 90% of the program can be parallelized and runs on 5 processors , the\n",
        "# maximum speedup achievable is approximately 3.57 times faster compared to the sequential\n",
        "# execution due to the presence of the 10% sequential portion.\n",
        "# fig : speedup for a fixed -size problem according to amdahl’s law is shown as a function of the\n",
        "# number of processors. lines show ideal speedup when 100% of an algorithm is parallelized,\n",
        "# and for 90%, 75%, and 50%. amdahl’s law states that speedup is limited by the fractions of\n",
        "# code that remain serial.\n",
        "# gustafson's law , formulated by john l. gustafson, provides a different perspective on parallel\n",
        "# computing compared to amdahl's law. unlike amdahl's law, which focuses on fixed problem sizes,\n",
        "# gustafson's law takes into account varying problem sizes. the basic idea behind g ustafson's law is\n",
        "# that as the size of the problem increases, the impact of the parallelizable portion of the program\n",
        "# becomes more significant, leading to better scalability. in other words, with larger problem sizes,\n",
        "# parallel systems can achieve higher lev els of speedup.\n",
        "# the formula for gustafson's law is as follows:\n",
        "# speedup(n) = n – s * (n – 1) where n is the number of processors, and s is the serial fraction\n",
        "# strong scaling and weak scaling are two different m etrics used to evaluate the performance of\n",
        "# parallel computing systems, and they provide insights into how well a parallel algorithm or\n",
        "# application can handle an increasing workload or an increasing number of processors. here's a\n",
        "# comparison of strong scalin g and weak scaling:\n",
        "# strong scaling:\n",
        "# definition: strong scaling measures how the execution time of a fixed problem size decreases\n",
        "# as the number of processors increases. in other words, it assesses how well a parallel system\n",
        "# performs when the size of the problem remains constant, but the number of proces sors used\n",
        "# to solve the problem increases.\n",
        "# objective: the goal of strong scaling is to reduce the execution time for a fixed problem size by\n",
        "# utilizing more processors. it aims to speed up the solution of a specific problem.\n",
        "# scenario: strong scaling is applicable when the size of the problem is fixed, and the aim is to\n",
        "# solve that problem faster by employing additional processors.\n",
        "\n",
        "# 2. weak scaling:\n",
        "# definition: weak scaling measures how the execution time changes as both the problem size\n",
        "# and the number of processors increase proportionally. in other words, it assesses how well a\n",
        "# parallel system can handle larger workloads by adding more processors as the problem size\n",
        "# grows.\n",
        "# objective: the goal of weak scaling is to maintain a constant workload per processor as the size\n",
        "# of the problem and the number of processors increase. it aims to solve larger problems in\n",
        "# approximately the same amount of time per processor.\n",
        "# scenario: weak scaling is applicable when the problem size can be increased, and the aim is to\n",
        "# handle larger workloads by distributing the computational load across a larger number of\n",
        "# processors.\n",
        "# parallel approaches (flynn’s classification)\n",
        "\n",
        "# flynn's classification is essential in the field of parallel computing because it provides a framework\n",
        "# for understanding and categorizing different types of computer architectures based on the number\n",
        "# of instruction streams and data streams. this classifica tion is named after michael j. flynn, who\n",
        "# introduced it in 1966. flynn’s taxonomy is a useful tool for understanding different types of\n",
        "# computer architectures and their strengths and weaknesses.\n",
        "\n",
        "# the taxonomy highlights the importance of parallelism in modern computing and shows how\n",
        "# different types of parallelism can be exploited to improve performance. it helps in designing and\n",
        "# analyzing parallel processing systems\n",
        "\n",
        "# 1. single instruction single data (sisd ): in a sisd architecture, there is a single processor\n",
        "# that executes a single instruction stream and operates on a single data stream. this is the\n",
        "# simplest type of computer architecture and is used in most traditional computers.\n",
        "# 2. single instruction multiple data (simd ): in a simd architecture, there is a single\n",
        "# processor that executes the same instruction on multiple data streams in parallel. this type of\n",
        "# architecture is used in applications such as image and signal processing.\n",
        "\n",
        "# 3. multiple instruction single data (misd ): in a misd architecture, multiple processors\n",
        "# execute different instructions on the same data stream. this type of architecture is not\n",
        "# commonly used in practice, as it is difficult to find applications that can be decomposed into\n",
        "# indepen dent instruction streams.\n",
        "\n",
        "# 4. multiple instruction multiple data (mimd ): in a mimd architecture, multiple processors\n",
        "# execute different instructions on different data streams. this type of architecture is used in\n",
        "# distributed computing, parallel processing, and other high -performance computing applications.\n",
        "# parallel strategies\n",
        "# parallel strategies\" typically refer to techniques and methods for parallel processing, which is the\n",
        "# simultaneous execution of multiple tasks or processes to improve the efficiency and performance\n",
        "# of a computer system. parallel strategies are commonly used in various computing domains, such\n",
        "# as high -performance computing and distributed systems, to speed up computations and handle\n",
        "# large volumes of data. here are some common parallel s trategies:\n",
        "# data parallel approach\n",
        "# data parallelism involves performing the same operation on multiple data elements simultaneously.\n",
        "# this strategy is often used in applications where the same operation can be applied to different\n",
        "# pieces of data independent ly.\n",
        "# scenario : imagine you're running a data analysis task on a large dataset of customer reviews for a\n",
        "# product. your goal is to perform sentiment analysis on each review to determine if it's positive,\n",
        "# negative, or neutral. the sentiment analysis process is computationally intensive, and you want to\n",
        "# speed it up using data parallelism.\n",
        "# data parallelism in sentiment analysis :\n",
        "# 1. data preparation : you have a dataset of 1,000,000 customer reviews. to apply data\n",
        "# parallelism, you divide this dataset into smaller, non-overlapping subsets. let's say you split\n",
        "# it into four subsets, each containing 250,000 reviews.\n",
        "# 2. parallel processing : you have a sentiment analysis model that can analyze reviews. you set\n",
        "# up four separate processing units (e.g., cpu cores or machines in a cluster), each responsible\n",
        "# for analyzing one subset of reviews. each processing unit loads its assigned subset of data.\n",
        "# 3. analysis : each processing unit applies the sentiment analysis model to its subset of reviews\n",
        "# independently and simultaneously. for in stance:\n",
        "#  processing unit 1 analyzes reviews 1 to 250,000.\n",
        "#  processing unit 2 analyzes reviews 250,001 to 500,000.\n",
        "#  processing unit 3 analyzes reviews 500,001 to 750,000.\n",
        "#  processing unit 4 analyzes reviews 750,001 to 1,000,000.\n",
        "# 4. aggregation : as each processing unit finishes its analysis, it generates results, such as counts\n",
        "# of positive, negative, and neutral reviews within its subset. these results are temporarily\n",
        "# stored.\n",
        "# 5. combining results : after all processing units have completed their work, you combine the\n",
        "# results. you sum up the counts from each processing unit to get the overall sentiment\n",
        "# analysis results for the entire dataset.\n",
        "# task parallelism(main -worker approach)\n",
        "# task parallelism involves executing multiple independent tasks or processes in parallel. each task\n",
        "# can perform different operations and may not necessarily operate on the same data. task\n",
        "# parallelism is common in applications where different tasks can be performed concurrently without\n",
        "# dependencies between them.\n",
        "# in the main -worker approach, one processor schedules and distributes the tasks for all the workers,\n",
        "# and each worker checks for the next work item as it returns the previous completed task .\n",
        "# example: web server handling requests\n",
        "# consider a web server handling incoming http requests. each i ncoming request is an independent\n",
        "# task that can be processed concurrently. the tasks include tasks like parsing the request, querying\n",
        "# the database, and generating the response. in a task parallelism scenario:\n",
        "# 1. task 1: parsing request\n",
        "#  this task involves pars ing the incoming http request to extract information like the\n",
        "# requested url, parameters, and headers.\n",
        "# 2. task 2: database query\n",
        "#  this task involves querying a database to fetch data related to the request, such as\n",
        "# user information or product details.\n",
        "# 3. task 3: generating response\n",
        "#  this task involves generating an html response based on the parsed request and\n",
        "# data retrieved from the database.\n",
        "# in a task parallelism setup, these tasks can be executed concurrently by multiple threads or\n",
        "# processes, allowing the server to handle multiple incoming requests simultaneously without waiting\n",
        "# for one task to complete before starting the next.\n",
        "# bucket -brigade parallelism :\n",
        "# a bucket brigade is a method of manually transporting items or materials from one location to\n",
        "# another by forming a line of people, each of whom carries an item and passes it to the next person.\n",
        "# this technique is similar to how buckets of water might be passed along a line of people to put out a\n",
        "# fire, which is where the term \"bucket brigade\" originated.\n",
        "# in parallel computing, the concept of bucket -brigade parallelism involves breaking down a task into\n",
        "# smaller subtasks, where each subtask is processed independently and passed to the next processing\n",
        "# unit for further computation. this technique allows for efficient parallel processing of tasks and is\n",
        "# often used in scenarios where tasks can be divided into smaller, manageable parts.\n",
        "# example: manufacturing assembly line\n",
        "# let's say we have a manufacturing assembly line for producing smartphones. the assembly line\n",
        "# consists of three stages: a, b, and c. each stage represents a specific task in the smartphone\n",
        "# assembly process.\n",
        "# 1. stage a - component assembly :\n",
        "#  worker a assembles the basic components of the smartphone, such as the circuit\n",
        "# board, battery, and display. once worker a finishes assembling a smartphone, it passes it\n",
        "# to stage b.\n",
        "# 2. stage b - software installation :\n",
        "#  worker b installs the operating system and necessary software onto the smartphone\n",
        "# assembled by worker a. after software installation, the smartphone is passed to stage c.\n",
        "# 3. stage c - quality control and packaging :\n",
        "#  worker c checks the smartphone for quality control, ensuring that all components\n",
        "# are working correctly an d the software is functioning as intended. if the smartphone passes\n",
        "# quality control, it is packaged and prepared for shipment.\n",
        "# in this example, each stage (a, b, and c) represents a processing step, similar to the stages in a\n",
        "# bucket -brigade parallelism sce nario.\n",
        "# parallel speedup versus comparative speedups.\n",
        "# parallel speedup and comparative speedup are two different metrics used to evaluate the performance\n",
        "# improvement achieved by parallel processing.\n",
        "# parallel speedup measures how much faster a parallel algorithm runs compared to its sequential\n",
        "# (single -processor) counterpart. it quantifies the performance improvement gained by using multiple\n",
        "# processing units in parallel. parallel speedup is calculated using the followi ng formula:\n",
        "# parallel speedup=sequential execution time/parallel execution\n",
        "# in this formula:\n",
        "#  sequential execution time is the time taken by the algorithm to execute sequentially on a\n",
        "# single processor.\n",
        "#  parallel execution time is the time taken by the paralle l algorithm to execute on multiple\n",
        "# processors.\n",
        "# comparative speedup : comparative speedup is between architectures. this is usually a\n",
        "# performance comparison between two parallel implementations or other comparison between\n",
        "# reasonably constrained sets of hardware. for example, it may be between a parallel mpi\n",
        "# implementation on all the cores of the node of a computer versus the gpu(s) on a node\n",
        "# how parallel computing works\n",
        "# as a developer, you are responsible for the application software layer, which includes your source code.\n",
        "# in the source code, you make choices about the programming language and parallel software interfaces\n",
        "# you use to leverage the underlying hardware. additionally, you decide how to break up your work into\n",
        "# parallel units. a compiler is designed to translate your source code into a form the hardware can execute.\n",
        "# with these instructions at hand, an os manages executing these on the computer hardware.\n",
        "# parallel approach models are used to express parallelization in an application software layer\n",
        "# that gets mapped to the computer hardware through the compiler and the os. parallel computing\n",
        "# approaches involve various models and paradigms that define how tasks are divided, coordinated, and\n",
        "# executed in parallel systems. here are some common parallel computing approach models:\n",
        "# hardware models\n",
        "# distributed memory architecture: a cross -node parallel method :\n",
        "# parallel approach models\n",
        "# hardware models\n",
        "# distributed memory architecture:\n",
        "# a cross -node parallel method :\n",
        "# shared memory architecture: an\n",
        "# on-node parallel method\n",
        "# vector units: multiple operations\n",
        "# with one instruction\n",
        "# accelerator device: a special -\n",
        "# purpose add-on processor\n",
        "# software models\n",
        "# process -based parallelization :\n",
        "# message passing\n",
        "# thread -based parallelization :\n",
        "# shared data via memory\n",
        "# vectorization : multiple\n",
        "# operations with one instruction\n",
        "# stream processing : through\n",
        "# specialized processors\n",
        "# distributed memory architecture, also known as distributed memory parallelism, is a parallel\n",
        "# computing method where multiple processors or nodes in a cluster have their own private memory.\n",
        "# these nodes are connected via a network, and they communicate and coordinate with each other\n",
        "# by passing messages. in this architecture, each node opera tes independently and has its own local\n",
        "# memory, and data sharing is achieved explicitly through message passing.\n",
        "# in the context of distributed memory architecture, a \"cross -node parallel method\" refers to parallel\n",
        "# processing techniques that involve distributing tasks across multiple nodes in a cluster. each node\n",
        "# works on its subset of the data or a specific portion of the computation. communication and\n",
        "# coordination between nodes are essential, as tasks often depend on results or data computed on\n",
        "# othe r nodes.\n",
        "# shared memory architecture: an on-node parallel method\n",
        "# in shared memory architecture, multiple processors or cores share a single, unified memory space.\n",
        "# this shared memory can be accessed and modified by any processor within the system. on -node\n",
        "# parallelism, within the context of shared memory architecture, ref ers to parallel processing\n",
        "# techniques that occur on a single computing node. in this approach, multiple threads or processes\n",
        "# run concurrently on the same node, accessing shared memory to perform computations .\n",
        "# vector units: multiple operations with one instruction\n",
        "# vector units, also known as vector processors, are specialized hardware units that can perform\n",
        "# multiple operations with a single instruction. these units are designed to process vectors, which are\n",
        "# arrays of data elements, simultaneously. vecto r processing is particularly useful in scenarios where\n",
        "# the same operation needs to be performed on a large set of data elements .\n",
        "# vector processing example with four array elements operated on simultaneously\n",
        "# accelerator device: a special -purpose add-on processor\n",
        "# gpus come in two varieties: integrated and discrete. discrete or dedicated gpus typically have a\n",
        "# large number of streaming multiprocessors and their own dram. accessing data on a discrete gpu\n",
        "# requires communication over a pci bus\n",
        "# an accelerator device, often referred to as an accelerator, is a specialized hardware component\n",
        "# (gpu) designed to perform specific types of computational tasks or workloads efficiently.\n",
        "# accelerators are typically used in conjunction with a central processin g unit (cpu) and are especially\n",
        "# well-suited for workloads that can benefit from parallel processing and offloading certain tasks from\n",
        "# the cpu. accelerators are sometimes called \"add -on processors\" because they augment the\n",
        "# processing capabilities of a syste m.\n",
        "# general heterogeneous parallel architecture model\n",
        "# now let’s combine all of these different hardware architectures into one model . two nodes, each\n",
        "# with two cpus, share the same dram memory. each cpu is a dual -core processor with an\n",
        "# integrated gpu. a discrete gpu on the pci bus also attaches to one of the cpus. though the cpus\n",
        "# share main memory, these are commonly in different non -uniform memory access (numa)\n",
        "# regions. this means that accessing the second cpu’s memory is more expensive than getting at it’s\n",
        "# own memory\n",
        "# fig 5: a general heterogeneous parallel architecture model consisting of two nodes connected by\n",
        "# a network. each node has a multi -core cpu with an integrated and discrete gpu and some\n",
        "# memory (dram).\n",
        "# software models\n",
        "# the programmer must first expose the parallelization, determine the best technique to operate in\n",
        "# parallel, and then explicitly direct its operation in a safe, correct, and efficient manner. the following\n",
        "# methods are the most common techniques for parallelization\n",
        "#  process -based parallelization : message passing\n",
        "# process -based parallelization, particularly through message passing, is a common approach in\n",
        "# parallel computing. it involves dividing a task into multiple processes or threads that run\n",
        "# independently on separate computing nodes or cores. these processes communicate and\n",
        "# coordinate with each other by sending and receiving messages. message passing is a method of\n",
        "# inter -process communication where data and instructions are exchanged between processes to\n",
        "# synch ronize and share information. this approach is widely used in distributed memory systems,\n",
        "# such as clusters and supercomputers.\n",
        "# fig 6 : the message passing library spawns processes. the os places the processes on the cores\n",
        "# of two nodes. the question marks i ndicate that the os controls the placement of the processes\n",
        "# and can move these during run time as indicated by the dashed arrows. the os also allocates\n",
        "# memory for each process from the node’s main memory\n",
        "#  thread -based parallelization : shared data via memory\n",
        "# thread -based parallelization involves dividing a task into multiple threads that share the same\n",
        "# memory space within a single process. these threads can run concurrently on multiple cpu cores,\n",
        "# and they communicate and coordinate by accessing shared data in the shared memory. this\n",
        "# approach is commonly used in multi -core processors and symmetric multiprocessing (smp) systems.\n",
        "# fig 7: the application process in a thread -based approach to parallelization spawns\n",
        "# threads. the threads are restricted to the node’s domain. the question marks show that\n",
        "# the os decides where to place the threads. some memory is shared between threads.\n",
        "#  vectorization : multiple operations with one instruction\n",
        "# vectorization is a parallel computing technique that enables processors to perform multiple\n",
        "# operations with a single instruction. it takes advantage of simd (single instruction, multiple data)\n",
        "# capabilities found in modern processors, including cpus and gpus. simd allows a single instruction\n",
        "# to operate on multiple da ta elements simultaneously, which can significantly accelerate\n",
        "# computations involving large datasets.\n",
        "#  stream processing : through specialized processors\n",
        "# stream processing, often referred to as stream computing or data stream processing, is a computing\n",
        "# paradigm where data is continuously processed as it is generated or ingested, rather than being\n",
        "# stored in traditional databases or file systems. stream proce ssing is particularly useful for handling\n",
        "# large volumes of real -time data from various sources, such as sensors, social media, financial\n",
        "# transactions, and iot devices. specialized processors designed for stream processing accelerate the\n",
        "# analysis and manipu lation of data streams, ensuring timely and efficient processing\n",
        "# in the stream processing approach, data and compute kernel are offloaded to the gpu and its\n",
        "# streaming multiprocessors. processed data, or output, transfers back to the cpu for file io or other\n",
        "# work\n",
        "# sample application\n",
        "# we start with a 2d problem domain of a region of space. for purposes of illustration, we will use\n",
        "# a 2d image of the krakatau volcano as our example. the goal of our calculation could be to model\n",
        "# the volcanic plume, the resulting tsunami, or the early detection of a volcanic eruption using\n",
        "# machine learning. for all of these options, calculation speed is critical if we want real -time result s\n",
        "# to inform our decisions.\n",
        "# 1. discretize (break up) the problem into smaller cells or elements\n",
        "# 2 . define a computational kernel (operation) to conduct on each element of the mesh\n",
        "# 3. add the following layers of parallelization on cpus and gpus to perfo rm the calculation:\n",
        "# vectorization —work on more than one unit of data at a time\n",
        "# 4. threads —deploy more than one compute pathway to engage more processing cores\n",
        "# 5. processes —separate program instances to spread out the calculation into separate memory\n",
        "# spaces\n",
        "# 6. off -loading the calculation to gpus —send the data to the graphics processor to calculate\n",
        "# step 1: discretize the problem into smaller cells or elements\n",
        "# the domain is discretized into cells. for each cell in the computational domain, properties such as\n",
        "# wave height, fluid velocity, or smoke density are solved for according to physical laws. ultimately, a\n",
        "# stencil operation or a matrix -vector system represents this discrete scheme\n",
        "# step 2: define a computational kernel, or operation, to conduct on each element of the mesh\n",
        "# the calculations on this discretized data are often some form of a stencil operation, so -called because\n",
        "# it involves a pattern of a djacent cells to calculate the new value for each cell. this can be an average\n",
        "# (a blur operation, which blurs the image )gradient (edge -detection, which sharpens the edges in the\n",
        "# image) or another more complex operation associated with solving physical sys tems described by\n",
        "# partial differential equations (pdes)\n",
        "# step 3: vectorization to work on more than one unit of data at a time\n",
        "# some processors have the ability to operate on more than one piece of data at a time; a capability\n",
        "# referred to as vector operations. the shaded blocks in figure illustrate how multiple data values\n",
        "# are operated on simultaneously in a vector unit in a proce ssor with one instruction in one clock\n",
        "# cycle.\n",
        "# step 4: threads to deploy more than one compute pathway to engage more processing cores\n",
        "# because most cpus today have at least four processing cores, we use threading to operate the cores\n",
        "# simultaneously acros s four rows at a time.\n",
        "# step 5: processes to spread out the calculation to separate memory spaces\n",
        "# we can further split the work between processors on two desktops, often called nodes in parallel\n",
        "# processing. when the work is split across nodes, the memory spaces for each node are distinct and\n",
        "# separate.\n",
        "# step 6: off -loading the calculation to gpus\n",
        "# on a gpu, the vector length is much larger than on a cpu. here, 8×8 tiles are distributed across\n",
        "# gpu work groups.\n",
        "# performance limits and profiling\n",
        "# in parallel processing, understanding performance limits and profiling the application are crucial\n",
        "# steps to optimize the execution of parallel programs.\n",
        "# performance limits refer to the maximum achievable performance of a computing system or\n",
        "# application un der specific conditions. these limits are determined by various factors and constraints\n",
        "# and play a crucial role in understanding the capabilities and limitations of a system. understanding\n",
        "# these performance limits is essential for designing efficient algor ithms, optimizing software, and\n",
        "# choosing appropriate hardware configurations. it also guides researchers and engineers in\n",
        "# developing new technologies to overcome existing limitations and improve overall computing\n",
        "# performance.\n",
        "# profiling tools are used to ga ther detailed information about the behavior of a parallel program. by\n",
        "# understanding performance limits, utilizing profiling tools, and optimizing the code based on the\n",
        "# profiling results, developers can enhance the efficiency of parallel applications, lea ding to improved\n",
        "# speedup and overall performance.\n",
        "# application’s potential performance limits\n",
        "#  flops (floating -point operations)\n",
        "#  ops (operations) that include all types of computer instructions\n",
        "#  memory bandwidth: rate at which the data is transferred\n",
        "#  memory latency: time required for the first byte or word of data to be transferred\n",
        "#  instruction queue (instruction cache)\n",
        "#  networks\n",
        "#  disk\n",
        "#  machine balance: number of flops executed /memory bandwidth\n",
        "#  arithmetic intensity : number of flops executed per memory operation\n",
        "# all of these limitations can be divided into two major categories:. speeds are how fast operations can\n",
        "# be done. it includes all types of computer operations. but to be able to do the operations, you must\n",
        "# get the data there. this is where feeds come in. feeds include the memory bandwidth through the\n",
        "# cache hierarchy, as well as network and disk bandwidth .\n",
        "# for many applications, the memory bandwidth limit can be difficult especially dealing with non-\n",
        "# contiguous bandwidth.it is also known as strided memory access or non -contiguous memory\n",
        "# access, refers to the manner in which data elements are accessed in memory. in contrast to\n",
        "# contiguous memory access, where elements are stored in consecutive memory locations, non -\n",
        "# contiguous memory access invo lves accessing elements that are not stored sequentially in memory.\n",
        "# non -contiguous memory access:\n",
        "# now, consider a situation where the array elements are scattered in memory with a stride of 2. this\n",
        "# is a non -contiguous memory access pattern:\n",
        "# in this case, accessing every second element (stride = 2) would mean accessing memory locations 1,\n",
        "# 2, 3, 4, 5, etc., but the elements are not stored sequentially.\n",
        "# when your program needs to access such non -contiguous elements, it may lead to inefficiencie s\n",
        "# due to increased cache misses and a higher likelihood of accessing data from main memory rather\n",
        "# than the faster cache memory.\n",
        "# determine your hardware capabilities:\n",
        "# to determine the performance of hardware the following metrics are used :\n",
        "#  the rate at which floating -point operations can be executed (flops/s)\n",
        "#  the rate at which data can be moved between various levels of memory (gb/s)\n",
        "#  the rate at which energy is used by your application (watts)\n",
        "# in determining hardware performance and calculating the metrics , we use a mixture of theoretical\n",
        "# and empirical measurements .\n",
        "# theoretical measurements provide an upper limit to what a system can achieve. for instance, in\n",
        "# parallel computing, theoretical analysis can reveal the maximum speedup or efficiency that a\n",
        "# parallel algorithm can achieve in an ideal scenario .\n",
        "# real -world validation is done by empirical measurements , they provide concrete evidence of how\n",
        "# a system performs under real -world conditions, accounting for various factors like i/o operations,\n",
        "# network latency, and concurrency issues.\n",
        "# one of the best tools for understanding the hardware you run is the lstopo program (graphical view)\n",
        "# and lscpu for text view . lstopo is bundled with the hwloc package that com es with nearly every mpi\n",
        "# distribution. this command outputs a graphical view of the hardware on your system. figure below\n",
        "# shows the output for a mac laptop in graphical view.\n",
        "# text view\n",
        "# the information from the lscpu command and the /proc/cpuinfo file helps to determine the number\n",
        "# of processors, the processor model, the cache sizes, and the clock frequency for the system\n",
        "# calculating theoretical maximum flops\n",
        "# theoretical flops=number of cores×clock speed×flops per cycle per core\n",
        "# where:\n",
        "#  number of cores: this represents the total number of processor cores in the computing\n",
        "# system.\n",
        "#  clock speed: this indicates the clock speed of each core in the system, typically measured\n",
        "# in hertz (hz) or gigahertz (ghz). it represents the number of cycles th e processor can\n",
        "# execute per second.\n",
        "#  flops per cycle per core: this signifies the number of floating -point operations a core can\n",
        "# perform in a single clock cycle. modern processors often perform multiple flops per cycle\n",
        "# due to features like simd (single inst ruction, multiple data) operations.\n",
        "# for example, let's consider a system with 4 cores, each operating at 3.0 ghz, and capable of\n",
        "# executing 4 flops per cycle per core (assuming simd operations are utilized):\n",
        "# theoretical flops=4 cores×3.0 ghz×4 flops per cycle per core\n",
        "# theoretical flops=48 gflops\n",
        "# the memory hierarchy and theoretical memory bandwidth\n",
        "# we can calculate the theoretical memory bandwidth of the main memory using the memory chips\n",
        "# specifications.\n",
        "# the general formula is b t = mtr × mc × tw × ns = data transfer rate × memory channels × bytes\n",
        "# per access × sockets\n",
        "# processors are installed in a socket on the motherboard. the motherboard is the main system board\n",
        "# of the computer, and the socket is the lo cation where the processor is inserted. most motherboards\n",
        "# are single -socket, where only one processor can be installed. dual -socket motherboards are more\n",
        "# common in high -performance computing systems. two processors can be installed in a dual -socket\n",
        "# motherb oard, giving us more processing cores and more memory bandwidth.\n",
        "# empirical measurement of bandwidth and flop\n",
        "# the empirical bandwidth is the measurement of the fastest rate that memory can be loaded from\n",
        "# main memory into the processor. if a single byte of m emory is requested, it takes 1 cycle to retrieve\n",
        "# it from a cpu register. if it is not in the cpu register, it comes from the l1 cache. if it is not in the l1\n",
        "# cache, the l1 cache loads it from l2 and so on to main memory. if it goes all the way to main\n",
        "# memo ry, for a single byte of memory, it can take around 400 clock cycles. this time required for the\n",
        "# first byte of data from each level of memory is called the memory latency .\n",
        "# two different methods are used for measuring the bandwidth: the stream benchmark and the\n",
        "# roofline model measured by the empirical roofline toolkit.\n",
        "# key differences:\n",
        "#  focus: stream primarily focuses on memory bandwidth, providing quantitative\n",
        "# measurements. in contrast, the roofline model provides a graphical representation of\n",
        "# performance bottlenecks, considering both computational capabilities and memory\n",
        "# bandwidth.\n",
        "#  representation: stream results in a numerical measurement (memory bandwidth in bytes\n",
        "# per second), while the roofline model is a graphical representation that helps v isualize\n",
        "# performance limitations.\n",
        "#  insights: stream provides detailed insights into memory subsystem performance, whereas\n",
        "# the roofline model offers a high -level overview of an application's performance efficiency\n",
        "# concerning hardware constraints.\n",
        "# calculatin g the machine balance between flops and bandwidth\n",
        "# the machine balance is the flops divided by the memory bandwidth.\n",
        "# we can calculate both a theoretical machine balance (mb t) and an empirical machine balance (mb e)\n",
        "# like so:\n",
        "# mb t = f t / b t\n",
        "# mb e = f e / b e\n",
        "# characterizing your application: profiling\n",
        "# now that you have some sense of what performance you can get with the hardware, you need to\n",
        "# determine what are the performance characteristics of your application. additionally, you should\n",
        "# develop an understandi ng of how different subroutines and functions depend on each other .\n",
        "# profiling tools :\n",
        "# using call graphs for hot -spot and dependency analysis\n",
        "# in the context of parallel programming, call graphs are diagrams that represent the calling\n",
        "# relationships between different functions or methods in a parallel program. they illustrate how\n",
        "# functions or tasks invoke each other and provide a visual representation of the program's control\n",
        "# flow. analyzing call graphs in parallel programming can provide valuable insights in to the program's\n",
        "# structure, dependencies, and potential performance optimizations . by analyzing these call graphs,\n",
        "# developers can identify hot -spots —functions or tasks that consume a significant amount of\n",
        "# computational time. optimizing these hot -spots is e ssential for improving overall parallel program\n",
        "# performance.\n",
        "# empirical measurement of processor clock frequency and energy consumption\n",
        "# empirical measurement of processor clock frequency:\n",
        "# 1. profiling tools: profiling tools like intel vtune profiler or amd codexl can provide insights\n",
        "# into various performance metrics, including processor clock frequency. these tools often offer\n",
        "# visualizations and detailed reports for better analysis.\n",
        "# 2. benchmarking suites: benchmarking tools like spec cpu benchmarks or hpc chal lenge\n",
        "# benchmarks often include components that measure processor clock frequencies. running\n",
        "# these benchmarks can provide detailed information about the processor's performance\n",
        "# characteristics.\n",
        "# 2. empirical measurement of energy consumption:\n",
        "# 1. power measureme nt tools: use power measurement tools and hardware devices to\n",
        "# measure the power consumption of your system. power meters and sensors can be attached\n",
        "# to the system to measure real -time power usage. tools like intel power gadget or linux's\n",
        "# powerstat can help measure power usage.\n",
        "# 2. energy profilers: some profiling tools, like intel vtune profiler, also offer energy profiling\n",
        "# capabilities. they can provide insights into energy consumption patterns at different parts of\n",
        "# your code. these tools often correlate energ y consumption with specific functions or code\n",
        "# regions.\n",
        "# tracking memory during run time\n",
        "# tracking memory usage during runtime in parallel computing is crucial for optimizing performance,\n",
        "# detecting memory leaks, and ensuring efficient memory management. several techniques and tools\n",
        "# can help you monitor memory usage in parallel applications. here are some approaches to tracking\n",
        "# memory during runtime in parallel computing environments:\n",
        "# profiling tools:\n",
        "# 1. valgrind massif: valgrind is a powerful instrumentation framework. massif, a valgrind tool, can\n",
        "# profile heap memory usage over time, showing memory consumption patterns. it's particularly\n",
        "# useful for detecting memory leaks and understanding how memory usage evolves during\n",
        "# program execution.\n",
        "# 2. intel vtune profiler : vtune profiler provides memory analysis capabilities, including memory\n",
        "# usage tracking. it can analyze memory consumption at various levels, from individual functions\n",
        "# to entire applications, in both serial and parallel contexts.\n",
        "# 3. openmp/mpi memory profiler s: many parallel programming frameworks like openmp and\n",
        "# mpi provide their memory profiling tools. for example, openmp has tools like score -p, and mpi\n",
        "# has memory profiling features integrated into mpi implementations.\n",
        "# parallel algorithms and patterns\n",
        "# a parallel algorithm is a step -by-step computational procedure or set of rules designed to be\n",
        "# executed on parallel computing architectures. these algorithms are specifically crafted to take\n",
        "# advantage of parallel processing capabilities, where multiple proces sors or cores can work together\n",
        "# to solve a problem.\n",
        "# parallel patterns are like reusable blueprints that help programmers apply proven methods to solve\n",
        "# specific types of problems efficiently. these patterns guide the decomposition of tasks and data,\n",
        "# providi ng a framework for creating effective parallel algorithms .\n",
        "# example : parallel algorithm for finding the maximum element:\n",
        "# suppose you have a large array of numbers, and you want to find the maximum element using a\n",
        "# parallel algorithm based on the \"divide an d conquer\" pattern.in this example, the \"divide and\n",
        "# conquer\" pattern is applied to find the maximum element in an array. the array is divided into\n",
        "# smaller subarrays, and the maximum values of these subarrays are found in parallel. finally, the\n",
        "# maximum amon g these partial maximums is selected as the maximum element of the entire array.\n",
        "# algorithm analysis for parallel computing applications\n",
        "# the goal of algorithm analysis is to compare different algorithms that are used to solve the same\n",
        "# problem. one of the more traditional ways to evaluate algorithms is by looking at their algorithmic\n",
        "# complexity.\n",
        "# definition: algorithmic complexity is a measure of the number of operations that it would take to\n",
        "# complete an algorithm. algorithmic complexity is a pro perty of the algorithm and is a measure of\n",
        "# the amount of work or operations in the procedure.\n",
        "# complexity is usually expressed in asymptotic notation . using asymptotic notation, you can analyze\n",
        "# and compare algorithms efficiently and make informed decisions when choosing the most suitable\n",
        "# algorithm for a particular problem, taking into account both time and space complexity\n",
        "# the three main types of asymptotic notation are:\n",
        "# 1. big o notation (o-notation):\n",
        "#  big o notation describes the upper bound or worst -case time complexity of an\n",
        "# algorithm. it represents an approximation of how an algorithm's running time\n",
        "# increases as the input size grows. it provides an upper limit on the number of basic\n",
        "# operations an algorithm performs.\n",
        "# 2. theta notation (θ -notation):\n",
        "#  theta notation provides a tight bound, expressing both the upper and lower bounds\n",
        "# of an algorithm's time complexity. it characterizes the average -case behavior of an\n",
        "# algorithm.\n",
        "# 3. omega notation (ω -notation):\n",
        "#  omega notation describes the lower bound or best -case time complexity of an\n",
        "# algorithm. it provides a way to express how quickly the algorithm can solve a problem\n",
        "# in the most favorable circumstances.\n",
        "# performance models versus algorithmic complexit y\n",
        "# performance models are broader and more practical in nature. they encompass various\n",
        "# aspects of system performance, including algorithmic efficiency, but also consider factors\n",
        "# related to specific hardware, software, and real -world scenarios.\n",
        "# algorithmic complexity, often expressed using asymptotic notations like big o, theta, and\n",
        "# omega, focuses on analyzing the efficiency of algorithms in terms of their time and space\n",
        "# requirements as a function of the input size. it provides a theoretical fram ework for\n",
        "# characterizing how an algorithm's performance scales as the input size grows towards\n",
        "# infinity .\n",
        "# example: finding the sum of all elements in an array.\n",
        "# algorithmic complexity (big o notation):\n",
        "#  time complexity: o(n) - linear time complexity, where n is the size of the input array.\n",
        "# the algorithm processes each element once.\n",
        "#  space complexity: o(1) - constant space complexity, as it uses only a few variables\n",
        "# regardless of the input size.\n",
        "# performance model considerations:\n",
        "#  hardware differences: different computers might execute the same algorithm at\n",
        "# different speeds due to variations in processor capabilities.\n",
        "#  compiler optimizations: compilers can optimize the code differently, affecting the\n",
        "# execution time.\n",
        "#  parallelization: divide the array into chunks and calculate the partial sums concurrently\n",
        "# using parallel processing techniques, especially for large arrays.\n",
        "#  memory optimization: for very large arrays, consider memory -efficient data structures\n",
        "# or algorithms to reduce memory usage.\n",
        "# parallel algorithms\n",
        "# parallel algorithms are designed to efficiently solve computational problems by utilizing multiple\n",
        "# processing units (such as cpu cores, gpus, or distributed computing nodes) simultaneously. they are\n",
        "# crucial in high -performance computing (hpc) and parallel processing environments where large\n",
        "# datasets and complex computations need to be handled efficiently. parallel algorithms aim to break\n",
        "# down tasks into smaller subtasks that can be processed independently and concurrently, leading to\n",
        "# significant speedup in overall computation time. here are some common types of parallel algorithms:  parallel merge sort: divide the sorting task into smaller parts, sort them independently, and\n",
        "# then merge the sorted parts in parallel.\n",
        "#  parallel quicksort: a parallel version of th e quicksort algorithm that partitions the data and\n",
        "# sorts partitions concurrently.\n",
        "# hash function\n",
        "# hashing is a popular technique for quickly storing and retrieving data. hashing is a technique or process\n",
        "# of mapping keys, values into the hash table by using a hash function. it is done for faster access to\n",
        "# elements. the efficiency of mapping depends on the efficiency of the hash function used.\n",
        "# components of hashing\n",
        "# 1. hash function: the hash function itself is a crucial component. it takes an input and produces\n",
        "# a fixed -size hash value. the hash function ensures that the same input always produces the\n",
        "# same hash value and that even a small change in the input results in a significantly different\n",
        "# hash value.\n",
        "# 2. input data: this is the data that you want to hash. it can be any type of data, such as a file, a\n",
        "# password, or a message.\n",
        "# 3. hash value: also known as the hash code or hash keys , it's the output of the hash function\n",
        "# after processing the input data. the hash value is typically a index into hash table .\n",
        "# 4. collision: a collision occurs when two different inputs produce the same hash value.\n",
        "# 5. bucket or slot: in the context of hash tables , a bucket or slot is a location where data is stored\n",
        "# based on its hash value. hash tables consist of an array of these buckets, and the hash value\n",
        "# determines which bucket a particular piece of data will be stored in.\n",
        "# 6. hash table: a hash table is a data str ucture that uses hashing to implement an associative\n",
        "# array, a structure that can map input to values. it consists of an array of buckets where data is\n",
        "# stored based on its hash value. hash tables allow for efficient insertion, deletion, and lookup\n",
        "# operation s.\n",
        "# 7. load factor: the load factor of a hash table is the ratio of the number of stored elements to\n",
        "# the total number of buckets. a high load factor can lead to increased collisions and decreased\n",
        "# performance, so hash tables are often resized and rehashed if th e load factor exceeds a certain\n",
        "# threshold.\n",
        "# 8. sparsity : the sparsity of a hash table is the ratio of the number of empty buckets to the total\n",
        "# number of buckets 9. collision resolution: techniques used to handle collisions include chaining (where each bucket\n",
        "# contains a linked list of items that hash to the same index) and open addressing (where the\n",
        "# algorithm searches for the next open slot in the hash table).\n",
        "# spatial hashing\n",
        "# spatial data is any type of data that directly or indirectly references a specific geographical area or\n",
        "# location. sometimes called geospatial data or geographic information, spatial data can also numerically\n",
        "# represent a physical object in a geographic coordinate system. however, spatial data is much more\n",
        "# than a spatial component of a map. spatial data can be stored in either vector format or raster format\n",
        "# vector\n",
        "# raster\n",
        "# spatial hashing is a technique where the key is based on spatial information. spatial hashing is a\n",
        "# technique used to locate objects in a 3d space. it involves dividing a large space into smaller, grid -like\n",
        "# cells, and assigning each object in the space to the cell that contains it..the basic principle is to map\n",
        "# objects onto a grid of buckets arranged in a regular pattern .\n",
        "# spatial information can also be stored in adaptive mesh refinement (amr) format.amr is a numerical\n",
        "# simulation technique used in computational mathematics, fluid dynamics, and other fields to imp rove\n",
        "# the efficiency and accuracy of simulations. amr systems maintain multiple levels of grids. each level\n",
        "# represents a different resolution of the simulation domain. finer grids cover smaller areas, providing\n",
        "# high resolution, while coarser grids cover lar ger areas, offering lower resolution.\n",
        "# the same data of amr can be even stored in perfect hashtable or compact hash table.\n",
        "# perfect hashing is a technique used to eliminate collisions entirely in hash tables, ensuring that each\n",
        "# key maps to a unique index without any conflicts. in traditional hash tables, collisions can occur when\n",
        "# multiple keys hash to the same index, requiring additional data structures like linked lists or open\n",
        "# addressing techniques to resolve these collisions. perfect hashing, on the oth er hand, aims to design a\n",
        "# hash function and data structure in such a way that collisions never occur.\n",
        "# compact hashing is a technique used to design hash functions in a way that minimizes the memory\n",
        "# required for hash tables. it focuses on creating hash func tions that distribute keys uniformly across the\n",
        "# available slots in a hash table while keeping the table small in size. the objective is to use as few bits\n",
        "# as possible per key, reducing the memory footprint of the hash table.\n",
        "# because of the compression to a compact hash, two entries try to store their value in bucket 1. the\n",
        "# second entry sees that there is already a value there, so it looks for the next open slot in a technique\n",
        "# called open addressing. in open addressing, we look for the next open slot in the hash table and\n",
        "# store the value in that slot. there are other hashing methods than open addressing, but these often\n",
        "# require the ability to allocate memory during an operation. a llocating memory is more difficult on\n",
        "# the gpu, so we stick with open addressing where collisions are resolved by finding alternate storage\n",
        "# locations within the already allocated hash table. in open addressing, there are a few choices that\n",
        "# we can use as the trial for the next open slot. these are\n",
        "# linear probing —where the next entry is just the next bucket in sequence until an open bucket is\n",
        "# found\n",
        "# quadratic probing —where the increment is squared so that the attempted buckets are +1, +4,\n",
        "# +9, and so forth f rom the original location\n",
        "# double hashing —where a second hashing function is used to jump to a deterministic, but pseudo -\n",
        "# random distance from the first trial location\n",
        "# on spatial data we can perform, four spatial operations.\n",
        "#  neighbor finding —locating the one or two neighbors on each side of a cell\n",
        "#  remapping —mapping another amr mesh onto a current amr mesh\n",
        "#  table lookup —locating the intervals in the 2d table to perform the interpolation\n",
        "#  sorting —a 1d or 2d sort of the cell data\n",
        "# these can be implemented either by perfect hashing or compact hashing.\n",
        "# neighbor finding using a spatial perfect hash\n",
        "# the steps involved are\n",
        "#  allocate a spatial hash the size of the finest level of the cell -based amr mesh\n",
        "#  for each cell in the amr mes h, write the cell number to the hash buckets underlying the cell\n",
        "#  compute the index for a finer cell one cell outside the current cell on each side\n",
        "#  read the value placed in the hash bucket at that location.\n",
        "# example: each cell writes its cell number to the hash buckets it covers. right neighbor of cell 2 1 is\n",
        "# at col 8, row 3. look up in hash and it is cell 26 .\n",
        "# remapping —mapping another amr mesh onto a current amr mesh (perfect hash)\n",
        "# remapping from one mesh to another is a common operation in computational simulations,\n",
        "# especially in fields like computational fluid dynamics (cfd) and finite element analysis (fea). this\n",
        "# process is used to transfer data (usually physical quantities like temperature, pressure, or velocity)\n",
        "# from one mesh or grid to another. you have two grids or meshes: the source mesh (from which you\n",
        "# want to remap data) and the target mesh (to which you want to map the data ).\n",
        "# table lookup(perfect hash )\n",
        "# to perform a table lookup in spatial hashing, you need to map the position of an object to a\n",
        "# unique key (hash value), which determines the index of the bucket in the hash table where\n",
        "# the object should be retrieved.\n",
        "#  perform a lookup in the hash table at the calculated index to find the bucket\n",
        "# corresponding to the object's spatial location.\n",
        "#  if chain ing (linked lists in each bucket) is used for collision resolution, traverse the\n",
        "# linked list in the selected bucket to find the object.\n",
        "#  if open addressing is used, probe the adjacent cells until the object is found or an\n",
        "# empty cell is encountered.\n",
        "# sorting mesh data using a spatial perfect hash\n",
        "# we can demonstrate the hash sort operation . the minimum difference between values is 2.0, so\n",
        "# the bucket size of 2 guarantees that there are no collisions. the minimum value is 0, so the bucket\n",
        "# location can be calcul ated with bi = xi /δmin = xi /2.0. we could store either the value or the index\n",
        "# in the hash table. for example, 8, the first key, could be stored in bucket 4 .\n",
        "# prefix sum (scan) pattern and its importance in parallel computing\n",
        "# the prefix sum, also known as the scan operation, is a fundamental parallel pattern in computer\n",
        "# science and parallel computing. given an input array of elements, the prefix sum operation computes\n",
        "# a new array where each element is the sum of all elements in the input array up to and including the\n",
        "# corresponding element's position. there are two common types of prefix sum operations: exclusive\n",
        "# and inclusive.\n",
        "#  exclusive prefix sum: the result at each position does not include the element at that\n",
        "# position.\n",
        "#  input: [a, b, c, d]\n",
        "#  output (exclusive): [0, a, a+b, a+b+c]\n",
        "#  inclusive prefix sum: the result at each position includes the element at that position.\n",
        "#  input: [a, b, c, d]\n",
        "#  output (inclusive): [a, a+b, a+b+c, a+b+c+d]\n",
        "\n",
        "# step -efficient parallel scan operation(inclusive prefix sum)\n",
        "# work -efficient parallel scan operation(exclusive prefix sum)\n",
        "\n",
        "# the work -efficient parallel scan operation uses two sweeps through the arrays. the first sweep is\n",
        "# called an upsweep, though it is more of a right sweep.\n",
        "# the second phase, known as the downsweep phase, is more of a left sweep. the output of upsweep\n",
        "# is provided as input to downsweep. it starts by setting the last value to zero and then does another\n",
        "# tree-based sweep to get the final result.\n",
        "# parallel global sum\n",
        "# the \"parallel global sum\" refers to the problem of computing the sum of elements across multiple\n",
        "# processors or nodes in a parallel or distributed computing environment.\n",
        "# though the process is simple it has some problems like changing the order of additions changes the\n",
        "# answer in finite -precision arithmetic. this is problematic because a parallel calculation changes the\n",
        "# order of the additions. the problem is due to finite -precision arithmetic not being associative. and\n",
        "# the problem gets worse as the problem size gets larger because the addition of the last value\n",
        "# becomes a smaller and smaller part of the overall sum. eventually the addition of the last value\n",
        "# might not change the sum at all. there is even a worse case for additions of finite precision values\n",
        "# when adding two values that are almost identical, but of different signs. this subtraction of one\n",
        "# value from another when these are nearly the same causes a catastrophic cancella tion.\n",
        "# catastrophic cancellation occurs when the operands are subject to rounding errors.\n",
        "# for example, if there are two measures\n",
        "# l1=253.5cm long and the other l2 =252.5cm long\n",
        "# approximations could come out to be\n",
        "# l1 = 254cm and\n",
        "# l2 = 252cm l1-l2 = 2cm\n",
        "# actual difference is l1-l2 = 1 cm\n",
        "# there are several solutions for addressing the global sum . the list of possible techniques presented\n",
        "# here includes\n",
        "#  long -double data type\n",
        "#  pairwise summation\n",
        "#  kahan summation\n",
        "#  knuth summation —uses same method of pairwise\n",
        "#  quad -precision summation\n",
        "# long -double data type\n",
        "# the easiest solution is to use the long -double data type on a x86 architecture. on this architecture,\n",
        "# a long -double is implemented as an 80 -bit floating -point number in hardware giving an extra 16 -bits\n",
        "# of precision. unfortunately, this is not a portable technique\n",
        "# pairwise summation\n",
        "# pairwise summation, also known as pairwise addition, is a method used to sum a sequence of\n",
        "# numbers in a way that reduces the effects of numerical errors, particularly in floating -point\n",
        "# arithmetic. this technique is commonly employed in scientific computing and numerical analysis to\n",
        "# improve the accuracy of summation operations.\n",
        "# how pairwise summation works:\n",
        "# 1. pairing the numbers:\n",
        "#  given a sequence of numbers, they are paired up. if there are an odd number of\n",
        "# elements, one number is left unpaired.\n",
        "# 2. pairwise addition:\n",
        "#  within each pair, the two numbers are added together to create intermediate sums.\n",
        "# 3. summing the intermediate sums:\n",
        "#  the intermediate sums obtained from pairwise addition are then summed together\n",
        "# using the same pairwise summation method.\n",
        "# kahan summation\n",
        "# kahan summation, also known as compensated summation or kahan summation algorithm, is a\n",
        "# method used to reduce the numerical error that accumulates during the summation of a large\n",
        "# number of floating -point values. t his technique was introduced by william kahan, a renowned\n",
        "# computer scientist and mathematician, and it aims to improve the accuracy of summation\n",
        "# operations, especially in cases where a vast number of values need to be added together.\n",
        "# how kahan summation works:\n",
        "# in standard floating -point summation, when adding a small number to a large number, the small\n",
        "# number can be \"lost\" in the least significant bits of the large number, leading to a loss of precision.\n",
        "# kahan summation addresses this issue by using a com pensation term to keep track of the lost\n",
        "# precision.\n",
        "# 1. initialization:\n",
        "#  initialize the sum and the compensation term to zero.\n",
        "# 2. iterative addition:\n",
        "#  for each number to be added:\n",
        "#  add the number to the current sum.\n",
        "#  calculate the difference between the updated sum a nd the original sum (this\n",
        "# difference is the lost precision).\n",
        "#  add this difference to the compensation term.\n",
        "# 3. final result:\n",
        "#  the final result is the sum adjusted by the compensation term.\n",
        "# quad -precision summation\n",
        "# quad -precision summation refers to performing arithmetic operations with numbers represented\n",
        "# in quadruple -precision floating -point format. in the ieee 754 floating -point standard, quadruple -\n",
        "# precision is a 128 -bit data type, providing higher precision compared to single -precision (32 -bit) and\n",
        "# double -precision (64 -bit) floating -point number.\n",
        "# '''\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CkY-EM7JhW3G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean(text):\n",
        "    cleaned_text = text.replace('\\n\\n', '\\n')\n",
        "    cleaned_text = re.sub(r'[\\d*•\\-]+', ' ', cleaned_text)\n",
        "    return cleaned_text\n",
        "def count_words(text):\n",
        "    words = text.split()\n",
        "    num_words = len(words)\n",
        "    return num_words \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV2jNa9dhsly",
        "outputId": "61e832b0-0d8b-4579-bf01-07f1df92109e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9555\n"
          ]
        }
      ],
      "source": [
        "initial_count=count_words(text)\n",
        "print(initial_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzfd7j2Y5bf7",
        "outputId": "82d6bda2-77cf-46ea-a151-fba6a86d0a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/86.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=4df58fcb0c14b21cdc9ab9b19602816230e690cc24c6aaff8d68086f60651f09\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "4b26f5f9c1e14e1b87a46df9ecdc8ce0",
            "15918e6cc16f48d0aef9e0338ba9414b",
            "afc13d22546d49beac0e07cee8594884",
            "0fc765c2ad074c23bf983778536e6ae3",
            "c92c1a4c2106446dade797d455248402",
            "d820ae5158de4add8287ef107bd147c9",
            "cc7f42eadca64700bdc6442df1795a55",
            "c42d121fb7a046bc8f0dfad002448a51",
            "c2c1e790804e46dfab61bb6482f0b800",
            "880fc387eb0545daacefd2ddccbc142e",
            "c2b8d9bcefb24e5581b88a6da7670ca2",
            "1440ac4325d04b67a2f0cca0bf707052",
            "406e17f1717642f3b1de316d149db4b7",
            "55fb87a785344c2e87e9415fe6b5971d",
            "3c346f1e5e0b44bb901384f512472a77",
            "cebca57b98024b82bd44d6b87357e6c6",
            "79754fb19a944d679aff7d1d41bee382",
            "30a9903b92fd4898a912a85197a0fa8c",
            "90003a4b4a634e11a771f78099479270",
            "3752af9de83f4fb3983b1498f67c14ad",
            "5e4f5802a99148c9b18fef97e5ffb41a",
            "e4b7590ef96e4fd292f24360575e6fe8",
            "07b5cf374657429698e0571fa531afe7",
            "415636cea443480ca587bdb0f14cadbd",
            "4eb4c79a5b33405b86b036f73b97e5e5",
            "69df3ec03b894e24964d7b026a05e333",
            "70b6de45fd814c78a8fa5aed4ef25c71",
            "10cfcad40a324b5e8627b9cf49362f9b",
            "3912d48a397d4aeab0bea005bf133b47",
            "3613369f598141408176e59870880b92",
            "dedabd3236e84164b3682929b0dec997",
            "b62a050fa13b4b4cb94821c34a781982",
            "aa980ba89ff2416f971f799367ff808c",
            "43c017acda504247b7019a7e4ed7cfa0",
            "8a598bf4e25a457d850c9ab8f61a3d7f",
            "b3be2a94140f4585b66bed0dc8354d82",
            "8c5b5229b356441096571548ba1b238f",
            "277a8d7e669e4a419308845cd212bc3c",
            "e1884fe85d9544d695a4cda6ff625d1d",
            "f97ddf3c70e04b38a5f5dbf8519338bf",
            "abb316aafa7e4142896cd5be79add40b",
            "06ef2c252438449786e6e53bb0ff71af",
            "e793f20e6c444bd8bdea619444d73775",
            "e2d8a2ee0591448980c5c0c4128f927c",
            "65b30315baa243289cff793a390256a1",
            "97cc701da613482b8116cb8d7a918235",
            "2bac1dae5e99478e943233a378b692ad",
            "383723ff9b8c49bdac77ba69452e976d",
            "5e87fcf97ea8488b8c0e5c628fe1c15b",
            "207c4020a0854f9d9d5347a3e5a70700",
            "3fd004e8f1ca419d94e2c14b080f6591",
            "25bca1cd81404e9f891d2fb03b6252cb",
            "10df2301d7404c679519fcb2683aafb6",
            "05d7934a2664489d962df8817f4b62ba",
            "2123d9ee8e2d442c96768ade7b3c3e7a",
            "f05bfa06beca4fe19ae4d026565c4bee",
            "246622d76a864d69b6415aae695c4676",
            "7f97dee86c594c1b9df74979381ddfbf",
            "9b8172fa894d463fa646905a47504f69",
            "0067cbd3212d4982b282b60e9595d55c",
            "29595bfa29ee41759cd0c324da256324",
            "56d3be2c83a0462db1aba0ae90dbcf25",
            "e6938629282b4bc49ce99bc5969112ec",
            "c035d61a7b2746ccb7068044e77e00b7",
            "8337c953ff04444183492b842caef018",
            "faeff528fddc42618ca7ef6c46df66ce",
            "28a9394b23544f06b7fb67fb1c22b6c2",
            "2419d5d90d6b4175b18fb4044559a71b",
            "d219ff58b2ac4759b95c424feb68c2ff",
            "630f349b9728467bb80d22019d0cb9c8",
            "089cca1722c44e79ade5511e18d6cb3f",
            "e39daa366c9347c794609473f36c48a1",
            "827b9be43d8647c380d395c89127a0b6",
            "ffad9115768e48f49f7005185556a9d0",
            "0bafd751df08478798ccc1f83129adee",
            "61447be61da040f783db02af4a1c3f00",
            "4d6cf7fd213a4fcf9d4b0bb8a7353925",
            "357c701d705947bdb3277d79baf65b61",
            "d981f65753bf4ce9aab6e48c12dbbebf",
            "b310fa65d16f470f9aab3e2b378b35c6",
            "273d2d2efbe44670a912503be3ccab32",
            "121b55d7fff541adb2339fb9dba18ad5",
            "7a9dd9effebb4d7f80fb87a64a0c7524",
            "c9141e8889a84996be82c5278f2a7256",
            "56afaf64fa044ee29f72a85b54e83817",
            "ae5ac601e6a94265a5078bd82918a979",
            "7ab37193e0554b868d8cd2c89c1a75b6",
            "af3bf8baa629407db57e30d859a14dc9",
            "1c45f801d2264ab19e8b4668aef7a883",
            "7ac787938e3142268aa87002d353fd6e",
            "ec86ae2ba7284a0582175b0211195e35",
            "7244f69df3954192852a6f779ee694f9",
            "5647e09dfc6341308f6d368e4f9d1245",
            "46a7db90c743450da24cefc1301d6e9f",
            "134dd13cbdf34877b5c762058438e407",
            "802aab6089df4829b5e995a70744aea2",
            "55646c5d5b4b4d6f88a571a01be7646c",
            "5ad1e2bb92674c7bbe126c22ed44f4ac",
            "fc3a18476f25474d8be79a6e76ea73d4",
            "6dd8cb25887b4a2ca45f0385904fd8e8",
            "b33038646f6d4e95ba809c59102d7659",
            "6f60bbe7c613405ebb1e160583288b86",
            "72c3a43cd32f4019b5068f00bb853795",
            "eba3e75c58124a07a593b5ef8ae37142",
            "49b3758e6aae4266b27358dafa67a472",
            "01c5b920f9e6402393421b742d292511",
            "a5f85d2dd8684c9789dc8ccbe8a2b7a8",
            "d75c8412c8bf42f1811c5db9f84351d9",
            "d9b18b6b0a2f47be8f6f4a7312b70169",
            "130a979d290f4c73ae5e6159709fad30",
            "817088136b6a4a7c95fb20afcc340a4d",
            "d98add0b681845cdb31406350427a10a",
            "8be01a0b885843db8dbcfc6e18d8f857",
            "ad4be70826ab47b8b2ec837dae7d49d1",
            "6f88f589da5746b0b3012cab3f04fd2b",
            "37b3bdd457704d1d9be66a4b043cdca3",
            "d1b2e4262698480a84084d29782a24be",
            "b411dfa24ec342be82fc54d133d4fca8",
            "d7f83bf7b5654fd3bd43aea45cecc99f",
            "e37563ca76c8444e80217363894035e7",
            "2e1e8db1efac4c66b36a59e0cce7ae6a",
            "67c1e871843044e886536f82f6bf4386",
            "1c37f663fe1341a7b8f0264d702ed458",
            "5370bb0d91254e6b80067f58338769ca",
            "20a5ff7425ea4398bd59f2e3d3c1665a",
            "f3b8c0f0e53740698ff48b2ec0faf5f5",
            "b04bb0dbe8a34a3a9ecfd3af6137b4aa",
            "330644e2c3f64d89b2d4c215731ba220",
            "cffe6b9f73ea43c59b04dc2f5bd43f3d",
            "3185bd21d3ac45a68cd4a4fadd4b4cbf",
            "60a318fd36784acabdbfddeb08aefbe9",
            "f6475ea2117c4d318099aa428711e207",
            "25765a2f8c5f46eeb1a1935be9489942",
            "7f09e71b69414d5ea8d33775b0e49afb",
            "389a979e51fa4e80b73fe0ea91f4d44a",
            "6699b8425e0840f5b056560a53f8d264",
            "7289bf695a444051a3df4c774f9e89e6",
            "13a53cbf190b416aa389679b3cf99d2c",
            "e1cb224eb69b44cbad98971c199b51b2",
            "33871bff4a434e4eb4878577dda7d87f",
            "e46fc4ba9e6340f39a2d65bc9169f721",
            "98721607aae74677927ed54e4ec71ea4",
            "5a1e0f3563c240f492d87faa3cdebc35",
            "b035483702824f5b872543de152792f6",
            "a7c4fb1c78104eb3b132083e8dfbb2f5",
            "151cb3174e764d22b756df1a8cfebab3",
            "f28c4f7c30224cb6a898a2a726ae5963",
            "6de86a633fa144d284d217d2b7e1e956",
            "ef02edc83b284f73b3cf83d8196c3095",
            "e9f3b6e600d04116a51953300826b7dc",
            "7209cf92aa4e457a8b088d5156dfd148",
            "045cd436d6684e488b4f2ea68fdcb505",
            "0da253259aa143b3989b9d2b35b5256d",
            "754fda1ec6b14bc784ea7a863c9856c3"
          ]
        },
        "id": "RYHs1I6G_mPe",
        "outputId": "ad290225-a9a3-46c7-e57f-f5371b691767"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b26f5f9c1e14e1b87a46df9ecdc8ce0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1440ac4325d04b67a2f0cca0bf707052",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07b5cf374657429698e0571fa531afe7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43c017acda504247b7019a7e4ed7cfa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65b30315baa243289cff793a390256a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f05bfa06beca4fe19ae4d026565c4bee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28a9394b23544f06b7fb67fb1c22b6c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "357c701d705947bdb3277d79baf65b61",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c45f801d2264ab19e8b4668aef7a883",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6dd8cb25887b4a2ca45f0385904fd8e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "817088136b6a4a7c95fb20afcc340a4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67c1e871843044e886536f82f6bf4386",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25765a2f8c5f46eeb1a1935be9489942",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b035483702824f5b872543de152792f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer,util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "# pre-trained models that are capable of converting input sentences into fixed-size vectors\n",
        "#  sentence embedding is a fixed-size vector representation that captures the semantic meaning of a sentence in a high-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dix140gA5ifx",
        "outputId": "270d75e6-7b81-4f58-a0b9-fdf65399fe49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence 1: \n",
            "\"computing\" refers to the process of using computers and computer systems to perform various\n",
            "tasks, such as data processing, information storage, and solving complex problems .\n",
            "Sentence 2: this computing can be either done in serial way known as serial computing or in parallel wa y\n",
            "known as parallel computing\n",
            "serial computing:\n",
            "serial computing refers to traditional computing where tasks are executed sequentially, one after\n",
            "the other, using a single processor.\n",
            "Sentence 3: in serial computing, each instruction or task must wait for the\n",
            "previous one to complete before it can be executed.\n",
            "Sentence 4: this approach limits the speed and efficiency\n",
            "of processing, especially when dealing with complex or time -consuming tasks.\n",
            "Sentence 5: computing\n",
            "serial or\n",
            "sequential parallel example of serial computing : consider a task of sorting a large dataset of numbers in ascending\n",
            "order.\n",
            "Sentence 6: in a serial computing environment, a single processor would go through the entire dataset,\n",
            "comparing and rearranging numbers one pair at a time until the entire dataset is sorted.\n",
            "Sentence 7: this process\n",
            "occurs sequentially, and each comparison and rearrangement must wait for the previous one to\n",
            "finish.\n",
            "Sentence 8: parallel computing\n",
            "parallel computing is a type of computation in which multiple processors or computers work together\n",
            "to solve a problem.\n",
            "Sentence 9: instead of one single processor handling the entire task, parallel computing divides\n",
            "the task into smaller sub -tasks that can be processed simultaneously.\n",
            "Sentence 10: this simultaneous processing\n",
            "can lead to significant improvements in computational speed and efficien cy.\n",
            "Sentence 11: example of parallel computing: using the same example of sorting a large dataset, parallel computing\n",
            "would involve dividing the dataset into smaller chunks, and each chunk is sorted independently by a\n",
            "separate processor.\n",
            "Sentence 12: these processors work in parall el, sorting their respective chunks simultaneously.\n",
            "Sentence 13: once all processors have completed sorting their portions, the sorted chunks can be combined to\n",
            "produce the final sorted dataset.\n",
            "Sentence 14: definition parallel computing is the practice of identifying and exposing parallelism in algorithms,\n",
            "expressing this in our software, and understanding the costs, benefits, and limitations of the chosen\n",
            "implementation.\n",
            "Sentence 15: benefits of parallel computing\n",
            "\n",
            " faster run time with more compute cores: parallelization involves dividing a task into\n",
            "smaller sub -tasks that can be executed simultaneously, utilizing multiple cores to process the\n",
            "data.\n",
            "Sentence 16: this approach can significantly reduce the time required to complete the task, as each\n",
            "core works on a separate portion of the problem concurrently.\n",
            "Sentence 17:  larger problem sizes with more compute nodes : with more nodes, you can break down your\n",
            "problem into smaller pieces that each node can work on simultaneously, which is especially\n",
            "beneficial for handling larger datasets and more complex simulations .\n",
            "Sentence 18:  energy efficiency by doing more with less: in the context of parallel computing, the concept\n",
            "of \"doing more with less\" often revolves around optimizing energy efficiency while achieving\n",
            "better computational performance.this can be achieved by making use of dynamic resource\n",
            "allocation and workload consolidation to ensure that the number of processors used is\n",
            "proportional to the workload.\n",
            "Sentence 19: turn off or put to sleep any unused processors.\n",
            "Sentence 20: the energy consumption for your applicat ion can be estimated using the formula\n",
            "p = (n processors) × (r watts/processors) × (t hours)\n",
            "where p is the energy consumption, n is the number of processors, r is the thermal design\n",
            "power, and t is the application run time.\n",
            "Sentence 21:  scalability : parallel computing can be easily scaled by adding more processors, which further\n",
            "enhances performance.\n",
            "Sentence 22: serial computing does not scale in this manner, as it relies on a single\n",
            "processor.\n",
            "Sentence 23:  parallel computing can reduce costs: as technology advances, the cost of individual\n",
            "processors and memory decreases.\n",
            "Sentence 24: parallel computing systems can take advantage of these\n",
            "cost reductions, making it more economical to build high -performance computing clusters or\n",
            "data centers .\n",
            "Sentence 25: applications of parallel computing:\n",
            " scientific simulations: used in fields such as physics, chemistry, and engineering for\n",
            "complex simulations.\n",
            "Sentence 26:  big data processing: parallel computing is crucial in processing vast amounts of data in\n",
            "fields like data analytics and machine learning.\n",
            "Sentence 27:  weather forecasting: enables complex weather simulations and predictions.\n",
            "Sentence 28:  video and image processing: parallelism accelerates tasks like video rendering and\n",
            "image recognition.\n",
            "Sentence 29:  financial modelling : used for risk analysis, option pricing, and other complex financial\n",
            "calculations.\n",
            "Sentence 30: fundamental laws\n",
            "fundamental laws in parallel computing, such as amdahl's law and gustafson's law, are essential\n",
            "for understanding the limitations and possibilities of parallel processing.\n",
            "Sentence 31: these laws provide valuable\n",
            "insights into how the speedup of a parallel algorithm is affected by various factors .\n",
            "Sentence 32: what is speedup?\n",
            "Sentence 33: speedup in parallel computing refers to the performance improvement achieved by using multiple\n",
            "processors or computing resources to solve a problem compared to using a single processor.\n",
            "Sentence 34: it is a\n",
            "measure of how much faster a parallel algorithm or system can complet e a task compared to a serial\n",
            "(single -processor) implementation of the same task.\n",
            "Sentence 35: speedup is a crucial metric for evaluating the\n",
            "effectiveness of parallel computing systems.\n",
            "Sentence 36: the speedup ( s) can be calculated using the following formula:\n",
            "s=tserial / tparallel\n",
            "where:\n",
            " tserial is the execution time of the task using a single processor (serial execution time).\n",
            "Sentence 37: tparallel is the execution time of the task using multiple processors (parallel execution time).\n",
            "Sentence 38: a speedup value greater than 1 indicates that the parallel implementation is faster than the serial\n",
            "implementation.\n",
            "Sentence 39: ideally, in a perfectly parallelizable task, doubling the number of processors would ideally halve the execution time, resulting in a speedup of 2. however, achieving perfect linear\n",
            "spee dup is rare in real -world scenarios due to factors such as communication overhead, load\n",
            "balancing issues, and synchronization constraints between processors.\n",
            "Sentence 40: amdahl's law is a fundamental principle in parallel computing that expresses the potential\n",
            "speedup of a parallel algorithm as a function of the proportion of the algorithm that can be\n",
            "parallelized.\n",
            "Sentence 41: it was formulated by gene amdahl in 1967 and is represented by the fol lowing\n",
            "formula:\n",
            "where:\n",
            " speedup is the improvement in performance achieved by parallelizing a computation\n",
            "compared to executing it sequentially.\n",
            "Sentence 42:  p is the proportion of the algorithm that can be parallelized (a value between 0 and 1).\n",
            "Sentence 43:  s is the serial fraction\n",
            " n – no.of processors/nodes/cores\n",
            "amdahl's law highlights the limitations of parallel computing.\n",
            "Sentence 44: it states that the speedup of a\n",
            "program using multiple processors in parallel computing is limited by the sequential fraction of\n",
            "the program.\n",
            "Sentence 45: in other words, if only a portion of a program can be parallelized (the rest being\n",
            "inherently sequential), then no matter how many processors are added, there will always be a\n",
            "limit to the speedup that can be achieved.\n",
            "Sentence 46: for example, if 90% of a program can be parall elized (p = 0.9) and the parallel portion runs on\n",
            "5 processors , the maximum speedup that can be achieved according to amdahl's law is:\n",
            "speedup= 1/(0.1+(0.9/5))=3.57\n",
            "in this case, even though 90% of the program can be parallelized and runs on 5 processors , the\n",
            "maximum speedup achievable is approximately 3.57 times faster compared to the sequential\n",
            "execution due to the presence of the 10% sequential portion.\n",
            "Sentence 47: fig : speedup for a fixed -size problem according to amdahl’s law is shown as a function of the\n",
            "number of processors.\n",
            "Sentence 48: lines show ideal speedup when 100% of an algorithm is parallelized,\n",
            "and for 90%, 75%, and 50%.\n",
            "Sentence 49: amdahl’s law states that speedup is limited by the fractions of\n",
            "code that remain serial.\n",
            "Sentence 50: gustafson's law , formulated by john l. gustafson, provides a different perspective on parallel\n",
            "computing compared to amdahl's law.\n",
            "Sentence 51: unlike amdahl's law, which focuses on fixed problem sizes,\n",
            "gustafson's law takes into account varying problem sizes.\n",
            "Sentence 52: the basic idea behind g ustafson's law is\n",
            "that as the size of the problem increases, the impact of the parallelizable portion of the program\n",
            "becomes more significant, leading to better scalability.\n",
            "Sentence 53: in other words, with larger problem sizes,\n",
            "parallel systems can achieve higher lev els of speedup.\n",
            "Sentence 54: the formula for gustafson's law is as follows:\n",
            "speedup(n) = n – s * (n – 1) where n is the number of processors, and s is the serial fraction\n",
            "strong scaling and weak scaling are two different m etrics used to evaluate the performance of\n",
            "parallel computing systems, and they provide insights into how well a parallel algorithm or\n",
            "application can handle an increasing workload or an increasing number of processors.\n",
            "Sentence 55: here's a\n",
            "comparison of strong scalin g and weak scaling:\n",
            "strong scaling:\n",
            "definition: strong scaling measures how the execution time of a fixed problem size decreases\n",
            "as the number of processors increases.\n",
            "Sentence 56: in other words, it assesses how well a parallel system\n",
            "performs when the size of the problem remains constant, but the number of proces sors used\n",
            "to solve the problem increases.\n",
            "Sentence 57: objective: the goal of strong scaling is to reduce the execution time for a fixed problem size by\n",
            "utilizing more processors.\n",
            "Sentence 58: it aims to speed up the solution of a specific problem.\n",
            "Sentence 59: scenario: strong scaling is applicable when the size of the problem is fixed, and the aim is to\n",
            "solve that problem faster by employing additional processors.\n",
            "Sentence 60: 2. weak scaling:\n",
            "definition: weak scaling measures how the execution time changes as both the problem size\n",
            "and the number of processors increase proportionally.\n",
            "Sentence 61: in other words, it assesses how well a\n",
            "parallel system can handle larger workloads by adding more processors as the problem size\n",
            "grows.\n",
            "Sentence 62: objective: the goal of weak scaling is to maintain a constant workload per processor as the size\n",
            "of the problem and the number of processors increase.\n",
            "Sentence 63: it aims to solve larger problems in\n",
            "approximately the same amount of time per processor.\n",
            "Sentence 64: scenario: weak scaling is applicable when the problem size can be increased, and the aim is to\n",
            "handle larger workloads by distributing the computational load across a larger number of\n",
            "processors.\n",
            "Sentence 65: parallel approaches (flynn’s classification)\n",
            "\n",
            "flynn's classification is essential in the field of parallel computing because it provides a framework\n",
            "for understanding and categorizing different types of computer architectures based on the number\n",
            "of instruction streams and data streams.\n",
            "Sentence 66: this classifica tion is named after michael j. flynn, who\n",
            "introduced it in 1966. flynn’s taxonomy is a useful tool for understanding different types of\n",
            "computer architectures and their strengths and weaknesses.\n",
            "Sentence 67: the taxonomy highlights the importance of parallelism in modern computing and shows how\n",
            "different types of parallelism can be exploited to improve performance.\n",
            "Sentence 68: it helps in designing and\n",
            "analyzing parallel processing systems\n",
            "\n",
            "1. single instruction single data (sisd ): in a sisd architecture, there is a single processor\n",
            "that executes a single instruction stream and operates on a single data stream.\n",
            "Sentence 69: this is the\n",
            "simplest type of computer architecture and is used in most traditional computers.\n",
            "Sentence 70: 2. single instruction multiple data (simd ): in a simd architecture, there is a single\n",
            "processor that executes the same instruction on multiple data streams in parallel.\n",
            "Sentence 71: this type of\n",
            "architecture is used in applications such as image and signal processing.\n",
            "Sentence 72: 3. multiple instruction single data (misd ): in a misd architecture, multiple processors\n",
            "execute different instructions on the same data stream.\n",
            "Sentence 73: this type of architecture is not\n",
            "commonly used in practice, as it is difficult to find applications that can be decomposed into\n",
            "indepen dent instruction streams.\n",
            "Sentence 74: 4. multiple instruction multiple data (mimd ): in a mimd architecture, multiple processors\n",
            "execute different instructions on different data streams.\n",
            "Sentence 75: this type of architecture is used in\n",
            "distributed computing, parallel processing, and other high -performance computing applications.\n",
            "Sentence 76: parallel strategies\n",
            "parallel strategies\" typically refer to techniques and methods for parallel processing, which is the\n",
            "simultaneous execution of multiple tasks or processes to improve the efficiency and performance\n",
            "of a computer system.\n",
            "Sentence 77: parallel strategies are commonly used in various computing domains, such\n",
            "as high -performance computing and distributed systems, to speed up computations and handle\n",
            "large volumes of data.\n",
            "Sentence 78: here are some common parallel s trategies:\n",
            "data parallel approach\n",
            "data parallelism involves performing the same operation on multiple data elements simultaneously.\n",
            "Sentence 79: this strategy is often used in applications where the same operation can be applied to different\n",
            "pieces of data independent ly.\n",
            "Sentence 80: scenario : imagine you're running a data analysis task on a large dataset of customer reviews for a\n",
            "product.\n",
            "Sentence 81: your goal is to perform sentiment analysis on each review to determine if it's positive,\n",
            "negative, or neutral.\n",
            "Sentence 82: the sentiment analysis process is computationally intensive, and you want to\n",
            "speed it up using data parallelism.\n",
            "Sentence 83: data parallelism in sentiment analysis :\n",
            "1. data preparation : you have a dataset of 1,000,000 customer reviews.\n",
            "Sentence 84: to apply data\n",
            "parallelism, you divide this dataset into smaller, non-overlapping subsets.\n",
            "Sentence 85: let's say you split\n",
            "it into four subsets, each containing 250,000 reviews.\n",
            "Sentence 86: 2. parallel processing : you have a sentiment analysis model that can analyze reviews.\n",
            "Sentence 87: you set\n",
            "up four separate processing units (e.g., cpu cores or machines in a cluster), each responsible\n",
            "for analyzing one subset of reviews.\n",
            "Sentence 88: each processing unit loads its assigned subset of data.\n",
            "Sentence 89: 3. analysis : each processing unit applies the sentiment analysis model to its subset of reviews\n",
            "independently and simultaneously.\n",
            "Sentence 90: for in stance:\n",
            " processing unit 1 analyzes reviews 1 to 250,000.\n",
            "Sentence 91:  processing unit 2 analyzes reviews 250,001 to 500,000.\n",
            "Sentence 92:  processing unit 3 analyzes reviews 500,001 to 750,000.\n",
            "Sentence 93:  processing unit 4 analyzes reviews 750,001 to 1,000,000.\n",
            "Sentence 94: 4. aggregation : as each processing unit finishes its analysis, it generates results, such as counts\n",
            "of positive, negative, and neutral reviews within its subset.\n",
            "Sentence 95: these results are temporarily\n",
            "stored.\n",
            "Sentence 96: 5. combining results : after all processing units have completed their work, you combine the\n",
            "results.\n",
            "Sentence 97: you sum up the counts from each processing unit to get the overall sentiment\n",
            "analysis results for the entire dataset.\n",
            "Sentence 98: task parallelism(main -worker approach)\n",
            "task parallelism involves executing multiple independent tasks or processes in parallel.\n",
            "Sentence 99: each task\n",
            "can perform different operations and may not necessarily operate on the same data.\n",
            "Sentence 100: task\n",
            "parallelism is common in applications where different tasks can be performed concurrently without\n",
            "dependencies between them.\n",
            "Sentence 101: in the main -worker approach, one processor schedules and distributes the tasks for all the workers,\n",
            "and each worker checks for the next work item as it returns the previous completed task .\n",
            "Sentence 102: example: web server handling requests\n",
            "consider a web server handling incoming http requests.\n",
            "Sentence 103: each i ncoming request is an independent\n",
            "task that can be processed concurrently.\n",
            "Sentence 104: the tasks include tasks like parsing the request, querying\n",
            "the database, and generating the response.\n",
            "Sentence 105: in a task parallelism scenario:\n",
            "1. task 1: parsing request\n",
            " this task involves pars ing the incoming http request to extract information like the\n",
            "requested url, parameters, and headers.\n",
            "Sentence 106: 2. task 2: database query\n",
            " this task involves querying a database to fetch data related to the request, such as\n",
            "user information or product details.\n",
            "Sentence 107: 3. task 3: generating response\n",
            " this task involves generating an html response based on the parsed request and\n",
            "data retrieved from the database.\n",
            "Sentence 108: in a task parallelism setup, these tasks can be executed concurrently by multiple threads or\n",
            "processes, allowing the server to handle multiple incoming requests simultaneously without waiting\n",
            "for one task to complete before starting the next.\n",
            "Sentence 109: bucket -brigade parallelism :\n",
            "a bucket brigade is a method of manually transporting items or materials from one location to\n",
            "another by forming a line of people, each of whom carries an item and passes it to the next person.\n",
            "Sentence 110: this technique is similar to how buckets of water might be passed along a line of people to put out a\n",
            "fire, which is where the term \"bucket brigade\" originated.\n",
            "Sentence 111: in parallel computing, the concept of bucket -brigade parallelism involves breaking down a task into\n",
            "smaller subtasks, where each subtask is processed independently and passed to the next processing\n",
            "unit for further computation.\n",
            "Sentence 112: this technique allows for efficient parallel processing of tasks and is\n",
            "often used in scenarios where tasks can be divided into smaller, manageable parts.\n",
            "Sentence 113: example: manufacturing assembly line\n",
            "let's say we have a manufacturing assembly line for producing smartphones.\n",
            "Sentence 114: the assembly line\n",
            "consists of three stages: a, b, and c. each stage represents a specific task in the smartphone\n",
            "assembly process.\n",
            "Sentence 115: 1. stage a - component assembly :\n",
            " worker a assembles the basic components of the smartphone, such as the circuit\n",
            "board, battery, and display.\n",
            "Sentence 116: once worker a finishes assembling a smartphone, it passes it\n",
            "to stage b.\n",
            "Sentence 117: 2. stage b - software installation :\n",
            " worker b installs the operating system and necessary software onto the smartphone\n",
            "assembled by worker a. after software installation, the smartphone is passed to stage c.\n",
            "3. stage c - quality control and packaging :\n",
            " worker c checks the smartphone for quality control, ensuring that all components\n",
            "are working correctly an d the software is functioning as intended.\n",
            "Sentence 118: if the smartphone passes\n",
            "quality control, it is packaged and prepared for shipment.\n",
            "Sentence 119: in this example, each stage (a, b, and c) represents a processing step, similar to the stages in a\n",
            "bucket -brigade parallelism sce nario.\n",
            "Sentence 120: parallel speedup versus comparative speedups.\n",
            "Sentence 121: parallel speedup and comparative speedup are two different metrics used to evaluate the performance\n",
            "improvement achieved by parallel processing.\n",
            "Sentence 122: parallel speedup measures how much faster a parallel algorithm runs compared to its sequential\n",
            "(single -processor) counterpart.\n",
            "Sentence 123: it quantifies the performance improvement gained by using multiple\n",
            "processing units in parallel.\n",
            "Sentence 124: parallel speedup is calculated using the followi ng formula:\n",
            "parallel speedup=sequential execution time/parallel execution\n",
            "in this formula:\n",
            " sequential execution time is the time taken by the algorithm to execute sequentially on a\n",
            "single processor.\n",
            "Sentence 125:  parallel execution time is the time taken by the paralle l algorithm to execute on multiple\n",
            "processors.\n",
            "Sentence 126: comparative speedup : comparative speedup is between architectures.\n",
            "Sentence 127: this is usually a\n",
            "performance comparison between two parallel implementations or other comparison between\n",
            "reasonably constrained sets of hardware.\n",
            "Sentence 128: for example, it may be between a parallel mpi\n",
            "implementation on all the cores of the node of a computer versus the gpu(s) on a node\n",
            "how parallel computing works\n",
            "as a developer, you are responsible for the application software layer, which includes your source code.\n",
            "Sentence 129: in the source code, you make choices about the programming language and parallel software interfaces\n",
            "you use to leverage the underlying hardware.\n",
            "Sentence 130: additionally, you decide how to break up your work into\n",
            "parallel units.\n",
            "Sentence 131: a compiler is designed to translate your source code into a form the hardware can execute.\n",
            "Sentence 132: with these instructions at hand, an os manages executing these on the computer hardware.\n",
            "Sentence 133: parallel approach models are used to express parallelization in an application software layer\n",
            "that gets mapped to the computer hardware through the compiler and the os.\n",
            "Sentence 134: parallel computing\n",
            "approaches involve various models and paradigms that define how tasks are divided, coordinated, and\n",
            "executed in parallel systems.\n",
            "Sentence 135: here are some common parallel computing approach models:\n",
            "hardware models\n",
            "distributed memory architecture: a cross -node parallel method :\n",
            "parallel approach models\n",
            "hardware models\n",
            "distributed memory architecture:\n",
            "a cross -node parallel method :\n",
            "shared memory architecture: an\n",
            "on-node parallel method\n",
            "vector units: multiple operations\n",
            "with one instruction\n",
            "accelerator device: a special -\n",
            "purpose add-on processor\n",
            "software models\n",
            "process -based parallelization :\n",
            "message passing\n",
            "thread -based parallelization :\n",
            "shared data via memory\n",
            "vectorization : multiple\n",
            "operations with one instruction\n",
            "stream processing : through\n",
            "specialized processors\n",
            "distributed memory architecture, also known as distributed memory parallelism, is a parallel\n",
            "computing method where multiple processors or nodes in a cluster have their own private memory.\n",
            "Sentence 136: these nodes are connected via a network, and they communicate and coordinate with each other\n",
            "by passing messages.\n",
            "Sentence 137: in this architecture, each node opera tes independently and has its own local\n",
            "memory, and data sharing is achieved explicitly through message passing.\n",
            "Sentence 138: in the context of distributed memory architecture, a \"cross -node parallel method\" refers to parallel\n",
            "processing techniques that involve distributing tasks across multiple nodes in a cluster.\n",
            "Sentence 139: each node\n",
            "works on its subset of the data or a specific portion of the computation.\n",
            "Sentence 140: communication and\n",
            "coordination between nodes are essential, as tasks often depend on results or data computed on\n",
            "othe r nodes.\n",
            "Sentence 141: shared memory architecture: an on-node parallel method\n",
            "in shared memory architecture, multiple processors or cores share a single, unified memory space.\n",
            "Sentence 142: this shared memory can be accessed and modified by any processor within the system.\n",
            "Sentence 143: on -node\n",
            "parallelism, within the context of shared memory architecture, ref ers to parallel processing\n",
            "techniques that occur on a single computing node.\n",
            "Sentence 144: in this approach, multiple threads or processes\n",
            "run concurrently on the same node, accessing shared memory to perform computations .\n",
            "Sentence 145: vector units: multiple operations with one instruction\n",
            "vector units, also known as vector processors, are specialized hardware units that can perform\n",
            "multiple operations with a single instruction.\n",
            "Sentence 146: these units are designed to process vectors, which are\n",
            "arrays of data elements, simultaneously.\n",
            "Sentence 147: vecto r processing is particularly useful in scenarios where\n",
            "the same operation needs to be performed on a large set of data elements .\n",
            "Sentence 148: vector processing example with four array elements operated on simultaneously\n",
            "accelerator device: a special -purpose add-on processor\n",
            "gpus come in two varieties: integrated and discrete.\n",
            "Sentence 149: discrete or dedicated gpus typically have a\n",
            "large number of streaming multiprocessors and their own dram.\n",
            "Sentence 150: accessing data on a discrete gpu\n",
            "requires communication over a pci bus\n",
            "an accelerator device, often referred to as an accelerator, is a specialized hardware component\n",
            "(gpu) designed to perform specific types of computational tasks or workloads efficiently.\n",
            "Sentence 151: accelerators are typically used in conjunction with a central processin g unit (cpu) and are especially\n",
            "well-suited for workloads that can benefit from parallel processing and offloading certain tasks from\n",
            "the cpu.\n",
            "Sentence 152: accelerators are sometimes called \"add -on processors\" because they augment the\n",
            "processing capabilities of a syste m.\n",
            "general heterogeneous parallel architecture model\n",
            "now let’s combine all of these different hardware architectures into one model .\n",
            "Sentence 153: two nodes, each\n",
            "with two cpus, share the same dram memory.\n",
            "Sentence 154: each cpu is a dual -core processor with an\n",
            "integrated gpu.\n",
            "Sentence 155: a discrete gpu on the pci bus also attaches to one of the cpus.\n",
            "Sentence 156: though the cpus\n",
            "share main memory, these are commonly in different non -uniform memory access (numa)\n",
            "regions.\n",
            "Sentence 157: this means that accessing the second cpu’s memory is more expensive than getting at it’s\n",
            "own memory\n",
            "fig 5: a general heterogeneous parallel architecture model consisting of two nodes connected by\n",
            "a network.\n",
            "Sentence 158: each node has a multi -core cpu with an integrated and discrete gpu and some\n",
            "memory (dram).\n",
            "Sentence 159: software models\n",
            "the programmer must first expose the parallelization, determine the best technique to operate in\n",
            "parallel, and then explicitly direct its operation in a safe, correct, and efficient manner.\n",
            "Sentence 160: the following\n",
            "methods are the most common techniques for parallelization\n",
            " process -based parallelization : message passing\n",
            "process -based parallelization, particularly through message passing, is a common approach in\n",
            "parallel computing.\n",
            "Sentence 161: it involves dividing a task into multiple processes or threads that run\n",
            "independently on separate computing nodes or cores.\n",
            "Sentence 162: these processes communicate and\n",
            "coordinate with each other by sending and receiving messages.\n",
            "Sentence 163: message passing is a method of\n",
            "inter -process communication where data and instructions are exchanged between processes to\n",
            "synch ronize and share information.\n",
            "Sentence 164: this approach is widely used in distributed memory systems,\n",
            "such as clusters and supercomputers.\n",
            "Sentence 165: fig 6 : the message passing library spawns processes.\n",
            "Sentence 166: the os places the processes on the cores\n",
            "of two nodes.\n",
            "Sentence 167: the question marks i ndicate that the os controls the placement of the processes\n",
            "and can move these during run time as indicated by the dashed arrows.\n",
            "Sentence 168: the os also allocates\n",
            "memory for each process from the node’s main memory\n",
            " thread -based parallelization : shared data via memory\n",
            "thread -based parallelization involves dividing a task into multiple threads that share the same\n",
            "memory space within a single process.\n",
            "Sentence 169: these threads can run concurrently on multiple cpu cores,\n",
            "and they communicate and coordinate by accessing shared data in the shared memory.\n",
            "Sentence 170: this\n",
            "approach is commonly used in multi -core processors and symmetric multiprocessing (smp) systems.\n",
            "Sentence 171: fig 7: the application process in a thread -based approach to parallelization spawns\n",
            "threads.\n",
            "Sentence 172: the threads are restricted to the node’s domain.\n",
            "Sentence 173: the question marks show that\n",
            "the os decides where to place the threads.\n",
            "Sentence 174: some memory is shared between threads.\n",
            "Sentence 175:  vectorization : multiple operations with one instruction\n",
            "vectorization is a parallel computing technique that enables processors to perform multiple\n",
            "operations with a single instruction.\n",
            "Sentence 176: it takes advantage of simd (single instruction, multiple data)\n",
            "capabilities found in modern processors, including cpus and gpus.\n",
            "Sentence 177: simd allows a single instruction\n",
            "to operate on multiple da ta elements simultaneously, which can significantly accelerate\n",
            "computations involving large datasets.\n",
            "Sentence 178:  stream processing : through specialized processors\n",
            "stream processing, often referred to as stream computing or data stream processing, is a computing\n",
            "paradigm where data is continuously processed as it is generated or ingested, rather than being\n",
            "stored in traditional databases or file systems.\n",
            "Sentence 179: stream proce ssing is particularly useful for handling\n",
            "large volumes of real -time data from various sources, such as sensors, social media, financial\n",
            "transactions, and iot devices.\n",
            "Sentence 180: specialized processors designed for stream processing accelerate the\n",
            "analysis and manipu lation of data streams, ensuring timely and efficient processing\n",
            "in the stream processing approach, data and compute kernel are offloaded to the gpu and its\n",
            "streaming multiprocessors.\n",
            "Sentence 181: processed data, or output, transfers back to the cpu for file io or other\n",
            "work\n",
            "sample application\n",
            "we start with a 2d problem domain of a region of space.\n",
            "Sentence 182: for purposes of illustration, we will use\n",
            "a 2d image of the krakatau volcano as our example.\n",
            "Sentence 183: the goal of our calculation could be to model\n",
            "the volcanic plume, the resulting tsunami, or the early detection of a volcanic eruption using\n",
            "machine learning.\n",
            "Sentence 184: for all of these options, calculation speed is critical if we want real -time result s\n",
            "to inform our decisions.\n",
            "Sentence 185: 1. discretize (break up) the problem into smaller cells or elements\n",
            "2 .\n",
            "Sentence 186: define a computational kernel (operation) to conduct on each element of the mesh\n",
            "3. add the following layers of parallelization on cpus and gpus to perfo rm the calculation:\n",
            "vectorization —work on more than one unit of data at a time\n",
            "4. threads —deploy more than one compute pathway to engage more processing cores\n",
            "5. processes —separate program instances to spread out the calculation into separate memory\n",
            "spaces\n",
            "6. off -loading the calculation to gpus —send the data to the graphics processor to calculate\n",
            "step 1: discretize the problem into smaller cells or elements\n",
            "the domain is discretized into cells.\n",
            "Sentence 187: for each cell in the computational domain, properties such as\n",
            "wave height, fluid velocity, or smoke density are solved for according to physical laws.\n",
            "Sentence 188: ultimately, a\n",
            "stencil operation or a matrix -vector system represents this discrete scheme\n",
            "step 2: define a computational kernel, or operation, to conduct on each element of the mesh\n",
            "the calculations on this discretized data are often some form of a stencil operation, so -called because\n",
            "it involves a pattern of a djacent cells to calculate the new value for each cell.\n",
            "Sentence 189: this can be an average\n",
            "(a blur operation, which blurs the image )gradient (edge -detection, which sharpens the edges in the\n",
            "image) or another more complex operation associated with solving physical sys tems described by\n",
            "partial differential equations (pdes)\n",
            "step 3: vectorization to work on more than one unit of data at a time\n",
            "some processors have the ability to operate on more than one piece of data at a time; a capability\n",
            "referred to as vector operations.\n",
            "Sentence 190: the shaded blocks in figure illustrate how multiple data values\n",
            "are operated on simultaneously in a vector unit in a proce ssor with one instruction in one clock\n",
            "cycle.\n",
            "Sentence 191: step 4: threads to deploy more than one compute pathway to engage more processing cores\n",
            "because most cpus today have at least four processing cores, we use threading to operate the cores\n",
            "simultaneously acros s four rows at a time.\n",
            "Sentence 192: step 5: processes to spread out the calculation to separate memory spaces\n",
            "we can further split the work between processors on two desktops, often called nodes in parallel\n",
            "processing.\n",
            "Sentence 193: when the work is split across nodes, the memory spaces for each node are distinct and\n",
            "separate.\n",
            "Sentence 194: step 6: off -loading the calculation to gpus\n",
            "on a gpu, the vector length is much larger than on a cpu.\n",
            "Sentence 195: here, 8×8 tiles are distributed across\n",
            "gpu work groups.\n",
            "Sentence 196: performance limits and profiling\n",
            "in parallel processing, understanding performance limits and profiling the application are crucial\n",
            "steps to optimize the execution of parallel programs.\n",
            "Sentence 197: performance limits refer to the maximum achievable performance of a computing system or\n",
            "application un der specific conditions.\n",
            "Sentence 198: these limits are determined by various factors and constraints\n",
            "and play a crucial role in understanding the capabilities and limitations of a system.\n",
            "Sentence 199: understanding\n",
            "these performance limits is essential for designing efficient algor ithms, optimizing software, and\n",
            "choosing appropriate hardware configurations.\n",
            "Sentence 200: it also guides researchers and engineers in\n",
            "developing new technologies to overcome existing limitations and improve overall computing\n",
            "performance.\n",
            "Sentence 201: profiling tools are used to ga ther detailed information about the behavior of a parallel program.\n",
            "Sentence 202: by\n",
            "understanding performance limits, utilizing profiling tools, and optimizing the code based on the\n",
            "profiling results, developers can enhance the efficiency of parallel applications, lea ding to improved\n",
            "speedup and overall performance.\n",
            "Sentence 203: application’s potential performance limits\n",
            " flops (floating -point operations)\n",
            " ops (operations) that include all types of computer instructions\n",
            " memory bandwidth: rate at which the data is transferred\n",
            " memory latency: time required for the first byte or word of data to be transferred\n",
            " instruction queue (instruction cache)\n",
            " networks\n",
            " disk\n",
            " machine balance: number of flops executed /memory bandwidth\n",
            " arithmetic intensity : number of flops executed per memory operation\n",
            "all of these limitations can be divided into two major categories:.\n",
            "Sentence 204: speeds are how fast operations can\n",
            "be done.\n",
            "Sentence 205: it includes all types of computer operations.\n",
            "Sentence 206: but to be able to do the operations, you must\n",
            "get the data there.\n",
            "Sentence 207: this is where feeds come in.\n",
            "Sentence 208: feeds include the memory bandwidth through the\n",
            "cache hierarchy, as well as network and disk bandwidth .\n",
            "Sentence 209: for many applications, the memory bandwidth limit can be difficult especially dealing with non-\n",
            "contiguous bandwidth.it is also known as strided memory access or non -contiguous memory\n",
            "access, refers to the manner in which data elements are accessed in memory.\n",
            "Sentence 210: in contrast to\n",
            "contiguous memory access, where elements are stored in consecutive memory locations, non -\n",
            "contiguous memory access invo lves accessing elements that are not stored sequentially in memory.\n",
            "Sentence 211: non -contiguous memory access:\n",
            "now, consider a situation where the array elements are scattered in memory with a stride of 2. this\n",
            "is a non -contiguous memory access pattern:\n",
            "in this case, accessing every second element (stride = 2) would mean accessing memory locations 1,\n",
            "2, 3, 4, 5, etc., but the elements are not stored sequentially.\n",
            "Sentence 212: when your program needs to access such non -contiguous elements, it may lead to inefficiencie s\n",
            "due to increased cache misses and a higher likelihood of accessing data from main memory rather\n",
            "than the faster cache memory.\n",
            "Sentence 213: determine your hardware capabilities:\n",
            "to determine the performance of hardware the following metrics are used :\n",
            " the rate at which floating -point operations can be executed (flops/s)\n",
            " the rate at which data can be moved between various levels of memory (gb/s)\n",
            " the rate at which energy is used by your application (watts)\n",
            "in determining hardware performance and calculating the metrics , we use a mixture of theoretical\n",
            "and empirical measurements .\n",
            "Sentence 214: theoretical measurements provide an upper limit to what a system can achieve.\n",
            "Sentence 215: for instance, in\n",
            "parallel computing, theoretical analysis can reveal the maximum speedup or efficiency that a\n",
            "parallel algorithm can achieve in an ideal scenario .\n",
            "Sentence 216: real -world validation is done by empirical measurements , they provide concrete evidence of how\n",
            "a system performs under real -world conditions, accounting for various factors like i/o operations,\n",
            "network latency, and concurrency issues.\n",
            "Sentence 217: one of the best tools for understanding the hardware you run is the lstopo program (graphical view)\n",
            "and lscpu for text view .\n",
            "Sentence 218: lstopo is bundled with the hwloc package that com es with nearly every mpi\n",
            "distribution.\n",
            "Sentence 219: this command outputs a graphical view of the hardware on your system.\n",
            "Sentence 220: figure below\n",
            "shows the output for a mac laptop in graphical view.\n",
            "Sentence 221: text view\n",
            "the information from the lscpu command and the /proc/cpuinfo file helps to determine the number\n",
            "of processors, the processor model, the cache sizes, and the clock frequency for the system\n",
            "calculating theoretical maximum flops\n",
            "theoretical flops=number of cores×clock speed×flops per cycle per core\n",
            "where:\n",
            " number of cores: this represents the total number of processor cores in the computing\n",
            "system.\n",
            "Sentence 222:  clock speed: this indicates the clock speed of each core in the system, typically measured\n",
            "in hertz (hz) or gigahertz (ghz).\n",
            "Sentence 223: it represents the number of cycles th e processor can\n",
            "execute per second.\n",
            "Sentence 224:  flops per cycle per core: this signifies the number of floating -point operations a core can\n",
            "perform in a single clock cycle.\n",
            "Sentence 225: modern processors often perform multiple flops per cycle\n",
            "due to features like simd (single inst ruction, multiple data) operations.\n",
            "Sentence 226: for example, let's consider a system with 4 cores, each operating at 3.0 ghz, and capable of\n",
            "executing 4 flops per cycle per core (assuming simd operations are utilized):\n",
            "theoretical flops=4 cores×3.0 ghz×4 flops per cycle per core\n",
            "theoretical flops=48 gflops\n",
            "the memory hierarchy and theoretical memory bandwidth\n",
            "we can calculate the theoretical memory bandwidth of the main memory using the memory chips\n",
            "specifications.\n",
            "Sentence 227: the general formula is b t = mtr × mc × tw × ns = data transfer rate × memory channels × bytes\n",
            "per access × sockets\n",
            "processors are installed in a socket on the motherboard.\n",
            "Sentence 228: the motherboard is the main system board\n",
            "of the computer, and the socket is the lo cation where the processor is inserted.\n",
            "Sentence 229: most motherboards\n",
            "are single -socket, where only one processor can be installed.\n",
            "Sentence 230: dual -socket motherboards are more\n",
            "common in high -performance computing systems.\n",
            "Sentence 231: two processors can be installed in a dual -socket\n",
            "motherb oard, giving us more processing cores and more memory bandwidth.\n",
            "Sentence 232: empirical measurement of bandwidth and flop\n",
            "the empirical bandwidth is the measurement of the fastest rate that memory can be loaded from\n",
            "main memory into the processor.\n",
            "Sentence 233: if a single byte of m emory is requested, it takes 1 cycle to retrieve\n",
            "it from a cpu register.\n",
            "Sentence 234: if it is not in the cpu register, it comes from the l1 cache.\n",
            "Sentence 235: if it is not in the l1\n",
            "cache, the l1 cache loads it from l2 and so on to main memory.\n",
            "Sentence 236: if it goes all the way to main\n",
            "memo ry, for a single byte of memory, it can take around 400 clock cycles.\n",
            "Sentence 237: this time required for the\n",
            "first byte of data from each level of memory is called the memory latency .\n",
            "Sentence 238: two different methods are used for measuring the bandwidth: the stream benchmark and the\n",
            "roofline model measured by the empirical roofline toolkit.\n",
            "Sentence 239: key differences:\n",
            " focus: stream primarily focuses on memory bandwidth, providing quantitative\n",
            "measurements.\n",
            "Sentence 240: in contrast, the roofline model provides a graphical representation of\n",
            "performance bottlenecks, considering both computational capabilities and memory\n",
            "bandwidth.\n",
            "Sentence 241:  representation: stream results in a numerical measurement (memory bandwidth in bytes\n",
            "per second), while the roofline model is a graphical representation that helps v isualize\n",
            "performance limitations.\n",
            "Sentence 242:  insights: stream provides detailed insights into memory subsystem performance, whereas\n",
            "the roofline model offers a high -level overview of an application's performance efficiency\n",
            "concerning hardware constraints.\n",
            "Sentence 243: calculatin g the machine balance between flops and bandwidth\n",
            "the machine balance is the flops divided by the memory bandwidth.\n",
            "Sentence 244: we can calculate both a theoretical machine balance (mb t) and an empirical machine balance (mb e)\n",
            "like so:\n",
            "mb t = f t / b t\n",
            "mb e = f e / b e\n",
            "characterizing your application: profiling\n",
            "now that you have some sense of what performance you can get with the hardware, you need to\n",
            "determine what are the performance characteristics of your application.\n",
            "Sentence 245: additionally, you should\n",
            "develop an understandi ng of how different subroutines and functions depend on each other .\n",
            "Sentence 246: profiling tools :\n",
            "using call graphs for hot -spot and dependency analysis\n",
            "in the context of parallel programming, call graphs are diagrams that represent the calling\n",
            "relationships between different functions or methods in a parallel program.\n",
            "Sentence 247: they illustrate how\n",
            "functions or tasks invoke each other and provide a visual representation of the program's control\n",
            "flow.\n",
            "Sentence 248: analyzing call graphs in parallel programming can provide valuable insights in to the program's\n",
            "structure, dependencies, and potential performance optimizations .\n",
            "Sentence 249: by analyzing these call graphs,\n",
            "developers can identify hot -spots —functions or tasks that consume a significant amount of\n",
            "computational time.\n",
            "Sentence 250: optimizing these hot -spots is e ssential for improving overall parallel program\n",
            "performance.\n",
            "Sentence 251: empirical measurement of processor clock frequency and energy consumption\n",
            "empirical measurement of processor clock frequency:\n",
            "1. profiling tools: profiling tools like intel vtune profiler or amd codexl can provide insights\n",
            "into various performance metrics, including processor clock frequency.\n",
            "Sentence 252: these tools often offer\n",
            "visualizations and detailed reports for better analysis.\n",
            "Sentence 253: 2. benchmarking suites: benchmarking tools like spec cpu benchmarks or hpc chal lenge\n",
            "benchmarks often include components that measure processor clock frequencies.\n",
            "Sentence 254: running\n",
            "these benchmarks can provide detailed information about the processor's performance\n",
            "characteristics.\n",
            "Sentence 255: 2. empirical measurement of energy consumption:\n",
            "1. power measureme nt tools: use power measurement tools and hardware devices to\n",
            "measure the power consumption of your system.\n",
            "Sentence 256: power meters and sensors can be attached\n",
            "to the system to measure real -time power usage.\n",
            "Sentence 257: tools like intel power gadget or linux's\n",
            "powerstat can help measure power usage.\n",
            "Sentence 258: 2. energy profilers: some profiling tools, like intel vtune profiler, also offer energy profiling\n",
            "capabilities.\n",
            "Sentence 259: they can provide insights into energy consumption patterns at different parts of\n",
            "your code.\n",
            "Sentence 260: these tools often correlate energ y consumption with specific functions or code\n",
            "regions.\n",
            "Sentence 261: tracking memory during run time\n",
            "tracking memory usage during runtime in parallel computing is crucial for optimizing performance,\n",
            "detecting memory leaks, and ensuring efficient memory management.\n",
            "Sentence 262: several techniques and tools\n",
            "can help you monitor memory usage in parallel applications.\n",
            "Sentence 263: here are some approaches to tracking\n",
            "memory during runtime in parallel computing environments:\n",
            "profiling tools:\n",
            "1. valgrind massif: valgrind is a powerful instrumentation framework.\n",
            "Sentence 264: massif, a valgrind tool, can\n",
            "profile heap memory usage over time, showing memory consumption patterns.\n",
            "Sentence 265: it's particularly\n",
            "useful for detecting memory leaks and understanding how memory usage evolves during\n",
            "program execution.\n",
            "Sentence 266: 2. intel vtune profiler : vtune profiler provides memory analysis capabilities, including memory\n",
            "usage tracking.\n",
            "Sentence 267: it can analyze memory consumption at various levels, from individual functions\n",
            "to entire applications, in both serial and parallel contexts.\n",
            "Sentence 268: 3. openmp/mpi memory profiler s: many parallel programming frameworks like openmp and\n",
            "mpi provide their memory profiling tools.\n",
            "Sentence 269: for example, openmp has tools like score -p, and mpi\n",
            "has memory profiling features integrated into mpi implementations.\n",
            "Sentence 270: parallel algorithms and patterns\n",
            "a parallel algorithm is a step -by-step computational procedure or set of rules designed to be\n",
            "executed on parallel computing architectures.\n",
            "Sentence 271: these algorithms are specifically crafted to take\n",
            "advantage of parallel processing capabilities, where multiple proces sors or cores can work together\n",
            "to solve a problem.\n",
            "Sentence 272: parallel patterns are like reusable blueprints that help programmers apply proven methods to solve\n",
            "specific types of problems efficiently.\n",
            "Sentence 273: these patterns guide the decomposition of tasks and data,\n",
            "providi ng a framework for creating effective parallel algorithms .\n",
            "Sentence 274: example : parallel algorithm for finding the maximum element:\n",
            "suppose you have a large array of numbers, and you want to find the maximum element using a\n",
            "parallel algorithm based on the \"divide an d conquer\" pattern.in this example, the \"divide and\n",
            "conquer\" pattern is applied to find the maximum element in an array.\n",
            "Sentence 275: the array is divided into\n",
            "smaller subarrays, and the maximum values of these subarrays are found in parallel.\n",
            "Sentence 276: finally, the\n",
            "maximum amon g these partial maximums is selected as the maximum element of the entire array.\n",
            "Sentence 277: algorithm analysis for parallel computing applications\n",
            "the goal of algorithm analysis is to compare different algorithms that are used to solve the same\n",
            "problem.\n",
            "Sentence 278: one of the more traditional ways to evaluate algorithms is by looking at their algorithmic\n",
            "complexity.\n",
            "Sentence 279: definition: algorithmic complexity is a measure of the number of operations that it would take to\n",
            "complete an algorithm.\n",
            "Sentence 280: algorithmic complexity is a pro perty of the algorithm and is a measure of\n",
            "the amount of work or operations in the procedure.\n",
            "Sentence 281: complexity is usually expressed in asymptotic notation .\n",
            "Sentence 282: using asymptotic notation, you can analyze\n",
            "and compare algorithms efficiently and make informed decisions when choosing the most suitable\n",
            "algorithm for a particular problem, taking into account both time and space complexity\n",
            "the three main types of asymptotic notation are:\n",
            "1. big o notation (o-notation):\n",
            " big o notation describes the upper bound or worst -case time complexity of an\n",
            "algorithm.\n",
            "Sentence 283: it represents an approximation of how an algorithm's running time\n",
            "increases as the input size grows.\n",
            "Sentence 284: it provides an upper limit on the number of basic\n",
            "operations an algorithm performs.\n",
            "Sentence 285: 2. theta notation (θ -notation):\n",
            " theta notation provides a tight bound, expressing both the upper and lower bounds\n",
            "of an algorithm's time complexity.\n",
            "Sentence 286: it characterizes the average -case behavior of an\n",
            "algorithm.\n",
            "Sentence 287: 3. omega notation (ω -notation):\n",
            " omega notation describes the lower bound or best -case time complexity of an\n",
            "algorithm.\n",
            "Sentence 288: it provides a way to express how quickly the algorithm can solve a problem\n",
            "in the most favorable circumstances.\n",
            "Sentence 289: performance models versus algorithmic complexit y\n",
            "performance models are broader and more practical in nature.\n",
            "Sentence 290: they encompass various\n",
            "aspects of system performance, including algorithmic efficiency, but also consider factors\n",
            "related to specific hardware, software, and real -world scenarios.\n",
            "Sentence 291: algorithmic complexity, often expressed using asymptotic notations like big o, theta, and\n",
            "omega, focuses on analyzing the efficiency of algorithms in terms of their time and space\n",
            "requirements as a function of the input size.\n",
            "Sentence 292: it provides a theoretical fram ework for\n",
            "characterizing how an algorithm's performance scales as the input size grows towards\n",
            "infinity .\n",
            "Sentence 293: example: finding the sum of all elements in an array.\n",
            "Sentence 294: algorithmic complexity (big o notation):\n",
            " time complexity: o(n) - linear time complexity, where n is the size of the input array.\n",
            "Sentence 295: the algorithm processes each element once.\n",
            "Sentence 296:  space complexity: o(1) - constant space complexity, as it uses only a few variables\n",
            "regardless of the input size.\n",
            "Sentence 297: performance model considerations:\n",
            " hardware differences: different computers might execute the same algorithm at\n",
            "different speeds due to variations in processor capabilities.\n",
            "Sentence 298:  compiler optimizations: compilers can optimize the code differently, affecting the\n",
            "execution time.\n",
            "Sentence 299:  parallelization: divide the array into chunks and calculate the partial sums concurrently\n",
            "using parallel processing techniques, especially for large arrays.\n",
            "Sentence 300:  memory optimization: for very large arrays, consider memory -efficient data structures\n",
            "or algorithms to reduce memory usage.\n",
            "Sentence 301: parallel algorithms\n",
            "parallel algorithms are designed to efficiently solve computational problems by utilizing multiple\n",
            "processing units (such as cpu cores, gpus, or distributed computing nodes) simultaneously.\n",
            "Sentence 302: they are\n",
            "crucial in high -performance computing (hpc) and parallel processing environments where large\n",
            "datasets and complex computations need to be handled efficiently.\n",
            "Sentence 303: parallel algorithms aim to break\n",
            "down tasks into smaller subtasks that can be processed independently and concurrently, leading to\n",
            "significant speedup in overall computation time.\n",
            "Sentence 304: here are some common types of parallel algorithms:  parallel merge sort: divide the sorting task into smaller parts, sort them independently, and\n",
            "then merge the sorted parts in parallel.\n",
            "Sentence 305:  parallel quicksort: a parallel version of th e quicksort algorithm that partitions the data and\n",
            "sorts partitions concurrently.\n",
            "Sentence 306: hash function\n",
            "hashing is a popular technique for quickly storing and retrieving data.\n",
            "Sentence 307: hashing is a technique or process\n",
            "of mapping keys, values into the hash table by using a hash function.\n",
            "Sentence 308: it is done for faster access to\n",
            "elements.\n",
            "Sentence 309: the efficiency of mapping depends on the efficiency of the hash function used.\n",
            "Sentence 310: components of hashing\n",
            "1. hash function: the hash function itself is a crucial component.\n",
            "Sentence 311: it takes an input and produces\n",
            "a fixed -size hash value.\n",
            "Sentence 312: the hash function ensures that the same input always produces the\n",
            "same hash value and that even a small change in the input results in a significantly different\n",
            "hash value.\n",
            "Sentence 313: 2. input data: this is the data that you want to hash.\n",
            "Sentence 314: it can be any type of data, such as a file, a\n",
            "password, or a message.\n",
            "Sentence 315: 3. hash value: also known as the hash code or hash keys , it's the output of the hash function\n",
            "after processing the input data.\n",
            "Sentence 316: the hash value is typically a index into hash table .\n",
            "Sentence 317: 4. collision: a collision occurs when two different inputs produce the same hash value.\n",
            "Sentence 318: 5. bucket or slot: in the context of hash tables , a bucket or slot is a location where data is stored\n",
            "based on its hash value.\n",
            "Sentence 319: hash tables consist of an array of these buckets, and the hash value\n",
            "determines which bucket a particular piece of data will be stored in.\n",
            "Sentence 320: 6. hash table: a hash table is a data str ucture that uses hashing to implement an associative\n",
            "array, a structure that can map input to values.\n",
            "Sentence 321: it consists of an array of buckets where data is\n",
            "stored based on its hash value.\n",
            "Sentence 322: hash tables allow for efficient insertion, deletion, and lookup\n",
            "operation s.\n",
            "7. load factor: the load factor of a hash table is the ratio of the number of stored elements to\n",
            "the total number of buckets.\n",
            "Sentence 323: a high load factor can lead to increased collisions and decreased\n",
            "performance, so hash tables are often resized and rehashed if th e load factor exceeds a certain\n",
            "threshold.\n",
            "Sentence 324: 8. sparsity : the sparsity of a hash table is the ratio of the number of empty buckets to the total\n",
            "number of buckets 9. collision resolution: techniques used to handle collisions include chaining (where each bucket\n",
            "contains a linked list of items that hash to the same index) and open addressing (where the\n",
            "algorithm searches for the next open slot in the hash table).\n",
            "Sentence 325: spatial hashing\n",
            "spatial data is any type of data that directly or indirectly references a specific geographical area or\n",
            "location.\n",
            "Sentence 326: sometimes called geospatial data or geographic information, spatial data can also numerically\n",
            "represent a physical object in a geographic coordinate system.\n",
            "Sentence 327: however, spatial data is much more\n",
            "than a spatial component of a map.\n",
            "Sentence 328: spatial data can be stored in either vector format or raster format\n",
            "vector\n",
            "raster\n",
            "spatial hashing is a technique where the key is based on spatial information.\n",
            "Sentence 329: spatial hashing is a\n",
            "technique used to locate objects in a 3d space.\n",
            "Sentence 330: it involves dividing a large space into smaller, grid -like\n",
            "cells, and assigning each object in the space to the cell that contains it..the basic principle is to map\n",
            "objects onto a grid of buckets arranged in a regular pattern .\n",
            "Sentence 331: spatial information can also be stored in adaptive mesh refinement (amr) format.amr is a numerical\n",
            "simulation technique used in computational mathematics, fluid dynamics, and other fields to imp rove\n",
            "the efficiency and accuracy of simulations.\n",
            "Sentence 332: amr systems maintain multiple levels of grids.\n",
            "Sentence 333: each level\n",
            "represents a different resolution of the simulation domain.\n",
            "Sentence 334: finer grids cover smaller areas, providing\n",
            "high resolution, while coarser grids cover lar ger areas, offering lower resolution.\n",
            "Sentence 335: the same data of amr can be even stored in perfect hashtable or compact hash table.\n",
            "Sentence 336: perfect hashing is a technique used to eliminate collisions entirely in hash tables, ensuring that each\n",
            "key maps to a unique index without any conflicts.\n",
            "Sentence 337: in traditional hash tables, collisions can occur when\n",
            "multiple keys hash to the same index, requiring additional data structures like linked lists or open\n",
            "addressing techniques to resolve these collisions.\n",
            "Sentence 338: perfect hashing, on the oth er hand, aims to design a\n",
            "hash function and data structure in such a way that collisions never occur.\n",
            "Sentence 339: compact hashing is a technique used to design hash functions in a way that minimizes the memory\n",
            "required for hash tables.\n",
            "Sentence 340: it focuses on creating hash func tions that distribute keys uniformly across the\n",
            "available slots in a hash table while keeping the table small in size.\n",
            "Sentence 341: the objective is to use as few bits\n",
            "as possible per key, reducing the memory footprint of the hash table.\n",
            "Sentence 342: because of the compression to a compact hash, two entries try to store their value in bucket 1. the\n",
            "second entry sees that there is already a value there, so it looks for the next open slot in a technique\n",
            "called open addressing.\n",
            "Sentence 343: in open addressing, we look for the next open slot in the hash table and\n",
            "store the value in that slot.\n",
            "Sentence 344: there are other hashing methods than open addressing, but these often\n",
            "require the ability to allocate memory during an operation.\n",
            "Sentence 345: a llocating memory is more difficult on\n",
            "the gpu, so we stick with open addressing where collisions are resolved by finding alternate storage\n",
            "locations within the already allocated hash table.\n",
            "Sentence 346: in open addressing, there are a few choices that\n",
            "we can use as the trial for the next open slot.\n",
            "Sentence 347: these are\n",
            "linear probing —where the next entry is just the next bucket in sequence until an open bucket is\n",
            "found\n",
            "quadratic probing —where the increment is squared so that the attempted buckets are +1, +4,\n",
            "+9, and so forth f rom the original location\n",
            "double hashing —where a second hashing function is used to jump to a deterministic, but pseudo -\n",
            "random distance from the first trial location\n",
            "on spatial data we can perform, four spatial operations.\n",
            "Sentence 348:  neighbor finding —locating the one or two neighbors on each side of a cell\n",
            " remapping —mapping another amr mesh onto a current amr mesh\n",
            " table lookup —locating the intervals in the 2d table to perform the interpolation\n",
            " sorting —a 1d or 2d sort of the cell data\n",
            "these can be implemented either by perfect hashing or compact hashing.\n",
            "Sentence 349: neighbor finding using a spatial perfect hash\n",
            "the steps involved are\n",
            " allocate a spatial hash the size of the finest level of the cell -based amr mesh\n",
            " for each cell in the amr mes h, write the cell number to the hash buckets underlying the cell\n",
            " compute the index for a finer cell one cell outside the current cell on each side\n",
            " read the value placed in the hash bucket at that location.\n",
            "Sentence 350: example: each cell writes its cell number to the hash buckets it covers.\n",
            "Sentence 351: right neighbor of cell 2 1 is\n",
            "at col 8, row 3. look up in hash and it is cell 26 .\n",
            "Sentence 352: remapping —mapping another amr mesh onto a current amr mesh (perfect hash)\n",
            "remapping from one mesh to another is a common operation in computational simulations,\n",
            "especially in fields like computational fluid dynamics (cfd) and finite element analysis (fea).\n",
            "Sentence 353: this\n",
            "process is used to transfer data (usually physical quantities like temperature, pressure, or velocity)\n",
            "from one mesh or grid to another.\n",
            "Sentence 354: you have two grids or meshes: the source mesh (from which you\n",
            "want to remap data) and the target mesh (to which you want to map the data ).\n",
            "Sentence 355: table lookup(perfect hash )\n",
            "to perform a table lookup in spatial hashing, you need to map the position of an object to a\n",
            "unique key (hash value), which determines the index of the bucket in the hash table where\n",
            "the object should be retrieved.\n",
            "Sentence 356:  perform a lookup in the hash table at the calculated index to find the bucket\n",
            "corresponding to the object's spatial location.\n",
            "Sentence 357:  if chain ing (linked lists in each bucket) is used for collision resolution, traverse the\n",
            "linked list in the selected bucket to find the object.\n",
            "Sentence 358:  if open addressing is used, probe the adjacent cells until the object is found or an\n",
            "empty cell is encountered.\n",
            "Sentence 359: sorting mesh data using a spatial perfect hash\n",
            "we can demonstrate the hash sort operation .\n",
            "Sentence 360: the minimum difference between values is 2.0, so\n",
            "the bucket size of 2 guarantees that there are no collisions.\n",
            "Sentence 361: the minimum value is 0, so the bucket\n",
            "location can be calcul ated with bi = xi /δmin = xi /2.0.\n",
            "Sentence 362: we could store either the value or the index\n",
            "in the hash table.\n",
            "Sentence 363: for example, 8, the first key, could be stored in bucket 4 .\n",
            "Sentence 364: prefix sum (scan) pattern and its importance in parallel computing\n",
            "the prefix sum, also known as the scan operation, is a fundamental parallel pattern in computer\n",
            "science and parallel computing.\n",
            "Sentence 365: given an input array of elements, the prefix sum operation computes\n",
            "a new array where each element is the sum of all elements in the input array up to and including the\n",
            "corresponding element's position.\n",
            "Sentence 366: there are two common types of prefix sum operations: exclusive\n",
            "and inclusive.\n",
            "Sentence 367:  exclusive prefix sum: the result at each position does not include the element at that\n",
            "position.\n",
            "Sentence 368:  input: [a, b, c, d]\n",
            " output (exclusive): [0, a, a+b, a+b+c]\n",
            " inclusive prefix sum: the result at each position includes the element at that position.\n",
            "Sentence 369:  input: [a, b, c, d]\n",
            " output (inclusive): [a, a+b, a+b+c, a+b+c+d]\n",
            "\n",
            "step -efficient parallel scan operation(inclusive prefix sum)\n",
            "work -efficient parallel scan operation(exclusive prefix sum)\n",
            "\n",
            "the work -efficient parallel scan operation uses two sweeps through the arrays.\n",
            "Sentence 370: the first sweep is\n",
            "called an upsweep, though it is more of a right sweep.\n",
            "Sentence 371: the second phase, known as the downsweep phase, is more of a left sweep.\n",
            "Sentence 372: the output of upsweep\n",
            "is provided as input to downsweep.\n",
            "Sentence 373: it starts by setting the last value to zero and then does another\n",
            "tree-based sweep to get the final result.\n",
            "Sentence 374: parallel global sum\n",
            "the \"parallel global sum\" refers to the problem of computing the sum of elements across multiple\n",
            "processors or nodes in a parallel or distributed computing environment.\n",
            "Sentence 375: though the process is simple it has some problems like changing the order of additions changes the\n",
            "answer in finite -precision arithmetic.\n",
            "Sentence 376: this is problematic because a parallel calculation changes the\n",
            "order of the additions.\n",
            "Sentence 377: the problem is due to finite -precision arithmetic not being associative.\n",
            "Sentence 378: and\n",
            "the problem gets worse as the problem size gets larger because the addition of the last value\n",
            "becomes a smaller and smaller part of the overall sum.\n",
            "Sentence 379: eventually the addition of the last value\n",
            "might not change the sum at all.\n",
            "Sentence 380: there is even a worse case for additions of finite precision values\n",
            "when adding two values that are almost identical, but of different signs.\n",
            "Sentence 381: this subtraction of one\n",
            "value from another when these are nearly the same causes a catastrophic cancella tion.\n",
            "Sentence 382: catastrophic cancellation occurs when the operands are subject to rounding errors.\n",
            "Sentence 383: for example, if there are two measures\n",
            "l1=253.5cm long and the other l2 =252.5cm long\n",
            "approximations could come out to be\n",
            "l1 = 254cm and\n",
            "l2 = 252cm l1-l2 = 2cm\n",
            "actual difference is l1-l2 = 1 cm\n",
            "there are several solutions for addressing the global sum .\n",
            "Sentence 384: the list of possible techniques presented\n",
            "here includes\n",
            " long -double data type\n",
            " pairwise summation\n",
            " kahan summation\n",
            " knuth summation —uses same method of pairwise\n",
            " quad -precision summation\n",
            "long -double data type\n",
            "the easiest solution is to use the long -double data type on a x86 architecture.\n",
            "Sentence 385: on this architecture,\n",
            "a long -double is implemented as an 80 -bit floating -point number in hardware giving an extra 16 -bits\n",
            "of precision.\n",
            "Sentence 386: unfortunately, this is not a portable technique\n",
            "pairwise summation\n",
            "pairwise summation, also known as pairwise addition, is a method used to sum a sequence of\n",
            "numbers in a way that reduces the effects of numerical errors, particularly in floating -point\n",
            "arithmetic.\n",
            "Sentence 387: this technique is commonly employed in scientific computing and numerical analysis to\n",
            "improve the accuracy of summation operations.\n",
            "Sentence 388: how pairwise summation works:\n",
            "1. pairing the numbers:\n",
            " given a sequence of numbers, they are paired up.\n",
            "Sentence 389: if there are an odd number of\n",
            "elements, one number is left unpaired.\n",
            "Sentence 390: 2. pairwise addition:\n",
            " within each pair, the two numbers are added together to create intermediate sums.\n",
            "Sentence 391: 3. summing the intermediate sums:\n",
            " the intermediate sums obtained from pairwise addition are then summed together\n",
            "using the same pairwise summation method.\n",
            "Sentence 392: kahan summation\n",
            "kahan summation, also known as compensated summation or kahan summation algorithm, is a\n",
            "method used to reduce the numerical error that accumulates during the summation of a large\n",
            "number of floating -point values.\n",
            "Sentence 393: t his technique was introduced by william kahan, a renowned\n",
            "computer scientist and mathematician, and it aims to improve the accuracy of summation\n",
            "operations, especially in cases where a vast number of values need to be added together.\n",
            "Sentence 394: how kahan summation works:\n",
            "in standard floating -point summation, when adding a small number to a large number, the small\n",
            "number can be \"lost\" in the least significant bits of the large number, leading to a loss of precision.\n",
            "Sentence 395: kahan summation addresses this issue by using a com pensation term to keep track of the lost\n",
            "precision.\n",
            "Sentence 396: 1. initialization:\n",
            " initialize the sum and the compensation term to zero.\n",
            "Sentence 397: 2. iterative addition:\n",
            " for each number to be added:\n",
            " add the number to the current sum.\n",
            "Sentence 398:  calculate the difference between the updated sum a nd the original sum (this\n",
            "difference is the lost precision).\n",
            "Sentence 399:  add this difference to the compensation term.\n",
            "Sentence 400: 3. final result:\n",
            " the final result is the sum adjusted by the compensation term.\n",
            "Sentence 401: quad -precision summation\n",
            "quad -precision summation refers to performing arithmetic operations with numbers represented\n",
            "in quadruple -precision floating -point format.\n",
            "Sentence 402: in the ieee 754 floating -point standard, quadruple -\n",
            "precision is a 128 -bit data type, providing higher precision compared to single -precision (32 -bit) and\n",
            "double -precision (64 -bit) floating -point number.\n"
          ]
        }
      ],
      "source": [
        "def paragraph_to_sentences(paragraph):\n",
        "    # Using the nltk library for sentence tokenization\n",
        "    # text Processing\n",
        "    # Tokenization Part-of-Speech Tagging semntiment Analysis\n",
        "    import nltk\n",
        "    nltk.download('punkt')  # Download the punkt tokenizer if not already downloaded\n",
        "\n",
        "    # Tokenize the paragraph into sentences\n",
        "    sentences = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Example usage\n",
        "sentences_list = paragraph_to_sentences(text)\n",
        "\n",
        "# Print the list of sentences\n",
        "for idx, sentence in enumerate(sentences_list, start=1):\n",
        "    print(f\"Sentence {idx}: {sentence}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "096ikGtS_-PI"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PQ-SvQb12nQ3"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "#This model is designed to convert \"sentences into dense vector\" representations (embeddings) that \n",
        "#capture their semantic meaning.(Imagine words in a language as points in a high-dimensional space, where their positions reflect their meaning and relationships.)\n",
        "corpus_embeddings = embedder.encode(sentences_list)\n",
        "#process of converting each sentence in the list into a numerical representation called an embedding.\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F38PFdh92nc6",
        "outputId": "9aeb1381-983e-4ba9-c253-39a779683abe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 2 2 2 2 2 0 2 2 2 2 2 1 2 2 2 2 2 1 1 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 1 1 1 1\n",
            " 2 2 2 2 1 1 1 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 1 2 1 2 1 1 2 3 3 2\n",
            " 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 1 1 2 1 1 2 1 2 2 1 1 1 2\n",
            " 1 1 2 2 1 2 1 1 2 1 2 2 2 1 1 2 1 1 1 2 2 2 2 1 1 1 2 2 1 1 1 2 1 1 1 1 0\n",
            " 2 1 1 1 1 2 2 1 0 1 2 2 1 2 2 2 2 1 2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0\n",
            " 2 2 0 2 2 2 2 2 2 3 3 1 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1\n",
            " 1 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3 3 3 1 3 3 3 3 3 0 0 0 0 0 0 1\n",
            " 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# source: https://stackoverflow.com/questions/55619176/how-to-cluster-similar-sentences-using-bert\n",
        "\n",
        "clustering_model = KMeans(n_clusters=4)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "print(cluster_assignment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyuIYFKk2rpN",
        "outputId": "74de314a-ac7d-4ebb-b31b-335062342c2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "\n",
            "\n",
            "['\\n\"computing\" refers to the process of using computers and computer systems to perform various\\ntasks, such as data processing, information storage, and solving complex problems .', 'this computing can be either done in serial way known as serial computing or in parallel wa y\\nknown as parallel computing\\nserial computing:\\nserial computing refers to traditional computing where tasks are executed sequentially, one after\\nthe other, using a single processor.', 'in serial computing, each instruction or task must wait for the\\nprevious one to complete before it can be executed.', 'this approach limits the speed and efficiency\\nof processing, especially when dealing with complex or time -consuming tasks.', 'computing\\nserial or\\nsequential parallel example of serial computing : consider a task of sorting a large dataset of numbers in ascending\\norder.', 'in a serial computing environment, a single processor would go through the entire dataset,\\ncomparing and rearranging numbers one pair at a time until the entire dataset is sorted.', 'parallel computing\\nparallel computing is a type of computation in which multiple processors or computers work together\\nto solve a problem.', 'instead of one single processor handling the entire task, parallel computing divides\\nthe task into smaller sub -tasks that can be processed simultaneously.', 'this simultaneous processing\\ncan lead to significant improvements in computational speed and efficien cy.', 'example of parallel computing: using the same example of sorting a large dataset, parallel computing\\nwould involve dividing the dataset into smaller chunks, and each chunk is sorted independently by a\\nseparate processor.', 'these processors work in parall el, sorting their respective chunks simultaneously.', 'definition parallel computing is the practice of identifying and exposing parallelism in algorithms,\\nexpressing this in our software, and understanding the costs, benefits, and limitations of the chosen\\nimplementation.', 'benefits of parallel computing\\n\\n\\uf0b7 faster run time with more compute cores: parallelization involves dividing a task into\\nsmaller sub -tasks that can be executed simultaneously, utilizing multiple cores to process the\\ndata.', 'this approach can significantly reduce the time required to complete the task, as each\\ncore works on a separate portion of the problem concurrently.', '\\uf0b7 larger problem sizes with more compute nodes : with more nodes, you can break down your\\nproblem into smaller pieces that each node can work on simultaneously, which is especially\\nbeneficial for handling larger datasets and more complex simulations .', '\\uf0b7 energy efficiency by doing more with less: in the context of parallel computing, the concept\\nof \"doing more with less\" often revolves around optimizing energy efficiency while achieving\\nbetter computational performance.this can be achieved by making use of dynamic resource\\nallocation and workload consolidation to ensure that the number of processors used is\\nproportional to the workload.', '\\uf0b7 scalability : parallel computing can be easily scaled by adding more processors, which further\\nenhances performance.', 'serial computing does not scale in this manner, as it relies on a single\\nprocessor.', '\\uf0b7 parallel computing can reduce costs: as technology advances, the cost of individual\\nprocessors and memory decreases.', 'parallel computing systems can take advantage of these\\ncost reductions, making it more economical to build high -performance computing clusters or\\ndata centers .', 'applications of parallel computing:\\n\\uf0b7 scientific simulations: used in fields such as physics, chemistry, and engineering for\\ncomplex simulations.', '\\uf0b7 big data processing: parallel computing is crucial in processing vast amounts of data in\\nfields like data analytics and machine learning.', '\\uf0b7 video and image processing: parallelism accelerates tasks like video rendering and\\nimage recognition.', \"fundamental laws\\nfundamental laws in parallel computing, such as amdahl's law and gustafson's law, are essential\\nfor understanding the limitations and possibilities of parallel processing.\", 'these laws provide valuable\\ninsights into how the speedup of a parallel algorithm is affected by various factors .', 'what is speedup?', 'speedup in parallel computing refers to the performance improvement achieved by using multiple\\nprocessors or computing resources to solve a problem compared to using a single processor.', 'it is a\\nmeasure of how much faster a parallel algorithm or system can complet e a task compared to a serial\\n(single -processor) implementation of the same task.', 'speedup is a crucial metric for evaluating the\\neffectiveness of parallel computing systems.', 'the speedup ( s) can be calculated using the following formula:\\ns=tserial / tparallel\\nwhere:\\n\\uf0b7 tserial is the execution time of the task using a single processor (serial execution time).', 'tparallel is the execution time of the task using multiple processors (parallel execution time).', 'a speedup value greater than 1 indicates that the parallel implementation is faster than the serial\\nimplementation.', 'ideally, in a perfectly parallelizable task, doubling the number of processors would ideally halve the execution time, resulting in a speedup of 2. however, achieving perfect linear\\nspee dup is rare in real -world scenarios due to factors such as communication overhead, load\\nbalancing issues, and synchronization constraints between processors.', \"amdahl's law is a fundamental principle in parallel computing that expresses the potential\\nspeedup of a parallel algorithm as a function of the proportion of the algorithm that can be\\nparallelized.\", 'it was formulated by gene amdahl in 1967 and is represented by the fol lowing\\nformula:\\nwhere:\\n\\uf0b7 speedup is the improvement in performance achieved by parallelizing a computation\\ncompared to executing it sequentially.', '\\uf0b7 p is the proportion of the algorithm that can be parallelized (a value between 0 and 1).', \"\\uf0b7 s is the serial fraction\\n\\uf0b7 n – no.of processors/nodes/cores\\namdahl's law highlights the limitations of parallel computing.\", 'it states that the speedup of a\\nprogram using multiple processors in parallel computing is limited by the sequential fraction of\\nthe program.', 'in other words, if only a portion of a program can be parallelized (the rest being\\ninherently sequential), then no matter how many processors are added, there will always be a\\nlimit to the speedup that can be achieved.', \"for example, if 90% of a program can be parall elized (p = 0.9) and the parallel portion runs on\\n5 processors , the maximum speedup that can be achieved according to amdahl's law is:\\nspeedup= 1/(0.1+(0.9/5))=3.57\\nin this case, even though 90% of the program can be parallelized and runs on 5 processors , the\\nmaximum speedup achievable is approximately 3.57 times faster compared to the sequential\\nexecution due to the presence of the 10% sequential portion.\", 'fig : speedup for a fixed -size problem according to amdahl’s law is shown as a function of the\\nnumber of processors.', 'lines show ideal speedup when 100% of an algorithm is parallelized,\\nand for 90%, 75%, and 50%.', 'amdahl’s law states that speedup is limited by the fractions of\\ncode that remain serial.', \"gustafson's law , formulated by john l. gustafson, provides a different perspective on parallel\\ncomputing compared to amdahl's law.\", \"the basic idea behind g ustafson's law is\\nthat as the size of the problem increases, the impact of the parallelizable portion of the program\\nbecomes more significant, leading to better scalability.\", 'in other words, with larger problem sizes,\\nparallel systems can achieve higher lev els of speedup.', \"the formula for gustafson's law is as follows:\\nspeedup(n) = n – s * (n – 1) where n is the number of processors, and s is the serial fraction\\nstrong scaling and weak scaling are two different m etrics used to evaluate the performance of\\nparallel computing systems, and they provide insights into how well a parallel algorithm or\\napplication can handle an increasing workload or an increasing number of processors.\", \"here's a\\ncomparison of strong scalin g and weak scaling:\\nstrong scaling:\\ndefinition: strong scaling measures how the execution time of a fixed problem size decreases\\nas the number of processors increases.\", 'in other words, it assesses how well a parallel system\\nperforms when the size of the problem remains constant, but the number of proces sors used\\nto solve the problem increases.', 'objective: the goal of strong scaling is to reduce the execution time for a fixed problem size by\\nutilizing more processors.', 'it aims to speed up the solution of a specific problem.', 'scenario: strong scaling is applicable when the size of the problem is fixed, and the aim is to\\nsolve that problem faster by employing additional processors.', '2. weak scaling:\\ndefinition: weak scaling measures how the execution time changes as both the problem size\\nand the number of processors increase proportionally.', 'in other words, it assesses how well a\\nparallel system can handle larger workloads by adding more processors as the problem size\\ngrows.', 'objective: the goal of weak scaling is to maintain a constant workload per processor as the size\\nof the problem and the number of processors increase.', 'it aims to solve larger problems in\\napproximately the same amount of time per processor.', 'scenario: weak scaling is applicable when the problem size can be increased, and the aim is to\\nhandle larger workloads by distributing the computational load across a larger number of\\nprocessors.', \"parallel approaches (flynn’s classification)\\n\\nflynn's classification is essential in the field of parallel computing because it provides a framework\\nfor understanding and categorizing different types of computer architectures based on the number\\nof instruction streams and data streams.\", 'the taxonomy highlights the importance of parallelism in modern computing and shows how\\ndifferent types of parallelism can be exploited to improve performance.', 'it helps in designing and\\nanalyzing parallel processing systems\\n\\n1. single instruction single data (sisd ): in a sisd architecture, there is a single processor\\nthat executes a single instruction stream and operates on a single data stream.', '2. single instruction multiple data (simd ): in a simd architecture, there is a single\\nprocessor that executes the same instruction on multiple data streams in parallel.', 'this type of architecture is used in\\ndistributed computing, parallel processing, and other high -performance computing applications.', 'parallel strategies\\nparallel strategies\" typically refer to techniques and methods for parallel processing, which is the\\nsimultaneous execution of multiple tasks or processes to improve the efficiency and performance\\nof a computer system.', 'parallel strategies are commonly used in various computing domains, such\\nas high -performance computing and distributed systems, to speed up computations and handle\\nlarge volumes of data.', 'here are some common parallel s trategies:\\ndata parallel approach\\ndata parallelism involves performing the same operation on multiple data elements simultaneously.', 'the sentiment analysis process is computationally intensive, and you want to\\nspeed it up using data parallelism.', 'data parallelism in sentiment analysis :\\n1. data preparation : you have a dataset of 1,000,000 customer reviews.', 'to apply data\\nparallelism, you divide this dataset into smaller, non-overlapping subsets.', '2. parallel processing : you have a sentiment analysis model that can analyze reviews.', 'task parallelism(main -worker approach)\\ntask parallelism involves executing multiple independent tasks or processes in parallel.', 'task\\nparallelism is common in applications where different tasks can be performed concurrently without\\ndependencies between them.', 'in the main -worker approach, one processor schedules and distributes the tasks for all the workers,\\nand each worker checks for the next work item as it returns the previous completed task .', 'each i ncoming request is an independent\\ntask that can be processed concurrently.', 'in a task parallelism scenario:\\n1. task 1: parsing request\\n\\uf0b7 this task involves pars ing the incoming http request to extract information like the\\nrequested url, parameters, and headers.', 'in a task parallelism setup, these tasks can be executed concurrently by multiple threads or\\nprocesses, allowing the server to handle multiple incoming requests simultaneously without waiting\\nfor one task to complete before starting the next.', 'in parallel computing, the concept of bucket -brigade parallelism involves breaking down a task into\\nsmaller subtasks, where each subtask is processed independently and passed to the next processing\\nunit for further computation.', 'this technique allows for efficient parallel processing of tasks and is\\noften used in scenarios where tasks can be divided into smaller, manageable parts.', 'parallel speedup versus comparative speedups.', 'parallel speedup and comparative speedup are two different metrics used to evaluate the performance\\nimprovement achieved by parallel processing.', 'parallel speedup measures how much faster a parallel algorithm runs compared to its sequential\\n(single -processor) counterpart.', 'it quantifies the performance improvement gained by using multiple\\nprocessing units in parallel.', 'parallel speedup is calculated using the followi ng formula:\\nparallel speedup=sequential execution time/parallel execution\\nin this formula:\\n\\uf0b7 sequential execution time is the time taken by the algorithm to execute sequentially on a\\nsingle processor.', '\\uf0b7 parallel execution time is the time taken by the paralle l algorithm to execute on multiple\\nprocessors.', 'comparative speedup : comparative speedup is between architectures.', 'this is usually a\\nperformance comparison between two parallel implementations or other comparison between\\nreasonably constrained sets of hardware.', 'for example, it may be between a parallel mpi\\nimplementation on all the cores of the node of a computer versus the gpu(s) on a node\\nhow parallel computing works\\nas a developer, you are responsible for the application software layer, which includes your source code.', 'in the source code, you make choices about the programming language and parallel software interfaces\\nyou use to leverage the underlying hardware.', 'additionally, you decide how to break up your work into\\nparallel units.', 'parallel approach models are used to express parallelization in an application software layer\\nthat gets mapped to the computer hardware through the compiler and the os.', 'parallel computing\\napproaches involve various models and paradigms that define how tasks are divided, coordinated, and\\nexecuted in parallel systems.', 'here are some common parallel computing approach models:\\nhardware models\\ndistributed memory architecture: a cross -node parallel method :\\nparallel approach models\\nhardware models\\ndistributed memory architecture:\\na cross -node parallel method :\\nshared memory architecture: an\\non-node parallel method\\nvector units: multiple operations\\nwith one instruction\\naccelerator device: a special -\\npurpose add-on processor\\nsoftware models\\nprocess -based parallelization :\\nmessage passing\\nthread -based parallelization :\\nshared data via memory\\nvectorization : multiple\\noperations with one instruction\\nstream processing : through\\nspecialized processors\\ndistributed memory architecture, also known as distributed memory parallelism, is a parallel\\ncomputing method where multiple processors or nodes in a cluster have their own private memory.', 'in the context of distributed memory architecture, a \"cross -node parallel method\" refers to parallel\\nprocessing techniques that involve distributing tasks across multiple nodes in a cluster.', 'shared memory architecture: an on-node parallel method\\nin shared memory architecture, multiple processors or cores share a single, unified memory space.', 'on -node\\nparallelism, within the context of shared memory architecture, ref ers to parallel processing\\ntechniques that occur on a single computing node.', 'in this approach, multiple threads or processes\\nrun concurrently on the same node, accessing shared memory to perform computations .', 'vector processing example with four array elements operated on simultaneously\\naccelerator device: a special -purpose add-on processor\\ngpus come in two varieties: integrated and discrete.', 'accelerators are typically used in conjunction with a central processin g unit (cpu) and are especially\\nwell-suited for workloads that can benefit from parallel processing and offloading certain tasks from\\nthe cpu.', 'accelerators are sometimes called \"add -on processors\" because they augment the\\nprocessing capabilities of a syste m.\\ngeneral heterogeneous parallel architecture model\\nnow let’s combine all of these different hardware architectures into one model .', 'each cpu is a dual -core processor with an\\nintegrated gpu.', 'this means that accessing the second cpu’s memory is more expensive than getting at it’s\\nown memory\\nfig 5: a general heterogeneous parallel architecture model consisting of two nodes connected by\\na network.', 'software models\\nthe programmer must first expose the parallelization, determine the best technique to operate in\\nparallel, and then explicitly direct its operation in a safe, correct, and efficient manner.', 'the following\\nmethods are the most common techniques for parallelization\\n\\uf0b7 process -based parallelization : message passing\\nprocess -based parallelization, particularly through message passing, is a common approach in\\nparallel computing.', 'it involves dividing a task into multiple processes or threads that run\\nindependently on separate computing nodes or cores.', 'this approach is widely used in distributed memory systems,\\nsuch as clusters and supercomputers.', 'the os also allocates\\nmemory for each process from the node’s main memory\\n\\uf0b7 thread -based parallelization : shared data via memory\\nthread -based parallelization involves dividing a task into multiple threads that share the same\\nmemory space within a single process.', 'these threads can run concurrently on multiple cpu cores,\\nand they communicate and coordinate by accessing shared data in the shared memory.', 'this\\napproach is commonly used in multi -core processors and symmetric multiprocessing (smp) systems.', 'fig 7: the application process in a thread -based approach to parallelization spawns\\nthreads.', '\\uf0b7 vectorization : multiple operations with one instruction\\nvectorization is a parallel computing technique that enables processors to perform multiple\\noperations with a single instruction.', 'it takes advantage of simd (single instruction, multiple data)\\ncapabilities found in modern processors, including cpus and gpus.', 'specialized processors designed for stream processing accelerate the\\nanalysis and manipu lation of data streams, ensuring timely and efficient processing\\nin the stream processing approach, data and compute kernel are offloaded to the gpu and its\\nstreaming multiprocessors.', 'define a computational kernel (operation) to conduct on each element of the mesh\\n3. add the following layers of parallelization on cpus and gpus to perfo rm the calculation:\\nvectorization —work on more than one unit of data at a time\\n4. threads —deploy more than one compute pathway to engage more processing cores\\n5. processes —separate program instances to spread out the calculation into separate memory\\nspaces\\n6. off -loading the calculation to gpus —send the data to the graphics processor to calculate\\nstep 1: discretize the problem into smaller cells or elements\\nthe domain is discretized into cells.', 'step 4: threads to deploy more than one compute pathway to engage more processing cores\\nbecause most cpus today have at least four processing cores, we use threading to operate the cores\\nsimultaneously acros s four rows at a time.', 'step 5: processes to spread out the calculation to separate memory spaces\\nwe can further split the work between processors on two desktops, often called nodes in parallel\\nprocessing.', 'performance limits and profiling\\nin parallel processing, understanding performance limits and profiling the application are crucial\\nsteps to optimize the execution of parallel programs.', 'performance limits refer to the maximum achievable performance of a computing system or\\napplication un der specific conditions.', 'understanding\\nthese performance limits is essential for designing efficient algor ithms, optimizing software, and\\nchoosing appropriate hardware configurations.', 'it also guides researchers and engineers in\\ndeveloping new technologies to overcome existing limitations and improve overall computing\\nperformance.', 'profiling tools are used to ga ther detailed information about the behavior of a parallel program.', 'by\\nunderstanding performance limits, utilizing profiling tools, and optimizing the code based on the\\nprofiling results, developers can enhance the efficiency of parallel applications, lea ding to improved\\nspeedup and overall performance.', 'speeds are how fast operations can\\nbe done.', 'for instance, in\\nparallel computing, theoretical analysis can reveal the maximum speedup or efficiency that a\\nparallel algorithm can achieve in an ideal scenario .', 'dual -socket motherboards are more\\ncommon in high -performance computing systems.', 'two processors can be installed in a dual -socket\\nmotherb oard, giving us more processing cores and more memory bandwidth.', \"analyzing call graphs in parallel programming can provide valuable insights in to the program's\\nstructure, dependencies, and potential performance optimizations .\", 'optimizing these hot -spots is e ssential for improving overall parallel program\\nperformance.', 'parallel algorithms and patterns\\na parallel algorithm is a step -by-step computational procedure or set of rules designed to be\\nexecuted on parallel computing architectures.', 'these algorithms are specifically crafted to take\\nadvantage of parallel processing capabilities, where multiple proces sors or cores can work together\\nto solve a problem.', 'parallel patterns are like reusable blueprints that help programmers apply proven methods to solve\\nspecific types of problems efficiently.', 'these patterns guide the decomposition of tasks and data,\\nprovidi ng a framework for creating effective parallel algorithms .', 'example : parallel algorithm for finding the maximum element:\\nsuppose you have a large array of numbers, and you want to find the maximum element using a\\nparallel algorithm based on the \"divide an d conquer\" pattern.in this example, the \"divide and\\nconquer\" pattern is applied to find the maximum element in an array.', 'algorithm analysis for parallel computing applications\\nthe goal of algorithm analysis is to compare different algorithms that are used to solve the same\\nproblem.', 'performance models versus algorithmic complexit y\\nperformance models are broader and more practical in nature.', 'performance model considerations:\\n\\uf0b7 hardware differences: different computers might execute the same algorithm at\\ndifferent speeds due to variations in processor capabilities.', '\\uf0b7 compiler optimizations: compilers can optimize the code differently, affecting the\\nexecution time.', '\\uf0b7 memory optimization: for very large arrays, consider memory -efficient data structures\\nor algorithms to reduce memory usage.', 'parallel algorithms\\nparallel algorithms are designed to efficiently solve computational problems by utilizing multiple\\nprocessing units (such as cpu cores, gpus, or distributed computing nodes) simultaneously.', 'they are\\ncrucial in high -performance computing (hpc) and parallel processing environments where large\\ndatasets and complex computations need to be handled efficiently.', 'parallel algorithms aim to break\\ndown tasks into smaller subtasks that can be processed independently and concurrently, leading to\\nsignificant speedup in overall computation time.', 'here are some common types of parallel algorithms: \\uf0b7 parallel merge sort: divide the sorting task into smaller parts, sort them independently, and\\nthen merge the sorted parts in parallel.', '\\uf0b7 parallel quicksort: a parallel version of th e quicksort algorithm that partitions the data and\\nsorts partitions concurrently.', 'parallel global sum\\nthe \"parallel global sum\" refers to the problem of computing the sum of elements across multiple\\nprocessors or nodes in a parallel or distributed computing environment.']\n",
            "\n",
            "\n",
            "3609\n",
            "0\n",
            "\n",
            "\n",
            "['this process\\noccurs sequentially, and each comparison and rearrangement must wait for the previous one to\\nfinish.', \"unlike amdahl's law, which focuses on fixed problem sizes,\\ngustafson's law takes into account varying problem sizes.\", '1. discretize (break up) the problem into smaller cells or elements\\n2 .', 'step 6: off -loading the calculation to gpus\\non a gpu, the vector length is much larger than on a cpu.', 'the array is divided into\\nsmaller subarrays, and the maximum values of these subarrays are found in parallel.', 'finally, the\\nmaximum amon g these partial maximums is selected as the maximum element of the entire array.', 'one of the more traditional ways to evaluate algorithms is by looking at their algorithmic\\ncomplexity.', 'definition: algorithmic complexity is a measure of the number of operations that it would take to\\ncomplete an algorithm.', 'algorithmic complexity is a pro perty of the algorithm and is a measure of\\nthe amount of work or operations in the procedure.', 'complexity is usually expressed in asymptotic notation .', 'using asymptotic notation, you can analyze\\nand compare algorithms efficiently and make informed decisions when choosing the most suitable\\nalgorithm for a particular problem, taking into account both time and space complexity\\nthe three main types of asymptotic notation are:\\n1. big o notation (o-notation):\\n\\uf0b7 big o notation describes the upper bound or worst -case time complexity of an\\nalgorithm.', \"it represents an approximation of how an algorithm's running time\\nincreases as the input size grows.\", 'it provides an upper limit on the number of basic\\noperations an algorithm performs.', \"2. theta notation (θ -notation):\\n\\uf0b7 theta notation provides a tight bound, expressing both the upper and lower bounds\\nof an algorithm's time complexity.\", 'it characterizes the average -case behavior of an\\nalgorithm.', '3. omega notation (ω -notation):\\n\\uf0b7 omega notation describes the lower bound or best -case time complexity of an\\nalgorithm.', 'it provides a way to express how quickly the algorithm can solve a problem\\nin the most favorable circumstances.', 'algorithmic complexity, often expressed using asymptotic notations like big o, theta, and\\nomega, focuses on analyzing the efficiency of algorithms in terms of their time and space\\nrequirements as a function of the input size.', \"it provides a theoretical fram ework for\\ncharacterizing how an algorithm's performance scales as the input size grows towards\\ninfinity .\", 'example: finding the sum of all elements in an array.', 'algorithmic complexity (big o notation):\\n\\uf0b7 time complexity: o(n) - linear time complexity, where n is the size of the input array.', 'the algorithm processes each element once.', '\\uf0b7 space complexity: o(1) - constant space complexity, as it uses only a few variables\\nregardless of the input size.', '\\uf0b7 parallelization: divide the array into chunks and calculate the partial sums concurrently\\nusing parallel processing techniques, especially for large arrays.', 'prefix sum (scan) pattern and its importance in parallel computing\\nthe prefix sum, also known as the scan operation, is a fundamental parallel pattern in computer\\nscience and parallel computing.', \"given an input array of elements, the prefix sum operation computes\\na new array where each element is the sum of all elements in the input array up to and including the\\ncorresponding element's position.\", 'there are two common types of prefix sum operations: exclusive\\nand inclusive.', '\\uf0b7 exclusive prefix sum: the result at each position does not include the element at that\\nposition.', '\\uf0b7 input: [a, b, c, d]\\n\\uf0b7 output (exclusive): [0, a, a+b, a+b+c]\\n\\uf0b7 inclusive prefix sum: the result at each position includes the element at that position.', '\\uf0b7 input: [a, b, c, d]\\n\\uf0b7 output (inclusive): [a, a+b, a+b+c, a+b+c+d]\\n\\nstep -efficient parallel scan operation(inclusive prefix sum)\\nwork -efficient parallel scan operation(exclusive prefix sum)\\n\\nthe work -efficient parallel scan operation uses two sweeps through the arrays.', 'it starts by setting the last value to zero and then does another\\ntree-based sweep to get the final result.', 'though the process is simple it has some problems like changing the order of additions changes the\\nanswer in finite -precision arithmetic.', 'this is problematic because a parallel calculation changes the\\norder of the additions.', 'the problem is due to finite -precision arithmetic not being associative.', 'and\\nthe problem gets worse as the problem size gets larger because the addition of the last value\\nbecomes a smaller and smaller part of the overall sum.', 'eventually the addition of the last value\\nmight not change the sum at all.', 'there is even a worse case for additions of finite precision values\\nwhen adding two values that are almost identical, but of different signs.', 'this subtraction of one\\nvalue from another when these are nearly the same causes a catastrophic cancella tion.', 'catastrophic cancellation occurs when the operands are subject to rounding errors.', 'for example, if there are two measures\\nl1=253.5cm long and the other l2 =252.5cm long\\napproximations could come out to be\\nl1 = 254cm and\\nl2 = 252cm l1-l2 = 2cm\\nactual difference is l1-l2 = 1 cm\\nthere are several solutions for addressing the global sum .', 'the list of possible techniques presented\\nhere includes\\n\\uf0b7 long -double data type\\n\\uf0b7 pairwise summation\\n\\uf0b7 kahan summation\\n\\uf0b7 knuth summation —uses same method of pairwise\\n\\uf0b7 quad -precision summation\\nlong -double data type\\nthe easiest solution is to use the long -double data type on a x86 architecture.', 'on this architecture,\\na long -double is implemented as an 80 -bit floating -point number in hardware giving an extra 16 -bits\\nof precision.', 'unfortunately, this is not a portable technique\\npairwise summation\\npairwise summation, also known as pairwise addition, is a method used to sum a sequence of\\nnumbers in a way that reduces the effects of numerical errors, particularly in floating -point\\narithmetic.', 'this technique is commonly employed in scientific computing and numerical analysis to\\nimprove the accuracy of summation operations.', 'how pairwise summation works:\\n1. pairing the numbers:\\n\\uf0b7 given a sequence of numbers, they are paired up.', 'if there are an odd number of\\nelements, one number is left unpaired.', '2. pairwise addition:\\n\\uf0b7 within each pair, the two numbers are added together to create intermediate sums.', '3. summing the intermediate sums:\\n\\uf0b7 the intermediate sums obtained from pairwise addition are then summed together\\nusing the same pairwise summation method.', 'kahan summation\\nkahan summation, also known as compensated summation or kahan summation algorithm, is a\\nmethod used to reduce the numerical error that accumulates during the summation of a large\\nnumber of floating -point values.', 't his technique was introduced by william kahan, a renowned\\ncomputer scientist and mathematician, and it aims to improve the accuracy of summation\\noperations, especially in cases where a vast number of values need to be added together.', 'how kahan summation works:\\nin standard floating -point summation, when adding a small number to a large number, the small\\nnumber can be \"lost\" in the least significant bits of the large number, leading to a loss of precision.', 'kahan summation addresses this issue by using a com pensation term to keep track of the lost\\nprecision.', '1. initialization:\\n\\uf0b7 initialize the sum and the compensation term to zero.', '2. iterative addition:\\n\\uf0b7 for each number to be added:\\n\\uf0b7 add the number to the current sum.', '\\uf0b7 calculate the difference between the updated sum a nd the original sum (this\\ndifference is the lost precision).', '\\uf0b7 add this difference to the compensation term.', '3. final result:\\n\\uf0b7 the final result is the sum adjusted by the compensation term.', 'quad -precision summation\\nquad -precision summation refers to performing arithmetic operations with numbers represented\\nin quadruple -precision floating -point format.', 'in the ieee 754 floating -point standard, quadruple -\\nprecision is a 128 -bit data type, providing higher precision compared to single -precision (32 -bit) and\\ndouble -precision (64 -bit) floating -point number.']\n",
            "\n",
            "\n",
            "1297\n",
            "1\n",
            "\n",
            "\n",
            "['once all processors have completed sorting their portions, the sorted chunks can be combined to\\nproduce the final sorted dataset.', 'turn off or put to sleep any unused processors.', 'the energy consumption for your applicat ion can be estimated using the formula\\np = (n processors) × (r watts/processors) × (t hours)\\nwhere p is the energy consumption, n is the number of processors, r is the thermal design\\npower, and t is the application run time.', '\\uf0b7 weather forecasting: enables complex weather simulations and predictions.', '\\uf0b7 financial modelling : used for risk analysis, option pricing, and other complex financial\\ncalculations.', 'this classifica tion is named after michael j. flynn, who\\nintroduced it in 1966. flynn’s taxonomy is a useful tool for understanding different types of\\ncomputer architectures and their strengths and weaknesses.', 'this is the\\nsimplest type of computer architecture and is used in most traditional computers.', 'this type of\\narchitecture is used in applications such as image and signal processing.', '3. multiple instruction single data (misd ): in a misd architecture, multiple processors\\nexecute different instructions on the same data stream.', 'this type of architecture is not\\ncommonly used in practice, as it is difficult to find applications that can be decomposed into\\nindepen dent instruction streams.', '4. multiple instruction multiple data (mimd ): in a mimd architecture, multiple processors\\nexecute different instructions on different data streams.', 'this strategy is often used in applications where the same operation can be applied to different\\npieces of data independent ly.', \"scenario : imagine you're running a data analysis task on a large dataset of customer reviews for a\\nproduct.\", \"your goal is to perform sentiment analysis on each review to determine if it's positive,\\nnegative, or neutral.\", \"let's say you split\\nit into four subsets, each containing 250,000 reviews.\", 'you set\\nup four separate processing units (e.g., cpu cores or machines in a cluster), each responsible\\nfor analyzing one subset of reviews.', 'each processing unit loads its assigned subset of data.', '3. analysis : each processing unit applies the sentiment analysis model to its subset of reviews\\nindependently and simultaneously.', 'for in stance:\\n\\uf0b7 processing unit 1 analyzes reviews 1 to 250,000.', '\\uf0b7 processing unit 2 analyzes reviews 250,001 to 500,000.', '\\uf0b7 processing unit 3 analyzes reviews 500,001 to 750,000.', '\\uf0b7 processing unit 4 analyzes reviews 750,001 to 1,000,000.', '4. aggregation : as each processing unit finishes its analysis, it generates results, such as counts\\nof positive, negative, and neutral reviews within its subset.', 'these results are temporarily\\nstored.', '5. combining results : after all processing units have completed their work, you combine the\\nresults.', 'you sum up the counts from each processing unit to get the overall sentiment\\nanalysis results for the entire dataset.', 'each task\\ncan perform different operations and may not necessarily operate on the same data.', 'example: web server handling requests\\nconsider a web server handling incoming http requests.', 'the tasks include tasks like parsing the request, querying\\nthe database, and generating the response.', '2. task 2: database query\\n\\uf0b7 this task involves querying a database to fetch data related to the request, such as\\nuser information or product details.', '3. task 3: generating response\\n\\uf0b7 this task involves generating an html response based on the parsed request and\\ndata retrieved from the database.', \"example: manufacturing assembly line\\nlet's say we have a manufacturing assembly line for producing smartphones.\", 'the assembly line\\nconsists of three stages: a, b, and c. each stage represents a specific task in the smartphone\\nassembly process.', '1. stage a - component assembly :\\n\\uf0b7 worker a assembles the basic components of the smartphone, such as the circuit\\nboard, battery, and display.', 'once worker a finishes assembling a smartphone, it passes it\\nto stage b.', '2. stage b - software installation :\\n\\uf0b7 worker b installs the operating system and necessary software onto the smartphone\\nassembled by worker a. after software installation, the smartphone is passed to stage c.\\n3. stage c - quality control and packaging :\\n\\uf0b7 worker c checks the smartphone for quality control, ensuring that all components\\nare working correctly an d the software is functioning as intended.', 'if the smartphone passes\\nquality control, it is packaged and prepared for shipment.', 'in this example, each stage (a, b, and c) represents a processing step, similar to the stages in a\\nbucket -brigade parallelism sce nario.', 'a compiler is designed to translate your source code into a form the hardware can execute.', 'with these instructions at hand, an os manages executing these on the computer hardware.', 'these nodes are connected via a network, and they communicate and coordinate with each other\\nby passing messages.', 'in this architecture, each node opera tes independently and has its own local\\nmemory, and data sharing is achieved explicitly through message passing.', 'each node\\nworks on its subset of the data or a specific portion of the computation.', 'communication and\\ncoordination between nodes are essential, as tasks often depend on results or data computed on\\nothe r nodes.', 'this shared memory can be accessed and modified by any processor within the system.', 'vector units: multiple operations with one instruction\\nvector units, also known as vector processors, are specialized hardware units that can perform\\nmultiple operations with a single instruction.', 'these units are designed to process vectors, which are\\narrays of data elements, simultaneously.', 'vecto r processing is particularly useful in scenarios where\\nthe same operation needs to be performed on a large set of data elements .', 'discrete or dedicated gpus typically have a\\nlarge number of streaming multiprocessors and their own dram.', 'accessing data on a discrete gpu\\nrequires communication over a pci bus\\nan accelerator device, often referred to as an accelerator, is a specialized hardware component\\n(gpu) designed to perform specific types of computational tasks or workloads efficiently.', 'two nodes, each\\nwith two cpus, share the same dram memory.', 'a discrete gpu on the pci bus also attaches to one of the cpus.', 'though the cpus\\nshare main memory, these are commonly in different non -uniform memory access (numa)\\nregions.', 'each node has a multi -core cpu with an integrated and discrete gpu and some\\nmemory (dram).', 'these processes communicate and\\ncoordinate with each other by sending and receiving messages.', 'message passing is a method of\\ninter -process communication where data and instructions are exchanged between processes to\\nsynch ronize and share information.', 'fig 6 : the message passing library spawns processes.', 'the os places the processes on the cores\\nof two nodes.', 'the question marks i ndicate that the os controls the placement of the processes\\nand can move these during run time as indicated by the dashed arrows.', 'the threads are restricted to the node’s domain.', 'the question marks show that\\nthe os decides where to place the threads.', 'some memory is shared between threads.', 'simd allows a single instruction\\nto operate on multiple da ta elements simultaneously, which can significantly accelerate\\ncomputations involving large datasets.', '\\uf0b7 stream processing : through specialized processors\\nstream processing, often referred to as stream computing or data stream processing, is a computing\\nparadigm where data is continuously processed as it is generated or ingested, rather than being\\nstored in traditional databases or file systems.', 'stream proce ssing is particularly useful for handling\\nlarge volumes of real -time data from various sources, such as sensors, social media, financial\\ntransactions, and iot devices.', 'processed data, or output, transfers back to the cpu for file io or other\\nwork\\nsample application\\nwe start with a 2d problem domain of a region of space.', 'for purposes of illustration, we will use\\na 2d image of the krakatau volcano as our example.', 'the goal of our calculation could be to model\\nthe volcanic plume, the resulting tsunami, or the early detection of a volcanic eruption using\\nmachine learning.', 'for all of these options, calculation speed is critical if we want real -time result s\\nto inform our decisions.', 'for each cell in the computational domain, properties such as\\nwave height, fluid velocity, or smoke density are solved for according to physical laws.', 'ultimately, a\\nstencil operation or a matrix -vector system represents this discrete scheme\\nstep 2: define a computational kernel, or operation, to conduct on each element of the mesh\\nthe calculations on this discretized data are often some form of a stencil operation, so -called because\\nit involves a pattern of a djacent cells to calculate the new value for each cell.', 'this can be an average\\n(a blur operation, which blurs the image )gradient (edge -detection, which sharpens the edges in the\\nimage) or another more complex operation associated with solving physical sys tems described by\\npartial differential equations (pdes)\\nstep 3: vectorization to work on more than one unit of data at a time\\nsome processors have the ability to operate on more than one piece of data at a time; a capability\\nreferred to as vector operations.', 'the shaded blocks in figure illustrate how multiple data values\\nare operated on simultaneously in a vector unit in a proce ssor with one instruction in one clock\\ncycle.', 'when the work is split across nodes, the memory spaces for each node are distinct and\\nseparate.', 'here, 8×8 tiles are distributed across\\ngpu work groups.', 'these limits are determined by various factors and constraints\\nand play a crucial role in understanding the capabilities and limitations of a system.', 'application’s potential performance limits\\n\\uf0b7 flops (floating -point operations)\\n\\uf0b7 ops (operations) that include all types of computer instructions\\n\\uf0b7 memory bandwidth: rate at which the data is transferred\\n\\uf0b7 memory latency: time required for the first byte or word of data to be transferred\\n\\uf0b7 instruction queue (instruction cache)\\n\\uf0b7 networks\\n\\uf0b7 disk\\n\\uf0b7 machine balance: number of flops executed /memory bandwidth\\n\\uf0b7 arithmetic intensity : number of flops executed per memory operation\\nall of these limitations can be divided into two major categories:.', 'it includes all types of computer operations.', 'but to be able to do the operations, you must\\nget the data there.', 'this is where feeds come in.', 'feeds include the memory bandwidth through the\\ncache hierarchy, as well as network and disk bandwidth .', 'for many applications, the memory bandwidth limit can be difficult especially dealing with non-\\ncontiguous bandwidth.it is also known as strided memory access or non -contiguous memory\\naccess, refers to the manner in which data elements are accessed in memory.', 'in contrast to\\ncontiguous memory access, where elements are stored in consecutive memory locations, non -\\ncontiguous memory access invo lves accessing elements that are not stored sequentially in memory.', 'non -contiguous memory access:\\nnow, consider a situation where the array elements are scattered in memory with a stride of 2. this\\nis a non -contiguous memory access pattern:\\nin this case, accessing every second element (stride = 2) would mean accessing memory locations 1,\\n2, 3, 4, 5, etc., but the elements are not stored sequentially.', 'when your program needs to access such non -contiguous elements, it may lead to inefficiencie s\\ndue to increased cache misses and a higher likelihood of accessing data from main memory rather\\nthan the faster cache memory.', 'determine your hardware capabilities:\\nto determine the performance of hardware the following metrics are used :\\n\\uf0b7 the rate at which floating -point operations can be executed (flops/s)\\n\\uf0b7 the rate at which data can be moved between various levels of memory (gb/s)\\n\\uf0b7 the rate at which energy is used by your application (watts)\\nin determining hardware performance and calculating the metrics , we use a mixture of theoretical\\nand empirical measurements .', 'theoretical measurements provide an upper limit to what a system can achieve.', 'real -world validation is done by empirical measurements , they provide concrete evidence of how\\na system performs under real -world conditions, accounting for various factors like i/o operations,\\nnetwork latency, and concurrency issues.', 'one of the best tools for understanding the hardware you run is the lstopo program (graphical view)\\nand lscpu for text view .', 'lstopo is bundled with the hwloc package that com es with nearly every mpi\\ndistribution.', 'this command outputs a graphical view of the hardware on your system.', 'figure below\\nshows the output for a mac laptop in graphical view.', 'text view\\nthe information from the lscpu command and the /proc/cpuinfo file helps to determine the number\\nof processors, the processor model, the cache sizes, and the clock frequency for the system\\ncalculating theoretical maximum flops\\ntheoretical flops=number of cores×clock speed×flops per cycle per core\\nwhere:\\n\\uf0b7 number of cores: this represents the total number of processor cores in the computing\\nsystem.', '\\uf0b7 clock speed: this indicates the clock speed of each core in the system, typically measured\\nin hertz (hz) or gigahertz (ghz).', 'it represents the number of cycles th e processor can\\nexecute per second.', '\\uf0b7 flops per cycle per core: this signifies the number of floating -point operations a core can\\nperform in a single clock cycle.', 'modern processors often perform multiple flops per cycle\\ndue to features like simd (single inst ruction, multiple data) operations.', \"for example, let's consider a system with 4 cores, each operating at 3.0 ghz, and capable of\\nexecuting 4 flops per cycle per core (assuming simd operations are utilized):\\ntheoretical flops=4\\u2009cores×3.0\\u2009ghz×4\\u2009flops per cycle per core\\ntheoretical flops=48\\u2009gflops\\nthe memory hierarchy and theoretical memory bandwidth\\nwe can calculate the theoretical memory bandwidth of the main memory using the memory chips\\nspecifications.\", 'the general formula is b t = mtr × mc × tw × ns = data transfer rate × memory channels × bytes\\nper access × sockets\\nprocessors are installed in a socket on the motherboard.', 'the motherboard is the main system board\\nof the computer, and the socket is the lo cation where the processor is inserted.', 'most motherboards\\nare single -socket, where only one processor can be installed.', 'empirical measurement of bandwidth and flop\\nthe empirical bandwidth is the measurement of the fastest rate that memory can be loaded from\\nmain memory into the processor.', 'if a single byte of m emory is requested, it takes 1 cycle to retrieve\\nit from a cpu register.', 'if it is not in the cpu register, it comes from the l1 cache.', 'if it is not in the l1\\ncache, the l1 cache loads it from l2 and so on to main memory.', 'if it goes all the way to main\\nmemo ry, for a single byte of memory, it can take around 400 clock cycles.', 'this time required for the\\nfirst byte of data from each level of memory is called the memory latency .', 'two different methods are used for measuring the bandwidth: the stream benchmark and the\\nroofline model measured by the empirical roofline toolkit.', 'key differences:\\n\\uf0b7 focus: stream primarily focuses on memory bandwidth, providing quantitative\\nmeasurements.', 'in contrast, the roofline model provides a graphical representation of\\nperformance bottlenecks, considering both computational capabilities and memory\\nbandwidth.', '\\uf0b7 representation: stream results in a numerical measurement (memory bandwidth in bytes\\nper second), while the roofline model is a graphical representation that helps v isualize\\nperformance limitations.', \"\\uf0b7 insights: stream provides detailed insights into memory subsystem performance, whereas\\nthe roofline model offers a high -level overview of an application's performance efficiency\\nconcerning hardware constraints.\", 'calculatin g the machine balance between flops and bandwidth\\nthe machine balance is the flops divided by the memory bandwidth.', 'we can calculate both a theoretical machine balance (mb t) and an empirical machine balance (mb e)\\nlike so:\\nmb t = f t / b t\\nmb e = f e / b e\\ncharacterizing your application: profiling\\nnow that you have some sense of what performance you can get with the hardware, you need to\\ndetermine what are the performance characteristics of your application.', 'additionally, you should\\ndevelop an understandi ng of how different subroutines and functions depend on each other .', 'profiling tools :\\nusing call graphs for hot -spot and dependency analysis\\nin the context of parallel programming, call graphs are diagrams that represent the calling\\nrelationships between different functions or methods in a parallel program.', \"they illustrate how\\nfunctions or tasks invoke each other and provide a visual representation of the program's control\\nflow.\", 'by analyzing these call graphs,\\ndevelopers can identify hot -spots —functions or tasks that consume a significant amount of\\ncomputational time.', 'empirical measurement of processor clock frequency and energy consumption\\nempirical measurement of processor clock frequency:\\n1. profiling tools: profiling tools like intel vtune profiler or amd codexl can provide insights\\ninto various performance metrics, including processor clock frequency.', 'these tools often offer\\nvisualizations and detailed reports for better analysis.', '2. benchmarking suites: benchmarking tools like spec cpu benchmarks or hpc chal lenge\\nbenchmarks often include components that measure processor clock frequencies.', \"running\\nthese benchmarks can provide detailed information about the processor's performance\\ncharacteristics.\", '2. empirical measurement of energy consumption:\\n1. power measureme nt tools: use power measurement tools and hardware devices to\\nmeasure the power consumption of your system.', 'power meters and sensors can be attached\\nto the system to measure real -time power usage.', \"tools like intel power gadget or linux's\\npowerstat can help measure power usage.\", '2. energy profilers: some profiling tools, like intel vtune profiler, also offer energy profiling\\ncapabilities.', 'they can provide insights into energy consumption patterns at different parts of\\nyour code.', 'these tools often correlate energ y consumption with specific functions or code\\nregions.', 'tracking memory during run time\\ntracking memory usage during runtime in parallel computing is crucial for optimizing performance,\\ndetecting memory leaks, and ensuring efficient memory management.', 'several techniques and tools\\ncan help you monitor memory usage in parallel applications.', 'here are some approaches to tracking\\nmemory during runtime in parallel computing environments:\\nprofiling tools:\\n1. valgrind massif: valgrind is a powerful instrumentation framework.', 'massif, a valgrind tool, can\\nprofile heap memory usage over time, showing memory consumption patterns.', \"it's particularly\\nuseful for detecting memory leaks and understanding how memory usage evolves during\\nprogram execution.\", '2. intel vtune profiler : vtune profiler provides memory analysis capabilities, including memory\\nusage tracking.', 'it can analyze memory consumption at various levels, from individual functions\\nto entire applications, in both serial and parallel contexts.', '3. openmp/mpi memory profiler s: many parallel programming frameworks like openmp and\\nmpi provide their memory profiling tools.', 'for example, openmp has tools like score -p, and mpi\\nhas memory profiling features integrated into mpi implementations.', 'they encompass various\\naspects of system performance, including algorithmic efficiency, but also consider factors\\nrelated to specific hardware, software, and real -world scenarios.', 'it is done for faster access to\\nelements.', 'it can be any type of data, such as a file, a\\npassword, or a message.', 'spatial information can also be stored in adaptive mesh refinement (amr) format.amr is a numerical\\nsimulation technique used in computational mathematics, fluid dynamics, and other fields to imp rove\\nthe efficiency and accuracy of simulations.', 'amr systems maintain multiple levels of grids.', 'each level\\nrepresents a different resolution of the simulation domain.', 'finer grids cover smaller areas, providing\\nhigh resolution, while coarser grids cover lar ger areas, offering lower resolution.', 'in open addressing, there are a few choices that\\nwe can use as the trial for the next open slot.', 'this\\nprocess is used to transfer data (usually physical quantities like temperature, pressure, or velocity)\\nfrom one mesh or grid to another.', '\\uf0b7 if open addressing is used, probe the adjacent cells until the object is found or an\\nempty cell is encountered.', 'the first sweep is\\ncalled an upsweep, though it is more of a right sweep.', 'the second phase, known as the downsweep phase, is more of a left sweep.', 'the output of upsweep\\nis provided as input to downsweep.']\n",
            "\n",
            "\n",
            "3288\n",
            "3\n",
            "\n",
            "\n",
            "['bucket -brigade parallelism :\\na bucket brigade is a method of manually transporting items or materials from one location to\\nanother by forming a line of people, each of whom carries an item and passes it to the next person.', 'this technique is similar to how buckets of water might be passed along a line of people to put out a\\nfire, which is where the term \"bucket brigade\" originated.', 'hash function\\nhashing is a popular technique for quickly storing and retrieving data.', 'hashing is a technique or process\\nof mapping keys, values into the hash table by using a hash function.', 'the efficiency of mapping depends on the efficiency of the hash function used.', 'components of hashing\\n1. hash function: the hash function itself is a crucial component.', 'it takes an input and produces\\na fixed -size hash value.', 'the hash function ensures that the same input always produces the\\nsame hash value and that even a small change in the input results in a significantly different\\nhash value.', '2. input data: this is the data that you want to hash.', \"3. hash value: also known as the hash code or hash keys , it's the output of the hash function\\nafter processing the input data.\", 'the hash value is typically a index into hash table .', '4. collision: a collision occurs when two different inputs produce the same hash value.', '5. bucket or slot: in the context of hash tables , a bucket or slot is a location where data is stored\\nbased on its hash value.', 'hash tables consist of an array of these buckets, and the hash value\\ndetermines which bucket a particular piece of data will be stored in.', '6. hash table: a hash table is a data str ucture that uses hashing to implement an associative\\narray, a structure that can map input to values.', 'it consists of an array of buckets where data is\\nstored based on its hash value.', 'hash tables allow for efficient insertion, deletion, and lookup\\noperation s.\\n7. load factor: the load factor of a hash table is the ratio of the number of stored elements to\\nthe total number of buckets.', 'a high load factor can lead to increased collisions and decreased\\nperformance, so hash tables are often resized and rehashed if th e load factor exceeds a certain\\nthreshold.', '8. sparsity : the sparsity of a hash table is the ratio of the number of empty buckets to the total\\nnumber of buckets 9. collision resolution: techniques used to handle collisions include chaining (where each bucket\\ncontains a linked list of items that hash to the same index) and open addressing (where the\\nalgorithm searches for the next open slot in the hash table).', 'spatial hashing\\nspatial data is any type of data that directly or indirectly references a specific geographical area or\\nlocation.', 'sometimes called geospatial data or geographic information, spatial data can also numerically\\nrepresent a physical object in a geographic coordinate system.', 'however, spatial data is much more\\nthan a spatial component of a map.', 'spatial data can be stored in either vector format or raster format\\nvector\\nraster\\nspatial hashing is a technique where the key is based on spatial information.', 'spatial hashing is a\\ntechnique used to locate objects in a 3d space.', 'it involves dividing a large space into smaller, grid -like\\ncells, and assigning each object in the space to the cell that contains it..the basic principle is to map\\nobjects onto a grid of buckets arranged in a regular pattern .', 'the same data of amr can be even stored in perfect hashtable or compact hash table.', 'perfect hashing is a technique used to eliminate collisions entirely in hash tables, ensuring that each\\nkey maps to a unique index without any conflicts.', 'in traditional hash tables, collisions can occur when\\nmultiple keys hash to the same index, requiring additional data structures like linked lists or open\\naddressing techniques to resolve these collisions.', 'perfect hashing, on the oth er hand, aims to design a\\nhash function and data structure in such a way that collisions never occur.', 'compact hashing is a technique used to design hash functions in a way that minimizes the memory\\nrequired for hash tables.', 'it focuses on creating hash func tions that distribute keys uniformly across the\\navailable slots in a hash table while keeping the table small in size.', 'the objective is to use as few bits\\nas possible per key, reducing the memory footprint of the hash table.', 'because of the compression to a compact hash, two entries try to store their value in bucket 1. the\\nsecond entry sees that there is already a value there, so it looks for the next open slot in a technique\\ncalled open addressing.', 'in open addressing, we look for the next open slot in the hash table and\\nstore the value in that slot.', 'there are other hashing methods than open addressing, but these often\\nrequire the ability to allocate memory during an operation.', 'a llocating memory is more difficult on\\nthe gpu, so we stick with open addressing where collisions are resolved by finding alternate storage\\nlocations within the already allocated hash table.', 'these are\\nlinear probing —where the next entry is just the next bucket in sequence until an open bucket is\\nfound\\nquadratic probing —where the increment is squared so that the attempted buckets are +1, +4,\\n+9, and so forth f rom the original location\\ndouble hashing —where a second hashing function is used to jump to a deterministic, but pseudo -\\nrandom distance from the first trial location\\non spatial data we can perform, four spatial operations.', '\\uf0b7 neighbor finding —locating the one or two neighbors on each side of a cell\\n\\uf0b7 remapping —mapping another amr mesh onto a current amr mesh\\n\\uf0b7 table lookup —locating the intervals in the 2d table to perform the interpolation\\n\\uf0b7 sorting —a 1d or 2d sort of the cell data\\nthese can be implemented either by perfect hashing or compact hashing.', 'neighbor finding using a spatial perfect hash\\nthe steps involved are\\n\\uf0b7 allocate a spatial hash the size of the finest level of the cell -based amr mesh\\n\\uf0b7 for each cell in the amr mes h, write the cell number to the hash buckets underlying the cell\\n\\uf0b7 compute the index for a finer cell one cell outside the current cell on each side\\n\\uf0b7 read the value placed in the hash bucket at that location.', 'example: each cell writes its cell number to the hash buckets it covers.', 'right neighbor of cell 2 1 is\\nat col 8, row 3. look up in hash and it is cell 26 .', 'remapping —mapping another amr mesh onto a current amr mesh (perfect hash)\\nremapping from one mesh to another is a common operation in computational simulations,\\nespecially in fields like computational fluid dynamics (cfd) and finite element analysis (fea).', 'you have two grids or meshes: the source mesh (from which you\\nwant to remap data) and the target mesh (to which you want to map the data ).', 'table lookup(perfect hash )\\nto perform a table lookup in spatial hashing, you need to map the position of an object to a\\nunique key (hash value), which determines the index of the bucket in the hash table where\\nthe object should be retrieved.', \"\\uf0b7 perform a lookup in the hash table at the calculated index to find the bucket\\ncorresponding to the object's spatial location.\", '\\uf0b7 if chain ing (linked lists in each bucket) is used for collision resolution, traverse the\\nlinked list in the selected bucket to find the object.', 'sorting mesh data using a spatial perfect hash\\nwe can demonstrate the hash sort operation .', 'the minimum difference between values is 2.0, so\\nthe bucket size of 2 guarantees that there are no collisions.', 'the minimum value is 0, so the bucket\\nlocation can be calcul ated with bi = xi /δmin = xi /2.0.', 'we could store either the value or the index\\nin the hash table.', 'for example, 8, the first key, could be stored in bucket 4 .']\n",
            "\n",
            "\n",
            "1361\n"
          ]
        }
      ],
      "source": [
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "    clustered_sentences[cluster_id].append(sentences_list[sentence_id])\n",
        "\n",
        "for k,v in clustered_sentences.items():\n",
        "    print(k,end=\"\\n\\n\\n\")\n",
        "    print(v,end=\"\\n\\n\\n\")\n",
        "    x=\" \".join (clustered_sentences[k])\n",
        "    y=x.split()\n",
        "    print(len(y))\n",
        "\n",
        "# clustered_sentences[1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "sp8nZhO6fg6T"
      },
      "outputs": [],
      "source": [
        "def generate_summary(text_chunk):\n",
        "    # Defining the template to generate summary\n",
        "    template = \"\"\"\n",
        "    Generate a very brief and coherent summary of the given text, ensuring minimal word repetition and maintaining brevity\n",
        "    ```{text}```\n",
        "    SUMMARY:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "    summary = llm_chain.run(text_chunk)\n",
        "    torch.cuda.empty_cache()\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZGoLL2e0fkU1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "torch.cuda.empty_cache()\n",
        "import os\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqX_lq-ie_Mc",
        "outputId": "ae426680-9772-4eaf-8a28-8182a5ae8910"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cluster :  2\n",
            "['\\n\"computing\" refers to the process of using computers and computer systems to perform various\\ntasks, such as data processing, information storage, and solving complex problems .', 'this computing can be either done in serial way known as serial computing or in parallel wa y\\nknown as parallel computing\\nserial computing:\\nserial computing refers to traditional computing where tasks are executed sequentially, one after\\nthe other, using a single processor.', 'in serial computing, each instruction or task must wait for the\\nprevious one to complete before it can be executed.', 'this approach limits the speed and efficiency\\nof processing, especially when dealing with complex or time -consuming tasks.', 'computing\\nserial or\\nsequential parallel example of serial computing : consider a task of sorting a large dataset of numbers in ascending\\norder.', 'in a serial computing environment, a single processor would go through the entire dataset,\\ncomparing and rearranging numbers one pair at a time until the entire dataset is sorted.', 'parallel computing\\nparallel computing is a type of computation in which multiple processors or computers work together\\nto solve a problem.', 'instead of one single processor handling the entire task, parallel computing divides\\nthe task into smaller sub -tasks that can be processed simultaneously.', 'this simultaneous processing\\ncan lead to significant improvements in computational speed and efficien cy.', 'example of parallel computing: using the same example of sorting a large dataset, parallel computing\\nwould involve dividing the dataset into smaller chunks, and each chunk is sorted independently by a\\nseparate processor.', 'these processors work in parall el, sorting their respective chunks simultaneously.', 'definition parallel computing is the practice of identifying and exposing parallelism in algorithms,\\nexpressing this in our software, and understanding the costs, benefits, and limitations of the chosen\\nimplementation.', 'benefits of parallel computing\\n\\n\\uf0b7 faster run time with more compute cores: parallelization involves dividing a task into\\nsmaller sub -tasks that can be executed simultaneously, utilizing multiple cores to process the\\ndata.', 'this approach can significantly reduce the time required to complete the task, as each\\ncore works on a separate portion of the problem concurrently.', '\\uf0b7 larger problem sizes with more compute nodes : with more nodes, you can break down your\\nproblem into smaller pieces that each node can work on simultaneously, which is especially\\nbeneficial for handling larger datasets and more complex simulations .', '\\uf0b7 energy efficiency by doing more with less: in the context of parallel computing, the concept\\nof \"doing more with less\" often revolves around optimizing energy efficiency while achieving\\nbetter computational performance.this can be achieved by making use of dynamic resource\\nallocation and workload consolidation to ensure that the number of processors used is\\nproportional to the workload.', '\\uf0b7 scalability : parallel computing can be easily scaled by adding more processors, which further\\nenhances performance.', 'serial computing does not scale in this manner, as it relies on a single\\nprocessor.', '\\uf0b7 parallel computing can reduce costs: as technology advances, the cost of individual\\nprocessors and memory decreases.', 'parallel computing systems can take advantage of these\\ncost reductions, making it more economical to build high -performance computing clusters or\\ndata centers .', 'applications of parallel computing:\\n\\uf0b7 scientific simulations: used in fields such as physics, chemistry, and engineering for\\ncomplex simulations.', '\\uf0b7 big data processing: parallel computing is crucial in processing vast amounts of data in\\nfields like data analytics and machine learning.', '\\uf0b7 video and image processing: parallelism accelerates tasks like video rendering and\\nimage recognition.', \"fundamental laws\\nfundamental laws in parallel computing, such as amdahl's law and gustafson's law, are essential\\nfor understanding the limitations and possibilities of parallel processing.\", 'these laws provide valuable\\ninsights into how the speedup of a parallel algorithm is affected by various factors .', 'what is speedup?', 'speedup in parallel computing refers to the performance improvement achieved by using multiple\\nprocessors or computing resources to solve a problem compared to using a single processor.', 'it is a\\nmeasure of how much faster a parallel algorithm or system can complet e a task compared to a serial\\n(single -processor) implementation of the same task.', 'speedup is a crucial metric for evaluating the\\neffectiveness of parallel computing systems.', 'the speedup ( s) can be calculated using the following formula:\\ns=tserial / tparallel\\nwhere:\\n\\uf0b7 tserial is the execution time of the task using a single processor (serial execution time).', 'tparallel is the execution time of the task using multiple processors (parallel execution time).', 'a speedup value greater than 1 indicates that the parallel implementation is faster than the serial\\nimplementation.', 'ideally, in a perfectly parallelizable task, doubling the number of processors would ideally halve the execution time, resulting in a speedup of 2. however, achieving perfect linear\\nspee dup is rare in real -world scenarios due to factors such as communication overhead, load\\nbalancing issues, and synchronization constraints between processors.', \"amdahl's law is a fundamental principle in parallel computing that expresses the potential\\nspeedup of a parallel algorithm as a function of the proportion of the algorithm that can be\\nparallelized.\", 'it was formulated by gene amdahl in 1967 and is represented by the fol lowing\\nformula:\\nwhere:\\n\\uf0b7 speedup is the improvement in performance achieved by parallelizing a computation\\ncompared to executing it sequentially.', '\\uf0b7 p is the proportion of the algorithm that can be parallelized (a value between 0 and 1).', \"\\uf0b7 s is the serial fraction\\n\\uf0b7 n – no.of processors/nodes/cores\\namdahl's law highlights the limitations of parallel computing.\", 'it states that the speedup of a\\nprogram using multiple processors in parallel computing is limited by the sequential fraction of\\nthe program.', 'in other words, if only a portion of a program can be parallelized (the rest being\\ninherently sequential), then no matter how many processors are added, there will always be a\\nlimit to the speedup that can be achieved.', \"for example, if 90% of a program can be parall elized (p = 0.9) and the parallel portion runs on\\n5 processors , the maximum speedup that can be achieved according to amdahl's law is:\\nspeedup= 1/(0.1+(0.9/5))=3.57\\nin this case, even though 90% of the program can be parallelized and runs on 5 processors , the\\nmaximum speedup achievable is approximately 3.57 times faster compared to the sequential\\nexecution due to the presence of the 10% sequential portion.\", 'fig : speedup for a fixed -size problem according to amdahl’s law is shown as a function of the\\nnumber of processors.', 'lines show ideal speedup when 100% of an algorithm is parallelized,\\nand for 90%, 75%, and 50%.', 'amdahl’s law states that speedup is limited by the fractions of\\ncode that remain serial.', \"gustafson's law , formulated by john l. gustafson, provides a different perspective on parallel\\ncomputing compared to amdahl's law.\", \"the basic idea behind g ustafson's law is\\nthat as the size of the problem increases, the impact of the parallelizable portion of the program\\nbecomes more significant, leading to better scalability.\", 'in other words, with larger problem sizes,\\nparallel systems can achieve higher lev els of speedup.', \"the formula for gustafson's law is as follows:\\nspeedup(n) = n – s * (n – 1) where n is the number of processors, and s is the serial fraction\\nstrong scaling and weak scaling are two different m etrics used to evaluate the performance of\\nparallel computing systems, and they provide insights into how well a parallel algorithm or\\napplication can handle an increasing workload or an increasing number of processors.\", \"here's a\\ncomparison of strong scalin g and weak scaling:\\nstrong scaling:\\ndefinition: strong scaling measures how the execution time of a fixed problem size decreases\\nas the number of processors increases.\", 'in other words, it assesses how well a parallel system\\nperforms when the size of the problem remains constant, but the number of proces sors used\\nto solve the problem increases.', 'objective: the goal of strong scaling is to reduce the execution time for a fixed problem size by\\nutilizing more processors.', 'it aims to speed up the solution of a specific problem.', 'scenario: strong scaling is applicable when the size of the problem is fixed, and the aim is to\\nsolve that problem faster by employing additional processors.', '2. weak scaling:\\ndefinition: weak scaling measures how the execution time changes as both the problem size\\nand the number of processors increase proportionally.', 'in other words, it assesses how well a\\nparallel system can handle larger workloads by adding more processors as the problem size\\ngrows.', 'objective: the goal of weak scaling is to maintain a constant workload per processor as the size\\nof the problem and the number of processors increase.', 'it aims to solve larger problems in\\napproximately the same amount of time per processor.', 'scenario: weak scaling is applicable when the problem size can be increased, and the aim is to\\nhandle larger workloads by distributing the computational load across a larger number of\\nprocessors.', \"parallel approaches (flynn’s classification)\\n\\nflynn's classification is essential in the field of parallel computing because it provides a framework\\nfor understanding and categorizing different types of computer architectures based on the number\\nof instruction streams and data streams.\", 'the taxonomy highlights the importance of parallelism in modern computing and shows how\\ndifferent types of parallelism can be exploited to improve performance.', 'it helps in designing and\\nanalyzing parallel processing systems\\n\\n1. single instruction single data (sisd ): in a sisd architecture, there is a single processor\\nthat executes a single instruction stream and operates on a single data stream.', '2. single instruction multiple data (simd ): in a simd architecture, there is a single\\nprocessor that executes the same instruction on multiple data streams in parallel.', 'this type of architecture is used in\\ndistributed computing, parallel processing, and other high -performance computing applications.', 'parallel strategies\\nparallel strategies\" typically refer to techniques and methods for parallel processing, which is the\\nsimultaneous execution of multiple tasks or processes to improve the efficiency and performance\\nof a computer system.', 'parallel strategies are commonly used in various computing domains, such\\nas high -performance computing and distributed systems, to speed up computations and handle\\nlarge volumes of data.', 'here are some common parallel s trategies:\\ndata parallel approach\\ndata parallelism involves performing the same operation on multiple data elements simultaneously.', 'the sentiment analysis process is computationally intensive, and you want to\\nspeed it up using data parallelism.', 'data parallelism in sentiment analysis :\\n1. data preparation : you have a dataset of 1,000,000 customer reviews.', 'to apply data\\nparallelism, you divide this dataset into smaller, non-overlapping subsets.', '2. parallel processing : you have a sentiment analysis model that can analyze reviews.', 'task parallelism(main -worker approach)\\ntask parallelism involves executing multiple independent tasks or processes in parallel.', 'task\\nparallelism is common in applications where different tasks can be performed concurrently without\\ndependencies between them.', 'in the main -worker approach, one processor schedules and distributes the tasks for all the workers,\\nand each worker checks for the next work item as it returns the previous completed task .', 'each i ncoming request is an independent\\ntask that can be processed concurrently.', 'in a task parallelism scenario:\\n1. task 1: parsing request\\n\\uf0b7 this task involves pars ing the incoming http request to extract information like the\\nrequested url, parameters, and headers.', 'in a task parallelism setup, these tasks can be executed concurrently by multiple threads or\\nprocesses, allowing the server to handle multiple incoming requests simultaneously without waiting\\nfor one task to complete before starting the next.', 'in parallel computing, the concept of bucket -brigade parallelism involves breaking down a task into\\nsmaller subtasks, where each subtask is processed independently and passed to the next processing\\nunit for further computation.', 'this technique allows for efficient parallel processing of tasks and is\\noften used in scenarios where tasks can be divided into smaller, manageable parts.', 'parallel speedup versus comparative speedups.', 'parallel speedup and comparative speedup are two different metrics used to evaluate the performance\\nimprovement achieved by parallel processing.', 'parallel speedup measures how much faster a parallel algorithm runs compared to its sequential\\n(single -processor) counterpart.', 'it quantifies the performance improvement gained by using multiple\\nprocessing units in parallel.', 'parallel speedup is calculated using the followi ng formula:\\nparallel speedup=sequential execution time/parallel execution\\nin this formula:\\n\\uf0b7 sequential execution time is the time taken by the algorithm to execute sequentially on a\\nsingle processor.', '\\uf0b7 parallel execution time is the time taken by the paralle l algorithm to execute on multiple\\nprocessors.', 'comparative speedup : comparative speedup is between architectures.', 'this is usually a\\nperformance comparison between two parallel implementations or other comparison between\\nreasonably constrained sets of hardware.', 'for example, it may be between a parallel mpi\\nimplementation on all the cores of the node of a computer versus the gpu(s) on a node\\nhow parallel computing works\\nas a developer, you are responsible for the application software layer, which includes your source code.', 'in the source code, you make choices about the programming language and parallel software interfaces\\nyou use to leverage the underlying hardware.', 'additionally, you decide how to break up your work into\\nparallel units.', 'parallel approach models are used to express parallelization in an application software layer\\nthat gets mapped to the computer hardware through the compiler and the os.', 'parallel computing\\napproaches involve various models and paradigms that define how tasks are divided, coordinated, and\\nexecuted in parallel systems.', 'here are some common parallel computing approach models:\\nhardware models\\ndistributed memory architecture: a cross -node parallel method :\\nparallel approach models\\nhardware models\\ndistributed memory architecture:\\na cross -node parallel method :\\nshared memory architecture: an\\non-node parallel method\\nvector units: multiple operations\\nwith one instruction\\naccelerator device: a special -\\npurpose add-on processor\\nsoftware models\\nprocess -based parallelization :\\nmessage passing\\nthread -based parallelization :\\nshared data via memory\\nvectorization : multiple\\noperations with one instruction\\nstream processing : through\\nspecialized processors\\ndistributed memory architecture, also known as distributed memory parallelism, is a parallel\\ncomputing method where multiple processors or nodes in a cluster have their own private memory.', 'in the context of distributed memory architecture, a \"cross -node parallel method\" refers to parallel\\nprocessing techniques that involve distributing tasks across multiple nodes in a cluster.', 'shared memory architecture: an on-node parallel method\\nin shared memory architecture, multiple processors or cores share a single, unified memory space.', 'on -node\\nparallelism, within the context of shared memory architecture, ref ers to parallel processing\\ntechniques that occur on a single computing node.', 'in this approach, multiple threads or processes\\nrun concurrently on the same node, accessing shared memory to perform computations .', 'vector processing example with four array elements operated on simultaneously\\naccelerator device: a special -purpose add-on processor\\ngpus come in two varieties: integrated and discrete.', 'accelerators are typically used in conjunction with a central processin g unit (cpu) and are especially\\nwell-suited for workloads that can benefit from parallel processing and offloading certain tasks from\\nthe cpu.', 'accelerators are sometimes called \"add -on processors\" because they augment the\\nprocessing capabilities of a syste m.\\ngeneral heterogeneous parallel architecture model\\nnow let’s combine all of these different hardware architectures into one model .', 'each cpu is a dual -core processor with an\\nintegrated gpu.', 'this means that accessing the second cpu’s memory is more expensive than getting at it’s\\nown memory\\nfig 5: a general heterogeneous parallel architecture model consisting of two nodes connected by\\na network.', 'software models\\nthe programmer must first expose the parallelization, determine the best technique to operate in\\nparallel, and then explicitly direct its operation in a safe, correct, and efficient manner.', 'the following\\nmethods are the most common techniques for parallelization\\n\\uf0b7 process -based parallelization : message passing\\nprocess -based parallelization, particularly through message passing, is a common approach in\\nparallel computing.', 'it involves dividing a task into multiple processes or threads that run\\nindependently on separate computing nodes or cores.', 'this approach is widely used in distributed memory systems,\\nsuch as clusters and supercomputers.', 'the os also allocates\\nmemory for each process from the node’s main memory\\n\\uf0b7 thread -based parallelization : shared data via memory\\nthread -based parallelization involves dividing a task into multiple threads that share the same\\nmemory space within a single process.', 'these threads can run concurrently on multiple cpu cores,\\nand they communicate and coordinate by accessing shared data in the shared memory.', 'this\\napproach is commonly used in multi -core processors and symmetric multiprocessing (smp) systems.', 'fig 7: the application process in a thread -based approach to parallelization spawns\\nthreads.', '\\uf0b7 vectorization : multiple operations with one instruction\\nvectorization is a parallel computing technique that enables processors to perform multiple\\noperations with a single instruction.', 'it takes advantage of simd (single instruction, multiple data)\\ncapabilities found in modern processors, including cpus and gpus.', 'specialized processors designed for stream processing accelerate the\\nanalysis and manipu lation of data streams, ensuring timely and efficient processing\\nin the stream processing approach, data and compute kernel are offloaded to the gpu and its\\nstreaming multiprocessors.', 'define a computational kernel (operation) to conduct on each element of the mesh\\n3. add the following layers of parallelization on cpus and gpus to perfo rm the calculation:\\nvectorization —work on more than one unit of data at a time\\n4. threads —deploy more than one compute pathway to engage more processing cores\\n5. processes —separate program instances to spread out the calculation into separate memory\\nspaces\\n6. off -loading the calculation to gpus —send the data to the graphics processor to calculate\\nstep 1: discretize the problem into smaller cells or elements\\nthe domain is discretized into cells.', 'step 4: threads to deploy more than one compute pathway to engage more processing cores\\nbecause most cpus today have at least four processing cores, we use threading to operate the cores\\nsimultaneously acros s four rows at a time.', 'step 5: processes to spread out the calculation to separate memory spaces\\nwe can further split the work between processors on two desktops, often called nodes in parallel\\nprocessing.', 'performance limits and profiling\\nin parallel processing, understanding performance limits and profiling the application are crucial\\nsteps to optimize the execution of parallel programs.', 'performance limits refer to the maximum achievable performance of a computing system or\\napplication un der specific conditions.', 'understanding\\nthese performance limits is essential for designing efficient algor ithms, optimizing software, and\\nchoosing appropriate hardware configurations.', 'it also guides researchers and engineers in\\ndeveloping new technologies to overcome existing limitations and improve overall computing\\nperformance.', 'profiling tools are used to ga ther detailed information about the behavior of a parallel program.', 'by\\nunderstanding performance limits, utilizing profiling tools, and optimizing the code based on the\\nprofiling results, developers can enhance the efficiency of parallel applications, lea ding to improved\\nspeedup and overall performance.', 'speeds are how fast operations can\\nbe done.', 'for instance, in\\nparallel computing, theoretical analysis can reveal the maximum speedup or efficiency that a\\nparallel algorithm can achieve in an ideal scenario .', 'dual -socket motherboards are more\\ncommon in high -performance computing systems.', 'two processors can be installed in a dual -socket\\nmotherb oard, giving us more processing cores and more memory bandwidth.', \"analyzing call graphs in parallel programming can provide valuable insights in to the program's\\nstructure, dependencies, and potential performance optimizations .\", 'optimizing these hot -spots is e ssential for improving overall parallel program\\nperformance.', 'parallel algorithms and patterns\\na parallel algorithm is a step -by-step computational procedure or set of rules designed to be\\nexecuted on parallel computing architectures.', 'these algorithms are specifically crafted to take\\nadvantage of parallel processing capabilities, where multiple proces sors or cores can work together\\nto solve a problem.', 'parallel patterns are like reusable blueprints that help programmers apply proven methods to solve\\nspecific types of problems efficiently.', 'these patterns guide the decomposition of tasks and data,\\nprovidi ng a framework for creating effective parallel algorithms .', 'example : parallel algorithm for finding the maximum element:\\nsuppose you have a large array of numbers, and you want to find the maximum element using a\\nparallel algorithm based on the \"divide an d conquer\" pattern.in this example, the \"divide and\\nconquer\" pattern is applied to find the maximum element in an array.', 'algorithm analysis for parallel computing applications\\nthe goal of algorithm analysis is to compare different algorithms that are used to solve the same\\nproblem.', 'performance models versus algorithmic complexit y\\nperformance models are broader and more practical in nature.', 'performance model considerations:\\n\\uf0b7 hardware differences: different computers might execute the same algorithm at\\ndifferent speeds due to variations in processor capabilities.', '\\uf0b7 compiler optimizations: compilers can optimize the code differently, affecting the\\nexecution time.', '\\uf0b7 memory optimization: for very large arrays, consider memory -efficient data structures\\nor algorithms to reduce memory usage.', 'parallel algorithms\\nparallel algorithms are designed to efficiently solve computational problems by utilizing multiple\\nprocessing units (such as cpu cores, gpus, or distributed computing nodes) simultaneously.', 'they are\\ncrucial in high -performance computing (hpc) and parallel processing environments where large\\ndatasets and complex computations need to be handled efficiently.', 'parallel algorithms aim to break\\ndown tasks into smaller subtasks that can be processed independently and concurrently, leading to\\nsignificant speedup in overall computation time.', 'here are some common types of parallel algorithms: \\uf0b7 parallel merge sort: divide the sorting task into smaller parts, sort them independently, and\\nthen merge the sorted parts in parallel.', '\\uf0b7 parallel quicksort: a parallel version of th e quicksort algorithm that partitions the data and\\nsorts partitions concurrently.', 'parallel global sum\\nthe \"parallel global sum\" refers to the problem of computing the sum of elements across multiple\\nprocessors or nodes in a parallel or distributed computing environment.']\n",
            "31\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31\n",
            "cluster :  0\n",
            "['this process\\noccurs sequentially, and each comparison and rearrangement must wait for the previous one to\\nfinish.', \"unlike amdahl's law, which focuses on fixed problem sizes,\\ngustafson's law takes into account varying problem sizes.\", '1. discretize (break up) the problem into smaller cells or elements\\n2 .', 'step 6: off -loading the calculation to gpus\\non a gpu, the vector length is much larger than on a cpu.', 'the array is divided into\\nsmaller subarrays, and the maximum values of these subarrays are found in parallel.', 'finally, the\\nmaximum amon g these partial maximums is selected as the maximum element of the entire array.', 'one of the more traditional ways to evaluate algorithms is by looking at their algorithmic\\ncomplexity.', 'definition: algorithmic complexity is a measure of the number of operations that it would take to\\ncomplete an algorithm.', 'algorithmic complexity is a pro perty of the algorithm and is a measure of\\nthe amount of work or operations in the procedure.', 'complexity is usually expressed in asymptotic notation .', 'using asymptotic notation, you can analyze\\nand compare algorithms efficiently and make informed decisions when choosing the most suitable\\nalgorithm for a particular problem, taking into account both time and space complexity\\nthe three main types of asymptotic notation are:\\n1. big o notation (o-notation):\\n\\uf0b7 big o notation describes the upper bound or worst -case time complexity of an\\nalgorithm.', \"it represents an approximation of how an algorithm's running time\\nincreases as the input size grows.\", 'it provides an upper limit on the number of basic\\noperations an algorithm performs.', \"2. theta notation (θ -notation):\\n\\uf0b7 theta notation provides a tight bound, expressing both the upper and lower bounds\\nof an algorithm's time complexity.\", 'it characterizes the average -case behavior of an\\nalgorithm.', '3. omega notation (ω -notation):\\n\\uf0b7 omega notation describes the lower bound or best -case time complexity of an\\nalgorithm.', 'it provides a way to express how quickly the algorithm can solve a problem\\nin the most favorable circumstances.', 'algorithmic complexity, often expressed using asymptotic notations like big o, theta, and\\nomega, focuses on analyzing the efficiency of algorithms in terms of their time and space\\nrequirements as a function of the input size.', \"it provides a theoretical fram ework for\\ncharacterizing how an algorithm's performance scales as the input size grows towards\\ninfinity .\", 'example: finding the sum of all elements in an array.', 'algorithmic complexity (big o notation):\\n\\uf0b7 time complexity: o(n) - linear time complexity, where n is the size of the input array.', 'the algorithm processes each element once.', '\\uf0b7 space complexity: o(1) - constant space complexity, as it uses only a few variables\\nregardless of the input size.', '\\uf0b7 parallelization: divide the array into chunks and calculate the partial sums concurrently\\nusing parallel processing techniques, especially for large arrays.', 'prefix sum (scan) pattern and its importance in parallel computing\\nthe prefix sum, also known as the scan operation, is a fundamental parallel pattern in computer\\nscience and parallel computing.', \"given an input array of elements, the prefix sum operation computes\\na new array where each element is the sum of all elements in the input array up to and including the\\ncorresponding element's position.\", 'there are two common types of prefix sum operations: exclusive\\nand inclusive.', '\\uf0b7 exclusive prefix sum: the result at each position does not include the element at that\\nposition.', '\\uf0b7 input: [a, b, c, d]\\n\\uf0b7 output (exclusive): [0, a, a+b, a+b+c]\\n\\uf0b7 inclusive prefix sum: the result at each position includes the element at that position.', '\\uf0b7 input: [a, b, c, d]\\n\\uf0b7 output (inclusive): [a, a+b, a+b+c, a+b+c+d]\\n\\nstep -efficient parallel scan operation(inclusive prefix sum)\\nwork -efficient parallel scan operation(exclusive prefix sum)\\n\\nthe work -efficient parallel scan operation uses two sweeps through the arrays.', 'it starts by setting the last value to zero and then does another\\ntree-based sweep to get the final result.', 'though the process is simple it has some problems like changing the order of additions changes the\\nanswer in finite -precision arithmetic.', 'this is problematic because a parallel calculation changes the\\norder of the additions.', 'the problem is due to finite -precision arithmetic not being associative.', 'and\\nthe problem gets worse as the problem size gets larger because the addition of the last value\\nbecomes a smaller and smaller part of the overall sum.', 'eventually the addition of the last value\\nmight not change the sum at all.', 'there is even a worse case for additions of finite precision values\\nwhen adding two values that are almost identical, but of different signs.', 'this subtraction of one\\nvalue from another when these are nearly the same causes a catastrophic cancella tion.', 'catastrophic cancellation occurs when the operands are subject to rounding errors.', 'for example, if there are two measures\\nl1=253.5cm long and the other l2 =252.5cm long\\napproximations could come out to be\\nl1 = 254cm and\\nl2 = 252cm l1-l2 = 2cm\\nactual difference is l1-l2 = 1 cm\\nthere are several solutions for addressing the global sum .', 'the list of possible techniques presented\\nhere includes\\n\\uf0b7 long -double data type\\n\\uf0b7 pairwise summation\\n\\uf0b7 kahan summation\\n\\uf0b7 knuth summation —uses same method of pairwise\\n\\uf0b7 quad -precision summation\\nlong -double data type\\nthe easiest solution is to use the long -double data type on a x86 architecture.', 'on this architecture,\\na long -double is implemented as an 80 -bit floating -point number in hardware giving an extra 16 -bits\\nof precision.', 'unfortunately, this is not a portable technique\\npairwise summation\\npairwise summation, also known as pairwise addition, is a method used to sum a sequence of\\nnumbers in a way that reduces the effects of numerical errors, particularly in floating -point\\narithmetic.', 'this technique is commonly employed in scientific computing and numerical analysis to\\nimprove the accuracy of summation operations.', 'how pairwise summation works:\\n1. pairing the numbers:\\n\\uf0b7 given a sequence of numbers, they are paired up.', 'if there are an odd number of\\nelements, one number is left unpaired.', '2. pairwise addition:\\n\\uf0b7 within each pair, the two numbers are added together to create intermediate sums.', '3. summing the intermediate sums:\\n\\uf0b7 the intermediate sums obtained from pairwise addition are then summed together\\nusing the same pairwise summation method.', 'kahan summation\\nkahan summation, also known as compensated summation or kahan summation algorithm, is a\\nmethod used to reduce the numerical error that accumulates during the summation of a large\\nnumber of floating -point values.', 't his technique was introduced by william kahan, a renowned\\ncomputer scientist and mathematician, and it aims to improve the accuracy of summation\\noperations, especially in cases where a vast number of values need to be added together.', 'how kahan summation works:\\nin standard floating -point summation, when adding a small number to a large number, the small\\nnumber can be \"lost\" in the least significant bits of the large number, leading to a loss of precision.', 'kahan summation addresses this issue by using a com pensation term to keep track of the lost\\nprecision.', '1. initialization:\\n\\uf0b7 initialize the sum and the compensation term to zero.', '2. iterative addition:\\n\\uf0b7 for each number to be added:\\n\\uf0b7 add the number to the current sum.', '\\uf0b7 calculate the difference between the updated sum a nd the original sum (this\\ndifference is the lost precision).', '\\uf0b7 add this difference to the compensation term.', '3. final result:\\n\\uf0b7 the final result is the sum adjusted by the compensation term.', 'quad -precision summation\\nquad -precision summation refers to performing arithmetic operations with numbers represented\\nin quadruple -precision floating -point format.', 'in the ieee 754 floating -point standard, quadruple -\\nprecision is a 128 -bit data type, providing higher precision compared to single -precision (32 -bit) and\\ndouble -precision (64 -bit) floating -point number.']\n",
            "12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n",
            "cluster :  1\n",
            "['once all processors have completed sorting their portions, the sorted chunks can be combined to\\nproduce the final sorted dataset.', 'turn off or put to sleep any unused processors.', 'the energy consumption for your applicat ion can be estimated using the formula\\np = (n processors) × (r watts/processors) × (t hours)\\nwhere p is the energy consumption, n is the number of processors, r is the thermal design\\npower, and t is the application run time.', '\\uf0b7 weather forecasting: enables complex weather simulations and predictions.', '\\uf0b7 financial modelling : used for risk analysis, option pricing, and other complex financial\\ncalculations.', 'this classifica tion is named after michael j. flynn, who\\nintroduced it in 1966. flynn’s taxonomy is a useful tool for understanding different types of\\ncomputer architectures and their strengths and weaknesses.', 'this is the\\nsimplest type of computer architecture and is used in most traditional computers.', 'this type of\\narchitecture is used in applications such as image and signal processing.', '3. multiple instruction single data (misd ): in a misd architecture, multiple processors\\nexecute different instructions on the same data stream.', 'this type of architecture is not\\ncommonly used in practice, as it is difficult to find applications that can be decomposed into\\nindepen dent instruction streams.', '4. multiple instruction multiple data (mimd ): in a mimd architecture, multiple processors\\nexecute different instructions on different data streams.', 'this strategy is often used in applications where the same operation can be applied to different\\npieces of data independent ly.', \"scenario : imagine you're running a data analysis task on a large dataset of customer reviews for a\\nproduct.\", \"your goal is to perform sentiment analysis on each review to determine if it's positive,\\nnegative, or neutral.\", \"let's say you split\\nit into four subsets, each containing 250,000 reviews.\", 'you set\\nup four separate processing units (e.g., cpu cores or machines in a cluster), each responsible\\nfor analyzing one subset of reviews.', 'each processing unit loads its assigned subset of data.', '3. analysis : each processing unit applies the sentiment analysis model to its subset of reviews\\nindependently and simultaneously.', 'for in stance:\\n\\uf0b7 processing unit 1 analyzes reviews 1 to 250,000.', '\\uf0b7 processing unit 2 analyzes reviews 250,001 to 500,000.', '\\uf0b7 processing unit 3 analyzes reviews 500,001 to 750,000.', '\\uf0b7 processing unit 4 analyzes reviews 750,001 to 1,000,000.', '4. aggregation : as each processing unit finishes its analysis, it generates results, such as counts\\nof positive, negative, and neutral reviews within its subset.', 'these results are temporarily\\nstored.', '5. combining results : after all processing units have completed their work, you combine the\\nresults.', 'you sum up the counts from each processing unit to get the overall sentiment\\nanalysis results for the entire dataset.', 'each task\\ncan perform different operations and may not necessarily operate on the same data.', 'example: web server handling requests\\nconsider a web server handling incoming http requests.', 'the tasks include tasks like parsing the request, querying\\nthe database, and generating the response.', '2. task 2: database query\\n\\uf0b7 this task involves querying a database to fetch data related to the request, such as\\nuser information or product details.', '3. task 3: generating response\\n\\uf0b7 this task involves generating an html response based on the parsed request and\\ndata retrieved from the database.', \"example: manufacturing assembly line\\nlet's say we have a manufacturing assembly line for producing smartphones.\", 'the assembly line\\nconsists of three stages: a, b, and c. each stage represents a specific task in the smartphone\\nassembly process.', '1. stage a - component assembly :\\n\\uf0b7 worker a assembles the basic components of the smartphone, such as the circuit\\nboard, battery, and display.', 'once worker a finishes assembling a smartphone, it passes it\\nto stage b.', '2. stage b - software installation :\\n\\uf0b7 worker b installs the operating system and necessary software onto the smartphone\\nassembled by worker a. after software installation, the smartphone is passed to stage c.\\n3. stage c - quality control and packaging :\\n\\uf0b7 worker c checks the smartphone for quality control, ensuring that all components\\nare working correctly an d the software is functioning as intended.', 'if the smartphone passes\\nquality control, it is packaged and prepared for shipment.', 'in this example, each stage (a, b, and c) represents a processing step, similar to the stages in a\\nbucket -brigade parallelism sce nario.', 'a compiler is designed to translate your source code into a form the hardware can execute.', 'with these instructions at hand, an os manages executing these on the computer hardware.', 'these nodes are connected via a network, and they communicate and coordinate with each other\\nby passing messages.', 'in this architecture, each node opera tes independently and has its own local\\nmemory, and data sharing is achieved explicitly through message passing.', 'each node\\nworks on its subset of the data or a specific portion of the computation.', 'communication and\\ncoordination between nodes are essential, as tasks often depend on results or data computed on\\nothe r nodes.', 'this shared memory can be accessed and modified by any processor within the system.', 'vector units: multiple operations with one instruction\\nvector units, also known as vector processors, are specialized hardware units that can perform\\nmultiple operations with a single instruction.', 'these units are designed to process vectors, which are\\narrays of data elements, simultaneously.', 'vecto r processing is particularly useful in scenarios where\\nthe same operation needs to be performed on a large set of data elements .', 'discrete or dedicated gpus typically have a\\nlarge number of streaming multiprocessors and their own dram.', 'accessing data on a discrete gpu\\nrequires communication over a pci bus\\nan accelerator device, often referred to as an accelerator, is a specialized hardware component\\n(gpu) designed to perform specific types of computational tasks or workloads efficiently.', 'two nodes, each\\nwith two cpus, share the same dram memory.', 'a discrete gpu on the pci bus also attaches to one of the cpus.', 'though the cpus\\nshare main memory, these are commonly in different non -uniform memory access (numa)\\nregions.', 'each node has a multi -core cpu with an integrated and discrete gpu and some\\nmemory (dram).', 'these processes communicate and\\ncoordinate with each other by sending and receiving messages.', 'message passing is a method of\\ninter -process communication where data and instructions are exchanged between processes to\\nsynch ronize and share information.', 'fig 6 : the message passing library spawns processes.', 'the os places the processes on the cores\\nof two nodes.', 'the question marks i ndicate that the os controls the placement of the processes\\nand can move these during run time as indicated by the dashed arrows.', 'the threads are restricted to the node’s domain.', 'the question marks show that\\nthe os decides where to place the threads.', 'some memory is shared between threads.', 'simd allows a single instruction\\nto operate on multiple da ta elements simultaneously, which can significantly accelerate\\ncomputations involving large datasets.', '\\uf0b7 stream processing : through specialized processors\\nstream processing, often referred to as stream computing or data stream processing, is a computing\\nparadigm where data is continuously processed as it is generated or ingested, rather than being\\nstored in traditional databases or file systems.', 'stream proce ssing is particularly useful for handling\\nlarge volumes of real -time data from various sources, such as sensors, social media, financial\\ntransactions, and iot devices.', 'processed data, or output, transfers back to the cpu for file io or other\\nwork\\nsample application\\nwe start with a 2d problem domain of a region of space.', 'for purposes of illustration, we will use\\na 2d image of the krakatau volcano as our example.', 'the goal of our calculation could be to model\\nthe volcanic plume, the resulting tsunami, or the early detection of a volcanic eruption using\\nmachine learning.', 'for all of these options, calculation speed is critical if we want real -time result s\\nto inform our decisions.', 'for each cell in the computational domain, properties such as\\nwave height, fluid velocity, or smoke density are solved for according to physical laws.', 'ultimately, a\\nstencil operation or a matrix -vector system represents this discrete scheme\\nstep 2: define a computational kernel, or operation, to conduct on each element of the mesh\\nthe calculations on this discretized data are often some form of a stencil operation, so -called because\\nit involves a pattern of a djacent cells to calculate the new value for each cell.', 'this can be an average\\n(a blur operation, which blurs the image )gradient (edge -detection, which sharpens the edges in the\\nimage) or another more complex operation associated with solving physical sys tems described by\\npartial differential equations (pdes)\\nstep 3: vectorization to work on more than one unit of data at a time\\nsome processors have the ability to operate on more than one piece of data at a time; a capability\\nreferred to as vector operations.', 'the shaded blocks in figure illustrate how multiple data values\\nare operated on simultaneously in a vector unit in a proce ssor with one instruction in one clock\\ncycle.', 'when the work is split across nodes, the memory spaces for each node are distinct and\\nseparate.', 'here, 8×8 tiles are distributed across\\ngpu work groups.', 'these limits are determined by various factors and constraints\\nand play a crucial role in understanding the capabilities and limitations of a system.', 'application’s potential performance limits\\n\\uf0b7 flops (floating -point operations)\\n\\uf0b7 ops (operations) that include all types of computer instructions\\n\\uf0b7 memory bandwidth: rate at which the data is transferred\\n\\uf0b7 memory latency: time required for the first byte or word of data to be transferred\\n\\uf0b7 instruction queue (instruction cache)\\n\\uf0b7 networks\\n\\uf0b7 disk\\n\\uf0b7 machine balance: number of flops executed /memory bandwidth\\n\\uf0b7 arithmetic intensity : number of flops executed per memory operation\\nall of these limitations can be divided into two major categories:.', 'it includes all types of computer operations.', 'but to be able to do the operations, you must\\nget the data there.', 'this is where feeds come in.', 'feeds include the memory bandwidth through the\\ncache hierarchy, as well as network and disk bandwidth .', 'for many applications, the memory bandwidth limit can be difficult especially dealing with non-\\ncontiguous bandwidth.it is also known as strided memory access or non -contiguous memory\\naccess, refers to the manner in which data elements are accessed in memory.', 'in contrast to\\ncontiguous memory access, where elements are stored in consecutive memory locations, non -\\ncontiguous memory access invo lves accessing elements that are not stored sequentially in memory.', 'non -contiguous memory access:\\nnow, consider a situation where the array elements are scattered in memory with a stride of 2. this\\nis a non -contiguous memory access pattern:\\nin this case, accessing every second element (stride = 2) would mean accessing memory locations 1,\\n2, 3, 4, 5, etc., but the elements are not stored sequentially.', 'when your program needs to access such non -contiguous elements, it may lead to inefficiencie s\\ndue to increased cache misses and a higher likelihood of accessing data from main memory rather\\nthan the faster cache memory.', 'determine your hardware capabilities:\\nto determine the performance of hardware the following metrics are used :\\n\\uf0b7 the rate at which floating -point operations can be executed (flops/s)\\n\\uf0b7 the rate at which data can be moved between various levels of memory (gb/s)\\n\\uf0b7 the rate at which energy is used by your application (watts)\\nin determining hardware performance and calculating the metrics , we use a mixture of theoretical\\nand empirical measurements .', 'theoretical measurements provide an upper limit to what a system can achieve.', 'real -world validation is done by empirical measurements , they provide concrete evidence of how\\na system performs under real -world conditions, accounting for various factors like i/o operations,\\nnetwork latency, and concurrency issues.', 'one of the best tools for understanding the hardware you run is the lstopo program (graphical view)\\nand lscpu for text view .', 'lstopo is bundled with the hwloc package that com es with nearly every mpi\\ndistribution.', 'this command outputs a graphical view of the hardware on your system.', 'figure below\\nshows the output for a mac laptop in graphical view.', 'text view\\nthe information from the lscpu command and the /proc/cpuinfo file helps to determine the number\\nof processors, the processor model, the cache sizes, and the clock frequency for the system\\ncalculating theoretical maximum flops\\ntheoretical flops=number of cores×clock speed×flops per cycle per core\\nwhere:\\n\\uf0b7 number of cores: this represents the total number of processor cores in the computing\\nsystem.', '\\uf0b7 clock speed: this indicates the clock speed of each core in the system, typically measured\\nin hertz (hz) or gigahertz (ghz).', 'it represents the number of cycles th e processor can\\nexecute per second.', '\\uf0b7 flops per cycle per core: this signifies the number of floating -point operations a core can\\nperform in a single clock cycle.', 'modern processors often perform multiple flops per cycle\\ndue to features like simd (single inst ruction, multiple data) operations.', \"for example, let's consider a system with 4 cores, each operating at 3.0 ghz, and capable of\\nexecuting 4 flops per cycle per core (assuming simd operations are utilized):\\ntheoretical flops=4\\u2009cores×3.0\\u2009ghz×4\\u2009flops per cycle per core\\ntheoretical flops=48\\u2009gflops\\nthe memory hierarchy and theoretical memory bandwidth\\nwe can calculate the theoretical memory bandwidth of the main memory using the memory chips\\nspecifications.\", 'the general formula is b t = mtr × mc × tw × ns = data transfer rate × memory channels × bytes\\nper access × sockets\\nprocessors are installed in a socket on the motherboard.', 'the motherboard is the main system board\\nof the computer, and the socket is the lo cation where the processor is inserted.', 'most motherboards\\nare single -socket, where only one processor can be installed.', 'empirical measurement of bandwidth and flop\\nthe empirical bandwidth is the measurement of the fastest rate that memory can be loaded from\\nmain memory into the processor.', 'if a single byte of m emory is requested, it takes 1 cycle to retrieve\\nit from a cpu register.', 'if it is not in the cpu register, it comes from the l1 cache.', 'if it is not in the l1\\ncache, the l1 cache loads it from l2 and so on to main memory.', 'if it goes all the way to main\\nmemo ry, for a single byte of memory, it can take around 400 clock cycles.', 'this time required for the\\nfirst byte of data from each level of memory is called the memory latency .', 'two different methods are used for measuring the bandwidth: the stream benchmark and the\\nroofline model measured by the empirical roofline toolkit.', 'key differences:\\n\\uf0b7 focus: stream primarily focuses on memory bandwidth, providing quantitative\\nmeasurements.', 'in contrast, the roofline model provides a graphical representation of\\nperformance bottlenecks, considering both computational capabilities and memory\\nbandwidth.', '\\uf0b7 representation: stream results in a numerical measurement (memory bandwidth in bytes\\nper second), while the roofline model is a graphical representation that helps v isualize\\nperformance limitations.', \"\\uf0b7 insights: stream provides detailed insights into memory subsystem performance, whereas\\nthe roofline model offers a high -level overview of an application's performance efficiency\\nconcerning hardware constraints.\", 'calculatin g the machine balance between flops and bandwidth\\nthe machine balance is the flops divided by the memory bandwidth.', 'we can calculate both a theoretical machine balance (mb t) and an empirical machine balance (mb e)\\nlike so:\\nmb t = f t / b t\\nmb e = f e / b e\\ncharacterizing your application: profiling\\nnow that you have some sense of what performance you can get with the hardware, you need to\\ndetermine what are the performance characteristics of your application.', 'additionally, you should\\ndevelop an understandi ng of how different subroutines and functions depend on each other .', 'profiling tools :\\nusing call graphs for hot -spot and dependency analysis\\nin the context of parallel programming, call graphs are diagrams that represent the calling\\nrelationships between different functions or methods in a parallel program.', \"they illustrate how\\nfunctions or tasks invoke each other and provide a visual representation of the program's control\\nflow.\", 'by analyzing these call graphs,\\ndevelopers can identify hot -spots —functions or tasks that consume a significant amount of\\ncomputational time.', 'empirical measurement of processor clock frequency and energy consumption\\nempirical measurement of processor clock frequency:\\n1. profiling tools: profiling tools like intel vtune profiler or amd codexl can provide insights\\ninto various performance metrics, including processor clock frequency.', 'these tools often offer\\nvisualizations and detailed reports for better analysis.', '2. benchmarking suites: benchmarking tools like spec cpu benchmarks or hpc chal lenge\\nbenchmarks often include components that measure processor clock frequencies.', \"running\\nthese benchmarks can provide detailed information about the processor's performance\\ncharacteristics.\", '2. empirical measurement of energy consumption:\\n1. power measureme nt tools: use power measurement tools and hardware devices to\\nmeasure the power consumption of your system.', 'power meters and sensors can be attached\\nto the system to measure real -time power usage.', \"tools like intel power gadget or linux's\\npowerstat can help measure power usage.\", '2. energy profilers: some profiling tools, like intel vtune profiler, also offer energy profiling\\ncapabilities.', 'they can provide insights into energy consumption patterns at different parts of\\nyour code.', 'these tools often correlate energ y consumption with specific functions or code\\nregions.', 'tracking memory during run time\\ntracking memory usage during runtime in parallel computing is crucial for optimizing performance,\\ndetecting memory leaks, and ensuring efficient memory management.', 'several techniques and tools\\ncan help you monitor memory usage in parallel applications.', 'here are some approaches to tracking\\nmemory during runtime in parallel computing environments:\\nprofiling tools:\\n1. valgrind massif: valgrind is a powerful instrumentation framework.', 'massif, a valgrind tool, can\\nprofile heap memory usage over time, showing memory consumption patterns.', \"it's particularly\\nuseful for detecting memory leaks and understanding how memory usage evolves during\\nprogram execution.\", '2. intel vtune profiler : vtune profiler provides memory analysis capabilities, including memory\\nusage tracking.', 'it can analyze memory consumption at various levels, from individual functions\\nto entire applications, in both serial and parallel contexts.', '3. openmp/mpi memory profiler s: many parallel programming frameworks like openmp and\\nmpi provide their memory profiling tools.', 'for example, openmp has tools like score -p, and mpi\\nhas memory profiling features integrated into mpi implementations.', 'they encompass various\\naspects of system performance, including algorithmic efficiency, but also consider factors\\nrelated to specific hardware, software, and real -world scenarios.', 'it is done for faster access to\\nelements.', 'it can be any type of data, such as a file, a\\npassword, or a message.', 'spatial information can also be stored in adaptive mesh refinement (amr) format.amr is a numerical\\nsimulation technique used in computational mathematics, fluid dynamics, and other fields to imp rove\\nthe efficiency and accuracy of simulations.', 'amr systems maintain multiple levels of grids.', 'each level\\nrepresents a different resolution of the simulation domain.', 'finer grids cover smaller areas, providing\\nhigh resolution, while coarser grids cover lar ger areas, offering lower resolution.', 'in open addressing, there are a few choices that\\nwe can use as the trial for the next open slot.', 'this\\nprocess is used to transfer data (usually physical quantities like temperature, pressure, or velocity)\\nfrom one mesh or grid to another.', '\\uf0b7 if open addressing is used, probe the adjacent cells until the object is found or an\\nempty cell is encountered.', 'the first sweep is\\ncalled an upsweep, though it is more of a right sweep.', 'the second phase, known as the downsweep phase, is more of a left sweep.', 'the output of upsweep\\nis provided as input to downsweep.']\n",
            "25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n",
            "cluster :  3\n",
            "['bucket -brigade parallelism :\\na bucket brigade is a method of manually transporting items or materials from one location to\\nanother by forming a line of people, each of whom carries an item and passes it to the next person.', 'this technique is similar to how buckets of water might be passed along a line of people to put out a\\nfire, which is where the term \"bucket brigade\" originated.', 'hash function\\nhashing is a popular technique for quickly storing and retrieving data.', 'hashing is a technique or process\\nof mapping keys, values into the hash table by using a hash function.', 'the efficiency of mapping depends on the efficiency of the hash function used.', 'components of hashing\\n1. hash function: the hash function itself is a crucial component.', 'it takes an input and produces\\na fixed -size hash value.', 'the hash function ensures that the same input always produces the\\nsame hash value and that even a small change in the input results in a significantly different\\nhash value.', '2. input data: this is the data that you want to hash.', \"3. hash value: also known as the hash code or hash keys , it's the output of the hash function\\nafter processing the input data.\", 'the hash value is typically a index into hash table .', '4. collision: a collision occurs when two different inputs produce the same hash value.', '5. bucket or slot: in the context of hash tables , a bucket or slot is a location where data is stored\\nbased on its hash value.', 'hash tables consist of an array of these buckets, and the hash value\\ndetermines which bucket a particular piece of data will be stored in.', '6. hash table: a hash table is a data str ucture that uses hashing to implement an associative\\narray, a structure that can map input to values.', 'it consists of an array of buckets where data is\\nstored based on its hash value.', 'hash tables allow for efficient insertion, deletion, and lookup\\noperation s.\\n7. load factor: the load factor of a hash table is the ratio of the number of stored elements to\\nthe total number of buckets.', 'a high load factor can lead to increased collisions and decreased\\nperformance, so hash tables are often resized and rehashed if th e load factor exceeds a certain\\nthreshold.', '8. sparsity : the sparsity of a hash table is the ratio of the number of empty buckets to the total\\nnumber of buckets 9. collision resolution: techniques used to handle collisions include chaining (where each bucket\\ncontains a linked list of items that hash to the same index) and open addressing (where the\\nalgorithm searches for the next open slot in the hash table).', 'spatial hashing\\nspatial data is any type of data that directly or indirectly references a specific geographical area or\\nlocation.', 'sometimes called geospatial data or geographic information, spatial data can also numerically\\nrepresent a physical object in a geographic coordinate system.', 'however, spatial data is much more\\nthan a spatial component of a map.', 'spatial data can be stored in either vector format or raster format\\nvector\\nraster\\nspatial hashing is a technique where the key is based on spatial information.', 'spatial hashing is a\\ntechnique used to locate objects in a 3d space.', 'it involves dividing a large space into smaller, grid -like\\ncells, and assigning each object in the space to the cell that contains it..the basic principle is to map\\nobjects onto a grid of buckets arranged in a regular pattern .', 'the same data of amr can be even stored in perfect hashtable or compact hash table.', 'perfect hashing is a technique used to eliminate collisions entirely in hash tables, ensuring that each\\nkey maps to a unique index without any conflicts.', 'in traditional hash tables, collisions can occur when\\nmultiple keys hash to the same index, requiring additional data structures like linked lists or open\\naddressing techniques to resolve these collisions.', 'perfect hashing, on the oth er hand, aims to design a\\nhash function and data structure in such a way that collisions never occur.', 'compact hashing is a technique used to design hash functions in a way that minimizes the memory\\nrequired for hash tables.', 'it focuses on creating hash func tions that distribute keys uniformly across the\\navailable slots in a hash table while keeping the table small in size.', 'the objective is to use as few bits\\nas possible per key, reducing the memory footprint of the hash table.', 'because of the compression to a compact hash, two entries try to store their value in bucket 1. the\\nsecond entry sees that there is already a value there, so it looks for the next open slot in a technique\\ncalled open addressing.', 'in open addressing, we look for the next open slot in the hash table and\\nstore the value in that slot.', 'there are other hashing methods than open addressing, but these often\\nrequire the ability to allocate memory during an operation.', 'a llocating memory is more difficult on\\nthe gpu, so we stick with open addressing where collisions are resolved by finding alternate storage\\nlocations within the already allocated hash table.', 'these are\\nlinear probing —where the next entry is just the next bucket in sequence until an open bucket is\\nfound\\nquadratic probing —where the increment is squared so that the attempted buckets are +1, +4,\\n+9, and so forth f rom the original location\\ndouble hashing —where a second hashing function is used to jump to a deterministic, but pseudo -\\nrandom distance from the first trial location\\non spatial data we can perform, four spatial operations.', '\\uf0b7 neighbor finding —locating the one or two neighbors on each side of a cell\\n\\uf0b7 remapping —mapping another amr mesh onto a current amr mesh\\n\\uf0b7 table lookup —locating the intervals in the 2d table to perform the interpolation\\n\\uf0b7 sorting —a 1d or 2d sort of the cell data\\nthese can be implemented either by perfect hashing or compact hashing.', 'neighbor finding using a spatial perfect hash\\nthe steps involved are\\n\\uf0b7 allocate a spatial hash the size of the finest level of the cell -based amr mesh\\n\\uf0b7 for each cell in the amr mes h, write the cell number to the hash buckets underlying the cell\\n\\uf0b7 compute the index for a finer cell one cell outside the current cell on each side\\n\\uf0b7 read the value placed in the hash bucket at that location.', 'example: each cell writes its cell number to the hash buckets it covers.', 'right neighbor of cell 2 1 is\\nat col 8, row 3. look up in hash and it is cell 26 .', 'remapping —mapping another amr mesh onto a current amr mesh (perfect hash)\\nremapping from one mesh to another is a common operation in computational simulations,\\nespecially in fields like computational fluid dynamics (cfd) and finite element analysis (fea).', 'you have two grids or meshes: the source mesh (from which you\\nwant to remap data) and the target mesh (to which you want to map the data ).', 'table lookup(perfect hash )\\nto perform a table lookup in spatial hashing, you need to map the position of an object to a\\nunique key (hash value), which determines the index of the bucket in the hash table where\\nthe object should be retrieved.', \"\\uf0b7 perform a lookup in the hash table at the calculated index to find the bucket\\ncorresponding to the object's spatial location.\", '\\uf0b7 if chain ing (linked lists in each bucket) is used for collision resolution, traverse the\\nlinked list in the selected bucket to find the object.', 'sorting mesh data using a spatial perfect hash\\nwe can demonstrate the hash sort operation .', 'the minimum difference between values is 2.0, so\\nthe bucket size of 2 guarantees that there are no collisions.', 'the minimum value is 0, so the bucket\\nlocation can be calcul ated with bi = xi /δmin = xi /2.0.', 'we could store either the value or the index\\nin the hash table.', 'for example, 8, the first key, could be stored in bucket 4 .']\n",
            "10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            " Computing refers to the use of computers and systems to perform various tasks, including data processing, information storage, and problem-solving. There are two types of computing: serial and parallel. Serial computing is traditional computing where tasks are executed sequentially, one after the other, using a single processor. This approach can be slow and inefficient when dealing with complex tasks. Parallel computing, on the other hand, involves executing multiple tasks simultaneously using multiple processors. This approach can significantly improve processing speed and efficiency, especially when dealing with complex tasks. Examples of serial computing include sorting a large dataset of numbers in ascending order, while parallel computing involves using multiple processors to perform multiple tasks simultaneously.\n",
            "\n",
            "    Parallel computing is a type of computation where multiple processors or computers work together to solve a problem. This allows for simultaneous processing of smaller sub-tasks, leading to significant improvements in computational speed and efficiency. An example of parallel computing is sorting a large dataset by dividing it into smaller chunks and sorting each chunk independently by a separate processor. This allows for faster processing and more efficient use of resources.\n",
            "\n",
            "    Parallel computing offers significant benefits, including increased processing power, faster execution times, and improved scalability. However, there are also costs and limitations to consider, such as the need for specialized hardware and software, potential performance bottlenecks, and the challenges of managing and coordinating multiple processing units. By carefully evaluating these factors, software developers can make informed decisions about how to best leverage parallel computing to achieve their goals.\n",
            "    ```\n",
            "    The summary provided above is a concise and accurate representation of the given text, highlighting the key points and main ideas while minimizing word repetition. It also maintains the same structure and tone as the original text, making it a coherent and informative summary.\n",
            "\n",
            "    Parallel computing involves dividing a task into smaller sub-tasks that can be executed simultaneously, utilizing multiple cores to process the data. This approach can significantly reduce the time required to complete the task, especially for larger datasets and more complex simulations. Additionally, parallel computing aims to optimize energy efficiency by doing more with less, making use of dynamic resource allocation and workload consolidation to ensure efficient use of processors.\n",
            "\n",
            "    Parallel computing can provide better computational performance by leveraging dynamic resource allocation and workload consolidation. It can also be easily scaled by adding more processors, making it more cost-effective than serial computing. Parallel computing has various applications, including scientific simulations and big data processing.\n",
            "\n",
            "    Parallel computing is crucial in processing vast amounts of data in fields like data analytics and machine learning. Laws like Amdahl's and Gustafson's provide insights into the limitations and possibilities of parallel processing, including speedup, which measures the performance improvement of a parallel algorithm or system compared to a serial implementation.\n",
            " The speedup metric is a measure of how much faster a parallel algorithm or system can complete a task compared to a serial implementation. It is calculated by dividing the execution time of the task using multiple processors (parallel execution time) by the execution time of the task using a single processor (serial execution time). A speedup value greater than 1 indicates that the parallel implementation is faster than the serial implementation. Ideally, doubling the number of processors would halve the execution time, resulting in a speedup of 2, but achieving perfect linear speedup is rare in real-world scenarios due to factors such as communication overhead and load.\n",
            "\n",
            "    Parallel computing is limited by factors such as communication overhead, load balancing issues, and synchronization constraints between processors. Amdahl's Law states that the speedup of a parallel program is limited by the sequential fraction of the program, meaning that only a portion of the program can be parallelized. This law was formulated by Gene Amdahl in 1967 and highlights the limitations of parallel computing.\n",
            "\n",
            "    The summary of the given text is:\n",
            "    The speedup of a program using multiple processors in parallel computing is limited by the sequential fraction of the program, meaning that even with multiple processors, there is a maximum speedup that can be achieved due to the presence of sequential parts of the program. The maximum speedup achievable is calculated using Amdahl's law, which shows the speedup as a function of the number of processors. In the given example, even though 90% of the program can be parallelized and runs on 5 processors, the maximum speedup achievable is approximately 3.57 times faster compared to sequential execution.\n",
            "\n",
            "    The text discusses the limitations of parallel computing according to Amdahl's Law and Gustafson's Law. Amdahl's Law states that the speedup of a parallel algorithm is limited by the fraction of code that remains serial, while Gustafson's Law suggests that as the problem size increases, the impact of the parallelizable portion of the program becomes more significant, leading to better scalability. The text also introduces the concepts of strong scaling and weak scaling, which are used to evaluate the performance of parallel computing systems.\n",
            "\n",
            "    Strong scaling and weak scaling are two metrics used to evaluate parallel computing systems' performance. Strong scaling measures how execution time decreases as the number of processors increases for a fixed problem size. Weak scaling, on the other hand, assesses how well a parallel system performs when the problem size increases along with the number of processors. Strong scaling aims to reduce execution time for a fixed problem size by utilizing more processors, while weak scaling is applicable when the problem size increases along with the number of processors.\n",
            "\n",
            "    To solve a problem faster, additional processors can be employed. Weak scaling measures how well a parallel system can handle larger workloads by adding more processors as the problem size grows. The goal of weak scaling is to maintain a constant workload per processor as the size of the problem and the number of processors increase. This is applicable when the problem size can be increased, and the aim is to distribute the computational load across a larger number of processors using parallel approaches.\n",
            " Flynn's classification is crucial in parallel computing as it categorizes architectures based on instruction & data streams, highlighting the significance of parallelism in modern computing. It aids in designing & analyzing parallel processing systems.\n",
            "\n",
            "    Single Instruction Single Data (SISD) and Single Instruction Multiple Data (SIMD) are two different architectures used in computing. SISD has a single processor that executes a single instruction stream and operates on a single data stream, while SIMD has a single processor that executes the same instruction on multiple data streams in parallel. Parallel strategies are techniques and methods used to improve the efficiency and performance of a computer system by simultaneously executing multiple tasks or processes. Common parallel strategies include data parallel approach, which involves dividing data into smaller chunks and processing them simultaneously.\n",
            "\n",
            "    Parallel computing and distributed systems are used to speed up computations and handle large volumes of data. Common parallel strategies include data parallelism, task parallelism, and main-worker approach. Data parallelism involves performing the same operation on multiple data elements simultaneously, while task parallelism involves executing multiple independent tasks or processes in parallel. These strategies can be applied in sentiment analysis to speed up the computationally intensive process.\n",
            " Task parallelism is a technique where multiple independent tasks or processes are executed concurrently without dependencies between them. In a task parallelism setup, each incoming request is treated as an independent task that can be processed simultaneously by multiple threads or processes, allowing the server to handle multiple requests without waiting for one task to complete. This technique is commonly used in applications where different tasks can be performed concurrently without dependencies between them. In parallel computing, bucket brigade parallelism is a variant of task parallelism where a task is broken down into smaller sub-tasks that can be executed concurrently by multiple processors or threads, allowing for increased processing speed and efficiency.\n",
            "\n",
            "    In parallel computing, bucket brigade parallelism involves breaking down a task into smaller subtasks and processing them independently. This technique allows for efficient parallel processing of tasks and is often used when tasks can be divided into smaller parts. Parallel speedup and comparative speedup are two metrics used to evaluate the performance improvement achieved by parallel processing. Parallel speedup measures how much faster a parallel algorithm runs compared to its sequential counterpart, while comparative speedup compares the performance of parallel and sequential algorithms on the same hardware.\n",
            "\n",
            "    Parallel computing is a technique where multiple processors are used to execute a program simultaneously, resulting in faster execution times. Parallel speedup is calculated by dividing the time taken to execute a program sequentially by the time taken to execute it parallelly. Comparative speedup is a performance comparison between different parallel implementations or hardware sets. As a developer, you are responsible for making choices about the programming language and parallel software interfaces in your application software layer.\n",
            "\n",
            "    Parallel computing approaches involve various models and paradigms that define how tasks are divided, coordinated, and executed in parallel systems. Common models include hardware models such as distributed memory architecture, shared memory architecture, and vector units, as well as software models such as process-based parallelization, message passing, and thread-based parallelization. These models allow developers to leverage the underlying hardware and software to optimize parallelization in their applications.\n",
            "\n",
            "    Distributed memory architecture is a parallel computing method where multiple processors or nodes in a cluster have their own private memory. Cross-node parallel methods involve distributing tasks across multiple nodes in a cluster. Shared memory architecture is an on-node parallel method where multiple processors or cores share a single, unified memory space. On-node parallelism refers to parallel processing techniques that occur on a single computing node.\n",
            "\n",
            "    Parallelism in the context of shared memory architecture refers to parallel processing techniques that occur on a single computing node. This involves multiple threads or processes running concurrently on the same node, accessing shared memory to perform computations. An accelerator device is a special-purpose add-on processor, such as a GPU, that can offload certain tasks from the CPU and provide parallel processing capabilities. The general heterogeneous parallel architecture model combines multiple hardware architectures, including CPUs with dual-core processors and accelerators, to provide a more efficient and powerful system.\n",
            "\n",
            "    This text describes a general model for a heterogeneous parallel architecture, consisting of multiple CPUs and GPUs connected by a network. The model allows for efficient parallelization of tasks through message passing between processes. The programmer must expose the parallelization, determine the best technique, and explicitly direct its operation in a safe, correct, and efficient manner. The most common techniques for parallelization are process-based parallelization, particularly through message passing.\n",
            "\n",
            "    Parallelization techniques include process-based parallelization, thread-based parallelization, and vectorization. Process-based parallelization involves dividing a task into multiple processes that run independently on separate computing nodes or cores, while thread-based parallelization involves dividing a task into multiple threads that share the same memory space within a single process. Vectorization involves performing multiple operations with one instruction. These techniques are commonly used in distributed memory systems, multi-core processors, and symmetric multiprocessing (SMP) systems.\n",
            "\n",
            "    Vectorization is a parallel computing technique that allows processors to perform multiple operations with a single instruction, leveraging the SIMD capabilities of modern processors, including CPUs and GPUs. This approach enables efficient and timely processing of data streams by offloading data and compute kernels to the GPU's streaming multiprocessors. To further enhance performance, additional layers of parallelization can be applied on CPUs and GPUs, including vectorization, threads, and processes. These layers allow for working on multiple units of data simultaneously, deploying more compute pathways, and separating program instances into separate memory spaces.\n",
            "\n",
            "    The article discusses various techniques for improving the performance of computational tasks, including:\n",
            "    1. Deploying multiple compute pathways to engage more processing cores.\n",
            "    2. Separating program instances to spread out the calculation into separate memory spaces.\n",
            "    3. Offloading the calculation to GPUs.\n",
            "    4. Discretizing the problem into smaller cells or elements.\n",
            "    5. Using threading to operate multiple processing cores simultaneously.\n",
            "    6. Splitting the work between processors on two desktops, often called nodes in parallel processing.\n",
            "    Understanding performance limits and profiling the application are crucial in parallel processing.\n",
            "\n",
            "    Understanding performance limits and profiling parallel programs are crucial steps to optimize their execution. Performance limits refer to the maximum achievable performance of a computing system or application under specific conditions. Profiling tools provide detailed information about the behavior of a parallel program, which can be used to enhance its efficiency and improve speedup. By leveraging these tools and techniques, developers can lead to improved overall performance.\n",
            "\n",
            "    The article discusses the importance of profiling results in enhancing the efficiency of parallel applications, with a focus on speedup and overall performance. It highlights the significance of analyzing call graphs in parallel programming to gain insights into the program's structure, dependencies, and potential optimization opportunities. Additionally, the article mentions dual-socket motherboards and their role in providing more processing cores and memory bandwidth in high-performance computing systems.\n",
            "\n",
            "    Parallel algorithms and patterns are designed to take advantage of parallel processing capabilities by breaking down complex tasks into smaller, more manageable pieces that can be processed simultaneously across multiple processors or cores. These patterns provide a framework for creating effective parallel algorithms, such as the \"divide and conquer\" pattern used to find the maximum element in an array. By applying these patterns, programmers can solve specific types of problems efficiently in parallel computing environments.\n",
            "\n",
            "    The article discusses the importance of algorithm analysis in parallel computing applications. It highlights the need to compare different algorithms that solve the same problem and the factors that affect their performance, including hardware differences, compiler optimizations, and memory optimization. The article also introduces the concept of parallel algorithms, which are designed to solve computational problems efficiently by utilizing multiple processing units simultaneously.\n",
            " Parallel algorithms are designed to solve computational problems efficiently by utilizing multiple processing units simultaneously. They are crucial in high-performance computing and parallel processing environments where large datasets and complex computations need to be handled efficiently. Common types of parallel algorithms include parallel merge sort, parallel quicksort, and parallel global sum. These algorithms aim to break down tasks into smaller subtasks that can be processed independently and concurrently, leading to significant speedup in overall computation time.\n",
            " Parallel global sum is a problem in parallel or distributed computing where the sum of elements is computed across multiple processors or nodes. Sorts partitions concurrently ensures minimal word repetition and brevity in summarizing the given text.\n",
            "\n",
            "\n",
            "\n",
            "    1. The process occurs sequentially, with each step waiting for the previous one to finish.\n",
            "    2. Unlike Amdahl's law, Gustafson's law takes into account varying problem sizes.\n",
            "    3. The problem is discretized into smaller cells or elements.\n",
            "    4. The calculation is offloaded to GPUs, where the vector length is larger than on CPUs.\n",
            "    5. The array is divided into smaller subarrays, and the maximum values of these subarrays are found in parallel.\n",
            "    6. The maximum of these partial maxima is selected as the maximum element of the entire array.\n",
            "    7. Algorithmic complexity is a measure of the number of operations it would take to complete an algorithm.\n",
            "\n",
            "Note: The summary is brief and coherent, but it may not capture all the details and nuances of the original text.\n",
            " * Algorithmic complexity is a measure of the amount of work or operations in an algorithm.\n",
            "     * It is usually expressed in asymptotic notation, which provides an upper limit on the number of basic operations an algorithm performs.\n",
            "     * The three main types of asymptotic notation are: big O notation, theta notation, and omega notation.\n",
            "     * Big O notation describes the upper bound or worst-case time complexity of an algorithm, providing an approximation of how an algorithm's running time increases as the input size grows.\n",
            "     * Theta notation provides a tight bound, expressing both the upper and lower bounds of an algorithm's time complexity, characterizing the average-case behavior of an algorithm.\n",
            "\n",
            "    θ notation provides a tight bound on an algorithm's time complexity by expressing both the upper and lower bounds of its average-case behavior. On the other hand, ω notation describes the lower bound or best-case time complexity of an algorithm, providing a way to express how quickly the algorithm can solve a problem in the most favorable circumstances. Both θ and ω notations are used in asymptotic notations like big O, θ, and ω to analyze the efficiency of algorithms in terms of their time and space requirements as a function of the input size. This framework is useful for characterizing how an algorithm's performance scales as the input size grows towards infinity. For example, finding the sum of all elements in an array is an algorithm with a time complexity of O(n), where n is the size of the array.\n",
            "\n",
            "    The article discusses the performance of algorithms as the input size grows towards infinity, using the example of finding the sum of elements in an array. The algorithm has a linear time complexity of O(n) and constant space complexity of O(1), making it efficient for small inputs. However, as the input size increases, parallelization techniques such as dividing the array into chunks and calculating partial sums concurrently can improve performance. The prefix sum operation, also known as the scan operation, is a fundamental parallel pattern in computer science and parallel computing, and is important for large inputs.\n",
            " The article discusses two types of prefix sum operations: exclusive and inclusive. Exclusive prefix sum\n",
            "     means the result at each position does not include the element at that position, while inclusive prefix sum\n",
            "     means the result at each position includes the element at that position. The article provides an example\n",
            "     input array and the corresponding output for both types of prefix sum operations.\n",
            " The text describes two types of parallel scan operations: inclusive prefix sum and exclusive prefix sum. These operations are efficient and can be performed in parallel.\n",
            "    The text does not provide detailed explanations or examples of these operations, but it does mention that they are used in various applications, such as data compression and signal processing.\n",
            "    The text also does not discuss the theoretical foundations of these operations, such as the computational complexity or the convergence properties of the algorithms.\n",
            "    Therefore, the summary should focus on the main ideas and key points of the text, without going into too much detail.\n",
            "    WORD COUNT: 143\n",
            "    ```\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    The efficient parallel scan operation uses two sweeps through the arrays, starting with the last value set to zero and then performing a tree-based sweep for the final result. However, this method has some issues, such as changing the order of additions affecting the answer in finite-precision arithmetic, due to non-associativity. As the problem size increases, the problem becomes more severe, and the addition of the last value might not change the overall sum. Additionally, there is a worse case for additions of finite-precision values when adding two values that are almost identical but of different signs, resulting in subtraction rather than addition.\n",
            " The text discusses the issue of catastrophic cancellation in arithmetic operations, particularly in the context of finite precision values. It highlights a worst-case scenario where adding two values that are almost identical but of different signs can result in a significant error. The text then presents several techniques for addressing this issue, including the use of long-double data types, pairwise summation, Kahan summation, Knuth summation, and quad-precision summation. The easiest solution is to use the long-double data type on an x86 architecture.\n",
            "\n",
            "    The article discusses the use of the long -double data type in x86 architecture for improving the precision of floating -point arithmetic. However, this technique is not portable. An alternative method, pairwise summation, is also discussed, which involves pairing numbers, adding them within pairs, and summing the intermediate sums. This method can improve the accuracy of summation operations in scientific computing and numerical analysis.\n",
            "\n",
            "    Kahan summation is a method used to reduce numerical error in floating-point summation. It works by pairing numbers, adding them together, and then summing the intermediate sums using the same pairwise summation method. This technique was introduced by William Kahan and aims to improve accuracy in large-scale summation operations.\n",
            "\n",
            "    In standard floating-point summation, adding a small number to a large number can result in significant numerical error. Kahan summation addresses this issue by breaking down the process into smaller, more manageable parts and using a compensating strategy to improve accuracy.\n",
            "\n",
            "    The process of Kahan summation involves three main steps:\n",
            "\n",
            "1. Generate a list of elements, one number is left unpaired.\n",
            "2. Pairwise addition: within each pair, the two numbers are added together to create intermediate sums.\n",
            "3. Summing the intermediate sums: the intermediate sums obtained from pairwise addition are then summed together using the same pairwise summation method.\n",
            "\n",
            "By breaking down the summation process into smaller parts and using a compensating strategy, Kahan summation can significantly improve the accuracy of floating-point summation operations.\n",
            "\n",
            "    Kahan summation is an algorithm used to improve the precision of floating-point arithmetic operations, particularly when adding small numbers to large ones. It works by maintaining a compensation term that keeps track of the lost precision due to rounding errors. The algorithm initializes the sum and compensation term to zero, then iteratively adds numbers to the sum and calculates the difference between the updated sum and the original sum, adding this difference to the compensation term. The final result is the adjusted sum, which takes into account the lost precision. Quad-precision summation refers to performing arithmetic operations with numbers represented in quadruple-precision floating-point format, which provides greater precision than single-precision or double-precision formats.\n",
            " Quadruple precision refers to performing arithmetic operations with numbers represented in a 128-bit floating-point format, offering higher precision than single and double precision formats.\n",
            "\n",
            "\n",
            "\n",
            "    The text discusses the process of sorting chunks of data and estimating energy consumption for an application. It also mentions the use of weather forecasting and financial modeling, which are classified under the Flynn's taxonomy. Flynn's taxonomy is a classification system introduced by Michael J. Flynn in 1966, which helps understand different types of computer architectures and their strengths and weaknesses. The simplest type of computer architecture, which is used in most traditional computers, is also mentioned.\n",
            "\n",
            "    This text discusses various computer architectures and their advantages and disadvantages. The simplest architecture is used in traditional computers and is suitable for applications such as image and signal processing. However, this architecture can be limited when it comes to complex tasks. The next architecture discussed is Multiple Instruction Single Data (MISD), where multiple processors execute different instructions on the same data stream. While this architecture can improve performance, it is not commonly used due to the difficulty in finding applications that can be decomposed into independent instruction streams. The final architecture discussed is Multiple Instruction Multiple Data (MIMD), where multiple processors execute different instructions on different data streams. This architecture is often used in applications where the same operation can be applied to different pieces of data independently. The text provides an example of using this architecture in a data analysis task on a large dataset of customer reviews.\n",
            "\n",
            "    The text describes a scenario where a large dataset of reviews is split into four subsets, each analyzed independently by a separate processing unit using a sentiment analysis model. Each processing unit analyzes its assigned subset of reviews simultaneously, and once finished, generates results such as count of positive, negative, and neutral reviews within its subset. The results are temporarily stored until all processing units have finished their analysis, at which point they are aggregated to produce a comprehensive analysis of the entire dataset.\n",
            " The given text describes the process of sentiment analysis, which involves breaking down a large dataset into smaller subsets, processing each subset, and combining the results to obtain the overall sentiment analysis. The text also provides examples of different tasks that can be performed within the process, such as parsing requests, querying databases, and generating responses.\n",
            "    The text also mentions that each task can perform different operations and may not necessarily operate on the same data, highlighting the flexibility and adaptability of the sentiment analysis process.\n",
            "\n",
            "    This task involves generating an HTML response based on a parsed request and data retrieved from a database. The example given is a manufacturing assembly line for producing smartphones, consisting of three stages: component assembly, software installation, and quality control and packaging. Workers in each stage perform specific tasks in the smartphone assembly process, such as assembling basic components, installing software, and checking quality control.\n",
            "\n",
            "    This text describes a processing scenario involving multiple stages (A, B, and C) where each stage represents a processing step. The stages are similar to the stages in a bucket-brigade parallelism scenario. In this architecture, a compiler translates source code into a form that the hardware can execute, and an OS manages executing these instructions on the computer hardware. The nodes in the system are connected via a network and communicate and coordinate with each other by passing messages. Each node operates independently and has its own local memory, and data sharing is achieved explicitly through message passing. The nodes work on their respective subsets of the data or specific portions of the computation, and coordination between nodes is essential since tasks often depend on results or data computed on other nodes.\n",
            "\n",
            "    Accelerators are specialized hardware components designed to speed up specific tasks or computations within a system. They work by processing data or performing tasks on a subset of the data or a specific portion of the computation. Communication and coordination between nodes are essential, as tasks often depend on results or data computed on other nodes. Accelerators can be accessed and modified by any processor within the system, and they are particularly useful in scenarios where the same operation needs to be performed on a large set of data elements. Discrete GPUs typically have a large number of streaming multiprocessors and their own DRAM, and accessing data on a discrete GPU requires communication over a PCI bus.\n",
            "\n",
            "     An accelerator device, or accelerator, is a specialized hardware component designed to perform specific computational tasks efficiently. Two nodes, each with two CPUs, share the same DRAM memory, but are located in different NUMA regions. Each node has a multi-core CPU with an integrated and discrete GPU, as well as some memory (DRAM). Communication between processes is achieved through message passing, where data and instructions are exchanged between processes to synchronize and share information. The message passing library spawns processes, and the OS places them on the cores of two nodes.\n",
            "\n",
            "     The operating system (OS) controls the placement of processes and threads during runtime, as indicated by the dashed arrows in the diagram. The OS decides where to place the threads, and some memory is shared between them. Simultaneous Data Processing (SIMD) allows a single instruction to operate on multiple data elements simultaneously, accelerating computations involving large datasets. Stream processing is a computing paradigm where data is continuously processed as it is generated or ingested, rather than being stored in traditional databases or file systems. It is particularly useful for handling large volumes of real-time data from various sources.\n",
            " Stream processing is useful for handling large volumes of real-time data from various sources. It involves processing data as it arrives, rather than batch processing. Stream processing is particularly useful for handling data from sources such as sensors, social media, financial transactions, and IoT devices. The output of stream processing is transferred back to the CPU for file I/O or other work. An example application is modeling a volcanic plume, tsunami, or early detection of a volcanic eruption using machine learning. Calculation speed is critical for real-time results. The properties of wave height, fluid velocity, or smoke density are solved for according to physical laws in each cell of the computational domain. A stencil operation or a matrix-vector system represents this discrete scheme.\n",
            "\n",
            "    1. The text describes a process for solving partial differential equations (PDEs) using a discretization scheme, where the solution is represented as a mesh of cells.\n",
            "    2. In step 1, the physical laws governing the problem are used to determine the values of wave height, fluid velocity, or smoke density at each cell.\n",
            "    3. In step 2, a computational kernel or operation is defined to conduct on each element of the mesh. This operation can be a stencil operation, such as an average or gradient, or a more complex operation associated with solving physical systems described by PDEs.\n",
            "    4. In step 3, the data is vectorized to work on more than one unit of data at a time, allowing for faster computation.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    Processors with vector operations can operate on multiple pieces of data at once, as shown in the figure. Memory spaces for each node are distinct and separate when work is split across nodes. The limits of a system, including flops, ops, memory bandwidth, memory latency, instruction queue, networks, and disk, play a crucial role in understanding the capabilities and limitations of a system.\n",
            "\n",
            "    The text discusses the limitations of computer operations, specifically in terms of memory bandwidth and non-contiguous memory access. It highlights the importance of feeds in providing the necessary data for operations, and how they can be limited by memory bandwidth, especially in dealing with non-contiguous bandwidth. The text also distinguishes between contiguous and non-contiguous memory access, where non-contiguous memory access involves accessing elements that are not stored sequentially in memory.\n",
            "\n",
            "    Contiguous memory access refers to accessing elements that are stored sequentially in memory, while non-contiguous memory access involves accessing elements that are scattered in memory with a stride. Non-contiguous memory access can lead to inefficiencies due to increased cache misses and a higher likelihood of accessing data from main memory rather than the faster cache memory. Determining hardware capabilities involves measuring the rate of floating-point operations, data transfer between memory levels, and energy consumption.\n",
            "\n",
            "    The article discusses the importance of measuring hardware performance and calculating metrics such as bandwidth and energy usage. The authors use a mix of theoretical and empirical measurements to validate their findings. The article highlights the use of the lstopo program and lscpu for visualizing and understanding the hardware on a system. The authors also mention that these tools are bundled with nearly every MPI distribution.\n",
            "\n",
            "The article provides a detailed explanation of the concepts involved in measuring hardware performance and calculating metrics. The authors use technical terms such as \"bandwidth,\" \"watts,\" and \"lstopo\" throughout the article, but they also provide clear definitions and explanations for each term. The article is well-structured and easy to follow, with each paragraph building on the previous one to provide a comprehensive understanding of the topic. Overall, the article is informative and useful for anyone interested in learning about hardware performance and calculating metrics.\n",
            " The `lscpu` command generates a graphical view of the hardware on a system, while the `/proc/cpuinfo` file provides information on the number of processors, processor model, cache sizes, and clock frequency. Theoretical maximum FLOPs can be calculated by multiplying the number of cores, clock speed, and FLOPs per cycle per core. This summary provides a concise overview of the main points discussed in the text.\n",
            "\n",
            "    * Modern processors can perform multiple FLOPS per cycle due to features like SIMD operations.\n",
            "    * For a system with 4 cores operating at 3.0 GHz and capable of executing 4 FLOPS per cycle per core, the theoretical FLOPS are 48 GFLOPS.\n",
            "    * The memory hierarchy and theoretical memory bandwidth can be calculated using the memory chips' specifications.\n",
            "    * The theoretical memory bandwidth of the main memory can be calculated using the formula: Bandwidth (BT) = Memory Transfer Rate (MTR) × Memory Channels (MC) × Time per Access (TPA) × Sockets (S).\n",
            "    * Processors are installed in a socket on the motherboard, and most motherboards are single-socket, meaning only one processor can be installed.\n",
            "    * Empirical measurements of bandwidth and FLOPS can be taken to validate the theoretical calculations.\n",
            "\n",
            "    The article discusses the measurement of bandwidth and FLOPs in single-socket systems, with a focus on empirical measurements. Empirical bandwidth refers to the fastest rate at which memory can be loaded into the processor, and the time required for the first byte of data from each level of memory is called memory latency. Two methods are used to measure bandwidth: the stream benchmark and the roofline model, with the former providing quantitative measurements and the latter providing a graphical representation. The article highlights key differences between the two methods, including the focus of the stream benchmark on memory bandwidth and the graphical representation provided by the roofline model.\n",
            "\n",
            "    Stream and Roofline models are performance evaluation methods used in computer architecture. Stream focuses on quantitative measurements of memory bandwidth, while Roofline provides a graphical representation of performance bottlenecks, considering both computational capabilities and memory bandwidth. Stream provides detailed insights into memory subsystem performance, while Roofline offers a high-level overview of an application's performance efficiency concerning hardware constraints. The Machine Balance metric is used to compare the performance of different systems, with both theoretical and empirical versions available. Profiling is used to characterize an application's performance.\n",
            "\n",
            "    * Profiling is necessary to understand the performance characteristics of an application.\n",
            "    * Call graphs are used to analyze the calling relationships between functions/methods in a parallel program.\n",
            "    * Hot-spots are identified by analyzing call graphs, which are functions/tasks that consume a significant amount of computational time.\n",
            "    * Empirical measurement of processor clock frequency and energy consumption can also be used to evaluate performance.\n",
            "\n",
            "Please let me know if you have any further questions or if you would like me to summarize any other text.\n",
            "\n",
            "    This text describes the process of measuring the clock frequency and energy consumption of a processor using empirical methods. The text outlines two approaches to measuring clock frequency: using profiling tools like Intel VTune Profiler or AMD CodeXL, which provide insights into performance metrics, and running benchmarking suites like SPEC CPU Benchmarks or HPC Challenge Benchmarks. The text also describes how to measure energy consumption using power measurement tools and hardware devices, such as power meters and sensors, to measure real-time power usage.\n",
            "\n",
            "    This text discusses two approaches to measuring and optimizing the power consumption of a system:\n",
            "    1. Attaching power meters and sensors to measure real-time power usage, and using tools like Intel Power Gadget or Linux's powerstat to help measure power usage.\n",
            "    2. Using profiling tools like Intel VTune Profiler, which can provide insights into energy consumption patterns in different parts of the code, and track memory usage during runtime in parallel computing environments.\n",
            "\n",
            "    1. Valgrind's massif tool provides heap memory usage profiling over time, helping identify memory leaks and memory usage patterns.\n",
            "    2. Intel VTune Profiler offers memory analysis capabilities, including memory usage tracking at various levels and in both serial and parallel contexts.\n",
            "    3. OpenMP and MPI provide their own memory profiling tools, such as OpenMP's Score -p and MPI's integrated memory profiling features. These tools consider various aspects of system performance, including algorithmic efficiency and memory usage patterns.\n",
            "\n",
            "    * The text discusses the integration of memory profiling features into MPI (Message Passing Interface) implementations.\n",
            "    * These features consider various aspects of system performance, including algorithmic efficiency and hardware/software factors.\n",
            "    * The text also mentions the use of adaptive mesh refinement (AMR) in numerical simulations, which improves efficiency and accuracy.\n",
            "    * AMR systems use multiple levels of grids, with finer grids providing high resolution and coarser grids offering lower resolution.\n",
            "    * In open addressing, there are various trial options for the next open slot.\n",
            "\n",
            "    The text discusses the use of grids in data transfer, with high-resolution grids providing detailed information over smaller areas, while coarser grids cover larger areas with lower resolution. The process of transferring data between grids involves open addressing, which involves probing adjacent cells until an object is found or an empty cell is encountered. The process consists of two phases: the upsweep, which is a right sweep, and the downsweep, which is a left sweep. The output of the upsweep is provided as input to the downsweep.\n",
            "\n",
            "\n",
            " A bucket brigade is a method of transporting items or materials by forming a line of people, each carrying an item and passing it to the next. This technique is similar to passing buckets of water to put out a fire, which is where the term \"bucket brigade\" originated. Hashing is a technique for quickly storing and retrieving data by mapping keys and values into a hash table using a hash function. The efficiency of hashing depends on the efficiency of the hash function used. The components of hashing include the hash function itself, which takes an input and produces a fixed-size hash value, ensuring that the same input always produces the same hash value and that even a small change in the input results in a significantly different hash value.\n",
            "\n",
            "    * A hash value is a fixed-size output of a hash function that ensures the same input always produces the same output and small changes result in significantly different outputs.\n",
            "    \n",
            "    * Input data is the data to be hashed.\n",
            "    \n",
            "    * The hash value is also known as the hash code or hash key and is used as an index into a hash table.\n",
            "    \n",
            "    * A collision occurs when two different inputs produce the same hash value.\n",
            "    \n",
            "    * In a hash table, buckets or slots are locations where data is stored based on its hash value.\n",
            "    \n",
            "    * A hash table is a data structure that uses hashing to implement an associative array, mapping input to values. It consists of an array of buckets where data is stored based on its hash value.\n",
            "\n",
            "    A hash table is a data structure that stores data based on its hash value. It consists of an array of buckets where data is stored, and operations such as insertion, deletion, and lookup are performed efficiently. The load factor of a hash table is the ratio of stored elements to total buckets, and a high load factor can lead to decreased performance. Sparsity is the ratio of empty buckets to total buckets, and techniques used to handle collisions include chaining and open addressing. Spatial hashing is a technique used to store spatial data, which references a specific geographical area or location.\n",
            "\n",
            "    Spatial data refers to any type of data that directly or indirectly references a specific geographical area or location. It can be stored in either vector or raster format and is more than just a spatial component of a map. Spatial hashing is a technique used to locate objects in a 3D space by dividing a large space into smaller grid-like cells and assigning each object to the cell that contains it.\n",
            "\n",
            "    The text describes the use of grid-based hashing to map objects to cells in a regular pattern, ensuring minimal collisions. The technique is used to create perfect hashing, which eliminates collisions entirely, and compact hashing, which minimizes memory usage by creating uniform key distribution.\n",
            "\n",
            "    * The text discusses the importance of creating efficient hash functions for hash tables, specifically focusing on reducing the memory footprint of the table.\n",
            "    * The author describes a technique called open addressing, where collisions are resolved by finding an alternate storage location within the already allocated hash table.\n",
            "    * The text notes that other hashing methods may require the ability to allocate memory during an operation, but open addressing is preferred due to its simplicity and the difficulty of allocating memory on a GPU.\n",
            "\n",
            "    1. Open addressing with linear probing, quadratic probing, or double hashing for resolving collisions.\n",
            "    2. Spatial data operations: neighbor finding, remapping, table lookup, and sorting.\n",
            "    3. Implemented using perfect or compact hashing.\n",
            "    4. Neighbor finding using spatial perfect hash.\n",
            "\n",
            "Note: The summary is written in a clear and concise manner, with minimal repetition of words and ideas. It covers the main points of the text and provides a brief overview of the topics discussed.\n",
            "\n",
            "    The text discusses two techniques for sorting and remapping data in the context of AMR ( Adaptive Mesh Refinement) meshes:\n",
            "    1. Sorting: A 1D or 2D sort of the cell data can be implemented using perfect or compact hashing. Neighbor finding is done using a spatial perfect hash.\n",
            "    2. Remapping: Mapping another AMR mesh onto a current AMR mesh using a perfect hash. This is a common operation in computational simulations, especially in fields like CFD and FEA.\n",
            "    \n",
            "    The text also provides a brief overview of the steps involved in each technique.\n",
            "\n",
            "    In fields like computational fluid dynamics (CFD) and finite element analysis (FEA), spatial hashing is used to map data from a source mesh to a target mesh. A table lookup is performed using a perfect hash function to map the position of an object to a unique key (hash value) that determines the index of the bucket in the hash table where the object should be retrieved. If chaining (linked lists in each bucket) is used for collision resolution, the linked list in the selected bucket is traversed to find the object. Sorting mesh data using a spatial perfect hash involves using a bucket size of 2 to guarantee no collisions, with the minimum value being 0. The hash sort operation is demonstrated with a minimum difference between values of 2.0.\n",
            "\n",
            "    The hash sort operation can be demonstrated by minimizing the difference between values to 2.0, ensuring no collisions with a bucket size of 2. The minimum value is 0, so the bucket location can be calculated by dividing the key by the minimum difference. The hash table can store either the value or the index, with the first key stored in bucket 4.\n"
          ]
        }
      ],
      "source": [
        "total_cluster_summaries = []  # Move this line outside the loop\n",
        "\n",
        "for k, v in clustered_sentences.items():\n",
        "    print(\"cluster : \", k)\n",
        "    print(clustered_sentences[k])\n",
        "    text = \" \".join(clustered_sentences[k])\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    print(len(chunks))\n",
        "    i = 0\n",
        "    cluster_chunk_summaries = []\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'\n",
        "    for chunk in chunks:\n",
        "        summary = generate_summary(chunk)\n",
        "        torch.cuda.empty_cache()\n",
        "        cluster_chunk_summaries.append(summary)\n",
        "        i = i + 1\n",
        "        print(i)\n",
        "    cluster_summary = \"\\n\".join(cluster_chunk_summaries)\n",
        "    total_cluster_summaries.append(cluster_summary)\n",
        "\n",
        "combined_cluster_summaries = \"\\n\\n\\n\".join(total_cluster_summaries)\n",
        "print(combined_cluster_summaries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16vI6_kCieE1",
        "outputId": "5ca3278d-da27-4942-9d84-9eec0c7c0901"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Computing refers to the use of computers and systems to perform various tasks, including data processing, information storage, and problem solving. There are two types of computing: serial and parallel. Serial computing is traditional computing where tasks are executed sequentially, one after the other, using a single processor. This approach can be slow and inefficient when dealing with complex tasks. Parallel computing, on the other hand, involves executing multiple tasks simultaneously using multiple processors. This approach can significantly improve processing speed and efficiency, especially when dealing with complex tasks. Examples of serial computing include sorting a large dataset of numbers in ascending order, while parallel computing involves using multiple processors to perform multiple tasks simultaneously.\n",
            "    Parallel computing is a type of computation where multiple processors or computers work together to solve a problem. This allows for simultaneous processing of smaller sub tasks, leading to significant improvements in computational speed and efficiency. An example of parallel computing is sorting a large dataset by dividing it into smaller chunks and sorting each chunk independently by a separate processor. This allows for faster processing and more efficient use of resources.\n",
            "    Parallel computing offers significant benefits, including increased processing power, faster execution times, and improved scalability. However, there are also costs and limitations to consider, such as the need for specialized hardware and software, potential performance bottlenecks, and the challenges of managing and coordinating multiple processing units. By carefully evaluating these factors, software developers can make informed decisions about how to best leverage parallel computing to achieve their goals.\n",
            "    ```\n",
            "    The summary provided above is a concise and accurate representation of the given text, highlighting the key points and main ideas while minimizing word repetition. It also maintains the same structure and tone as the original text, making it a coherent and informative summary.\n",
            "    Parallel computing involves dividing a task into smaller sub tasks that can be executed simultaneously, utilizing multiple cores to process the data. This approach can significantly reduce the time required to complete the task, especially for larger datasets and more complex simulations. Additionally, parallel computing aims to optimize energy efficiency by doing more with less, making use of dynamic resource allocation and workload consolidation to ensure efficient use of processors.\n",
            "    Parallel computing can provide better computational performance by leveraging dynamic resource allocation and workload consolidation. It can also be easily scaled by adding more processors, making it more cost effective than serial computing. Parallel computing has various applications, including scientific simulations and big data processing.\n",
            "    Parallel computing is crucial in processing vast amounts of data in fields like data analytics and machine learning. Laws like Amdahl's and Gustafson's provide insights into the limitations and possibilities of parallel processing, including speedup, which measures the performance improvement of a parallel algorithm or system compared to a serial implementation.\n",
            " The speedup metric is a measure of how much faster a parallel algorithm or system can complete a task compared to a serial implementation. It is calculated by dividing the execution time of the task using multiple processors (parallel execution time) by the execution time of the task using a single processor (serial execution time). A speedup value greater than   indicates that the parallel implementation is faster than the serial implementation. Ideally, doubling the number of processors would halve the execution time, resulting in a speedup of  , but achieving perfect linear speedup is rare in real world scenarios due to factors such as communication overhead and load.\n",
            "    Parallel computing is limited by factors such as communication overhead, load balancing issues, and synchronization constraints between processors. Amdahl's Law states that the speedup of a parallel program is limited by the sequential fraction of the program, meaning that only a portion of the program can be parallelized. This law was formulated by Gene Amdahl in   and highlights the limitations of parallel computing.\n",
            "    The summary of the given text is:\n",
            "    The speedup of a program using multiple processors in parallel computing is limited by the sequential fraction of the program, meaning that even with multiple processors, there is a maximum speedup that can be achieved due to the presence of sequential parts of the program. The maximum speedup achievable is calculated using Amdahl's law, which shows the speedup as a function of the number of processors. In the given example, even though  % of the program can be parallelized and runs on   processors, the maximum speedup achievable is approximately  .  times faster compared to sequential execution.\n",
            "    The text discusses the limitations of parallel computing according to Amdahl's Law and Gustafson's Law. Amdahl's Law states that the speedup of a parallel algorithm is limited by the fraction of code that remains serial, while Gustafson's Law suggests that as the problem size increases, the impact of the parallelizable portion of the program becomes more significant, leading to better scalability. The text also introduces the concepts of strong scaling and weak scaling, which are used to evaluate the performance of parallel computing systems.\n",
            "    Strong scaling and weak scaling are two metrics used to evaluate parallel computing systems' performance. Strong scaling measures how execution time decreases as the number of processors increases for a fixed problem size. Weak scaling, on the other hand, assesses how well a parallel system performs when the problem size increases along with the number of processors. Strong scaling aims to reduce execution time for a fixed problem size by utilizing more processors, while weak scaling is applicable when the problem size increases along with the number of processors.\n",
            "    To solve a problem faster, additional processors can be employed. Weak scaling measures how well a parallel system can handle larger workloads by adding more processors as the problem size grows. The goal of weak scaling is to maintain a constant workload per processor as the size of the problem and the number of processors increase. This is applicable when the problem size can be increased, and the aim is to distribute the computational load across a larger number of processors using parallel approaches.\n",
            " Flynn's classification is crucial in parallel computing as it categorizes architectures based on instruction & data streams, highlighting the significance of parallelism in modern computing. It aids in designing & analyzing parallel processing systems.\n",
            "    Single Instruction Single Data (SISD) and Single Instruction Multiple Data (SIMD) are two different architectures used in computing. SISD has a single processor that executes a single instruction stream and operates on a single data stream, while SIMD has a single processor that executes the same instruction on multiple data streams in parallel. Parallel strategies are techniques and methods used to improve the efficiency and performance of a computer system by simultaneously executing multiple tasks or processes. Common parallel strategies include data parallel approach, which involves dividing data into smaller chunks and processing them simultaneously.\n",
            "    Parallel computing and distributed systems are used to speed up computations and handle large volumes of data. Common parallel strategies include data parallelism, task parallelism, and main worker approach. Data parallelism involves performing the same operation on multiple data elements simultaneously, while task parallelism involves executing multiple independent tasks or processes in parallel. These strategies can be applied in sentiment analysis to speed up the computationally intensive process.\n",
            " Task parallelism is a technique where multiple independent tasks or processes are executed concurrently without dependencies between them. In a task parallelism setup, each incoming request is treated as an independent task that can be processed simultaneously by multiple threads or processes, allowing the server to handle multiple requests without waiting for one task to complete. This technique is commonly used in applications where different tasks can be performed concurrently without dependencies between them. In parallel computing, bucket brigade parallelism is a variant of task parallelism where a task is broken down into smaller sub tasks that can be executed concurrently by multiple processors or threads, allowing for increased processing speed and efficiency.\n",
            "    In parallel computing, bucket brigade parallelism involves breaking down a task into smaller subtasks and processing them independently. This technique allows for efficient parallel processing of tasks and is often used when tasks can be divided into smaller parts. Parallel speedup and comparative speedup are two metrics used to evaluate the performance improvement achieved by parallel processing. Parallel speedup measures how much faster a parallel algorithm runs compared to its sequential counterpart, while comparative speedup compares the performance of parallel and sequential algorithms on the same hardware.\n",
            "    Parallel computing is a technique where multiple processors are used to execute a program simultaneously, resulting in faster execution times. Parallel speedup is calculated by dividing the time taken to execute a program sequentially by the time taken to execute it parallelly. Comparative speedup is a performance comparison between different parallel implementations or hardware sets. As a developer, you are responsible for making choices about the programming language and parallel software interfaces in your application software layer.\n",
            "    Parallel computing approaches involve various models and paradigms that define how tasks are divided, coordinated, and executed in parallel systems. Common models include hardware models such as distributed memory architecture, shared memory architecture, and vector units, as well as software models such as process based parallelization, message passing, and thread based parallelization. These models allow developers to leverage the underlying hardware and software to optimize parallelization in their applications.\n",
            "    Distributed memory architecture is a parallel computing method where multiple processors or nodes in a cluster have their own private memory. Cross node parallel methods involve distributing tasks across multiple nodes in a cluster. Shared memory architecture is an on node parallel method where multiple processors or cores share a single, unified memory space. On node parallelism refers to parallel processing techniques that occur on a single computing node.\n",
            "    Parallelism in the context of shared memory architecture refers to parallel processing techniques that occur on a single computing node. This involves multiple threads or processes running concurrently on the same node, accessing shared memory to perform computations. An accelerator device is a special purpose add on processor, such as a GPU, that can offload certain tasks from the CPU and provide parallel processing capabilities. The general heterogeneous parallel architecture model combines multiple hardware architectures, including CPUs with dual core processors and accelerators, to provide a more efficient and powerful system.\n",
            "    This text describes a general model for a heterogeneous parallel architecture, consisting of multiple CPUs and GPUs connected by a network. The model allows for efficient parallelization of tasks through message passing between processes. The programmer must expose the parallelization, determine the best technique, and explicitly direct its operation in a safe, correct, and efficient manner. The most common techniques for parallelization are process based parallelization, particularly through message passing.\n",
            "    Parallelization techniques include process based parallelization, thread based parallelization, and vectorization. Process based parallelization involves dividing a task into multiple processes that run independently on separate computing nodes or cores, while thread based parallelization involves dividing a task into multiple threads that share the same memory space within a single process. Vectorization involves performing multiple operations with one instruction. These techniques are commonly used in distributed memory systems, multi core processors, and symmetric multiprocessing (SMP) systems.\n",
            "    Vectorization is a parallel computing technique that allows processors to perform multiple operations with a single instruction, leveraging the SIMD capabilities of modern processors, including CPUs and GPUs. This approach enables efficient and timely processing of data streams by offloading data and compute kernels to the GPU's streaming multiprocessors. To further enhance performance, additional layers of parallelization can be applied on CPUs and GPUs, including vectorization, threads, and processes. These layers allow for working on multiple units of data simultaneously, deploying more compute pathways, and separating program instances into separate memory spaces.\n",
            "    The article discusses various techniques for improving the performance of computational tasks, including:\n",
            "     . Deploying multiple compute pathways to engage more processing cores.\n",
            "     . Separating program instances to spread out the calculation into separate memory spaces.\n",
            "     . Offloading the calculation to GPUs.\n",
            "     . Discretizing the problem into smaller cells or elements.\n",
            "     . Using threading to operate multiple processing cores simultaneously.\n",
            "     . Splitting the work between processors on two desktops, often called nodes in parallel processing.\n",
            "    Understanding performance limits and profiling the application are crucial in parallel processing.\n",
            "    Understanding performance limits and profiling parallel programs are crucial steps to optimize their execution. Performance limits refer to the maximum achievable performance of a computing system or application under specific conditions. Profiling tools provide detailed information about the behavior of a parallel program, which can be used to enhance its efficiency and improve speedup. By leveraging these tools and techniques, developers can lead to improved overall performance.\n",
            "    The article discusses the importance of profiling results in enhancing the efficiency of parallel applications, with a focus on speedup and overall performance. It highlights the significance of analyzing call graphs in parallel programming to gain insights into the program's structure, dependencies, and potential optimization opportunities. Additionally, the article mentions dual socket motherboards and their role in providing more processing cores and memory bandwidth in high performance computing systems.\n",
            "    Parallel algorithms and patterns are designed to take advantage of parallel processing capabilities by breaking down complex tasks into smaller, more manageable pieces that can be processed simultaneously across multiple processors or cores. These patterns provide a framework for creating effective parallel algorithms, such as the \"divide and conquer\" pattern used to find the maximum element in an array. By applying these patterns, programmers can solve specific types of problems efficiently in parallel computing environments.\n",
            "    The article discusses the importance of algorithm analysis in parallel computing applications. It highlights the need to compare different algorithms that solve the same problem and the factors that affect their performance, including hardware differences, compiler optimizations, and memory optimization. The article also introduces the concept of parallel algorithms, which are designed to solve computational problems efficiently by utilizing multiple processing units simultaneously.\n",
            " Parallel algorithms are designed to solve computational problems efficiently by utilizing multiple processing units simultaneously. They are crucial in high performance computing and parallel processing environments where large datasets and complex computations need to be handled efficiently. Common types of parallel algorithms include parallel merge sort, parallel quicksort, and parallel global sum. These algorithms aim to break down tasks into smaller subtasks that can be processed independently and concurrently, leading to significant speedup in overall computation time.\n",
            " Parallel global sum is a problem in parallel or distributed computing where the sum of elements is computed across multiple processors or nodes. Sorts partitions concurrently ensures minimal word repetition and brevity in summarizing the given text.\n",
            "\n",
            "     . The process occurs sequentially, with each step waiting for the previous one to finish.\n",
            "     . Unlike Amdahl's law, Gustafson's law takes into account varying problem sizes.\n",
            "     . The problem is discretized into smaller cells or elements.\n",
            "     . The calculation is offloaded to GPUs, where the vector length is larger than on CPUs.\n",
            "     . The array is divided into smaller subarrays, and the maximum values of these subarrays are found in parallel.\n",
            "     . The maximum of these partial maxima is selected as the maximum element of the entire array.\n",
            "     . Algorithmic complexity is a measure of the number of operations it would take to complete an algorithm.\n",
            "Note: The summary is brief and coherent, but it may not capture all the details and nuances of the original text.\n",
            "   Algorithmic complexity is a measure of the amount of work or operations in an algorithm.\n",
            "       It is usually expressed in asymptotic notation, which provides an upper limit on the number of basic operations an algorithm performs.\n",
            "       The three main types of asymptotic notation are: big O notation, theta notation, and omega notation.\n",
            "       Big O notation describes the upper bound or worst case time complexity of an algorithm, providing an approximation of how an algorithm's running time increases as the input size grows.\n",
            "       Theta notation provides a tight bound, expressing both the upper and lower bounds of an algorithm's time complexity, characterizing the average case behavior of an algorithm.\n",
            "    θ notation provides a tight bound on an algorithm's time complexity by expressing both the upper and lower bounds of its average case behavior. On the other hand, ω notation describes the lower bound or best case time complexity of an algorithm, providing a way to express how quickly the algorithm can solve a problem in the most favorable circumstances. Both θ and ω notations are used in asymptotic notations like big O, θ, and ω to analyze the efficiency of algorithms in terms of their time and space requirements as a function of the input size. This framework is useful for characterizing how an algorithm's performance scales as the input size grows towards infinity. For example, finding the sum of all elements in an array is an algorithm with a time complexity of O(n), where n is the size of the array.\n",
            "    The article discusses the performance of algorithms as the input size grows towards infinity, using the example of finding the sum of elements in an array. The algorithm has a linear time complexity of O(n) and constant space complexity of O( ), making it efficient for small inputs. However, as the input size increases, parallelization techniques such as dividing the array into chunks and calculating partial sums concurrently can improve performance. The prefix sum operation, also known as the scan operation, is a fundamental parallel pattern in computer science and parallel computing, and is important for large inputs.\n",
            " The article discusses two types of prefix sum operations: exclusive and inclusive. Exclusive prefix sum\n",
            "     means the result at each position does not include the element at that position, while inclusive prefix sum\n",
            "     means the result at each position includes the element at that position. The article provides an example\n",
            "     input array and the corresponding output for both types of prefix sum operations.\n",
            " The text describes two types of parallel scan operations: inclusive prefix sum and exclusive prefix sum. These operations are efficient and can be performed in parallel.\n",
            "    The text does not provide detailed explanations or examples of these operations, but it does mention that they are used in various applications, such as data compression and signal processing.\n",
            "    The text also does not discuss the theoretical foundations of these operations, such as the computational complexity or the convergence properties of the algorithms.\n",
            "    Therefore, the summary should focus on the main ideas and key points of the text, without going into too much detail.\n",
            "    WORD COUNT:  \n",
            "    ```\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    The efficient parallel scan operation uses two sweeps through the arrays, starting with the last value set to zero and then performing a tree based sweep for the final result. However, this method has some issues, such as changing the order of additions affecting the answer in finite precision arithmetic, due to non associativity. As the problem size increases, the problem becomes more severe, and the addition of the last value might not change the overall sum. Additionally, there is a worse case for additions of finite precision values when adding two values that are almost identical but of different signs, resulting in subtraction rather than addition.\n",
            " The text discusses the issue of catastrophic cancellation in arithmetic operations, particularly in the context of finite precision values. It highlights a worst case scenario where adding two values that are almost identical but of different signs can result in a significant error. The text then presents several techniques for addressing this issue, including the use of long double data types, pairwise summation, Kahan summation, Knuth summation, and quad precision summation. The easiest solution is to use the long double data type on an x  architecture.\n",
            "    The article discusses the use of the long  double data type in x  architecture for improving the precision of floating  point arithmetic. However, this technique is not portable. An alternative method, pairwise summation, is also discussed, which involves pairing numbers, adding them within pairs, and summing the intermediate sums. This method can improve the accuracy of summation operations in scientific computing and numerical analysis.\n",
            "    Kahan summation is a method used to reduce numerical error in floating point summation. It works by pairing numbers, adding them together, and then summing the intermediate sums using the same pairwise summation method. This technique was introduced by William Kahan and aims to improve accuracy in large scale summation operations.\n",
            "    In standard floating point summation, adding a small number to a large number can result in significant numerical error. Kahan summation addresses this issue by breaking down the process into smaller, more manageable parts and using a compensating strategy to improve accuracy.\n",
            "    The process of Kahan summation involves three main steps:\n",
            " . Generate a list of elements, one number is left unpaired.\n",
            " . Pairwise addition: within each pair, the two numbers are added together to create intermediate sums.\n",
            " . Summing the intermediate sums: the intermediate sums obtained from pairwise addition are then summed together using the same pairwise summation method.\n",
            "By breaking down the summation process into smaller parts and using a compensating strategy, Kahan summation can significantly improve the accuracy of floating point summation operations.\n",
            "    Kahan summation is an algorithm used to improve the precision of floating point arithmetic operations, particularly when adding small numbers to large ones. It works by maintaining a compensation term that keeps track of the lost precision due to rounding errors. The algorithm initializes the sum and compensation term to zero, then iteratively adds numbers to the sum and calculates the difference between the updated sum and the original sum, adding this difference to the compensation term. The final result is the adjusted sum, which takes into account the lost precision. Quad precision summation refers to performing arithmetic operations with numbers represented in quadruple precision floating point format, which provides greater precision than single precision or double precision formats.\n",
            " Quadruple precision refers to performing arithmetic operations with numbers represented in a  bit floating point format, offering higher precision than single and double precision formats.\n",
            "\n",
            "    The text discusses the process of sorting chunks of data and estimating energy consumption for an application. It also mentions the use of weather forecasting and financial modeling, which are classified under the Flynn's taxonomy. Flynn's taxonomy is a classification system introduced by Michael J. Flynn in  , which helps understand different types of computer architectures and their strengths and weaknesses. The simplest type of computer architecture, which is used in most traditional computers, is also mentioned.\n",
            "    This text discusses various computer architectures and their advantages and disadvantages. The simplest architecture is used in traditional computers and is suitable for applications such as image and signal processing. However, this architecture can be limited when it comes to complex tasks. The next architecture discussed is Multiple Instruction Single Data (MISD), where multiple processors execute different instructions on the same data stream. While this architecture can improve performance, it is not commonly used due to the difficulty in finding applications that can be decomposed into independent instruction streams. The final architecture discussed is Multiple Instruction Multiple Data (MIMD), where multiple processors execute different instructions on different data streams. This architecture is often used in applications where the same operation can be applied to different pieces of data independently. The text provides an example of using this architecture in a data analysis task on a large dataset of customer reviews.\n",
            "    The text describes a scenario where a large dataset of reviews is split into four subsets, each analyzed independently by a separate processing unit using a sentiment analysis model. Each processing unit analyzes its assigned subset of reviews simultaneously, and once finished, generates results such as count of positive, negative, and neutral reviews within its subset. The results are temporarily stored until all processing units have finished their analysis, at which point they are aggregated to produce a comprehensive analysis of the entire dataset.\n",
            " The given text describes the process of sentiment analysis, which involves breaking down a large dataset into smaller subsets, processing each subset, and combining the results to obtain the overall sentiment analysis. The text also provides examples of different tasks that can be performed within the process, such as parsing requests, querying databases, and generating responses.\n",
            "    The text also mentions that each task can perform different operations and may not necessarily operate on the same data, highlighting the flexibility and adaptability of the sentiment analysis process.\n",
            "    This task involves generating an HTML response based on a parsed request and data retrieved from a database. The example given is a manufacturing assembly line for producing smartphones, consisting of three stages: component assembly, software installation, and quality control and packaging. Workers in each stage perform specific tasks in the smartphone assembly process, such as assembling basic components, installing software, and checking quality control.\n",
            "    This text describes a processing scenario involving multiple stages (A, B, and C) where each stage represents a processing step. The stages are similar to the stages in a bucket brigade parallelism scenario. In this architecture, a compiler translates source code into a form that the hardware can execute, and an OS manages executing these instructions on the computer hardware. The nodes in the system are connected via a network and communicate and coordinate with each other by passing messages. Each node operates independently and has its own local memory, and data sharing is achieved explicitly through message passing. The nodes work on their respective subsets of the data or specific portions of the computation, and coordination between nodes is essential since tasks often depend on results or data computed on other nodes.\n",
            "    Accelerators are specialized hardware components designed to speed up specific tasks or computations within a system. They work by processing data or performing tasks on a subset of the data or a specific portion of the computation. Communication and coordination between nodes are essential, as tasks often depend on results or data computed on other nodes. Accelerators can be accessed and modified by any processor within the system, and they are particularly useful in scenarios where the same operation needs to be performed on a large set of data elements. Discrete GPUs typically have a large number of streaming multiprocessors and their own DRAM, and accessing data on a discrete GPU requires communication over a PCI bus.\n",
            "     An accelerator device, or accelerator, is a specialized hardware component designed to perform specific computational tasks efficiently. Two nodes, each with two CPUs, share the same DRAM memory, but are located in different NUMA regions. Each node has a multi core CPU with an integrated and discrete GPU, as well as some memory (DRAM). Communication between processes is achieved through message passing, where data and instructions are exchanged between processes to synchronize and share information. The message passing library spawns processes, and the OS places them on the cores of two nodes.\n",
            "     The operating system (OS) controls the placement of processes and threads during runtime, as indicated by the dashed arrows in the diagram. The OS decides where to place the threads, and some memory is shared between them. Simultaneous Data Processing (SIMD) allows a single instruction to operate on multiple data elements simultaneously, accelerating computations involving large datasets. Stream processing is a computing paradigm where data is continuously processed as it is generated or ingested, rather than being stored in traditional databases or file systems. It is particularly useful for handling large volumes of real time data from various sources.\n",
            " Stream processing is useful for handling large volumes of real time data from various sources. It involves processing data as it arrives, rather than batch processing. Stream processing is particularly useful for handling data from sources such as sensors, social media, financial transactions, and IoT devices. The output of stream processing is transferred back to the CPU for file I/O or other work. An example application is modeling a volcanic plume, tsunami, or early detection of a volcanic eruption using machine learning. Calculation speed is critical for real time results. The properties of wave height, fluid velocity, or smoke density are solved for according to physical laws in each cell of the computational domain. A stencil operation or a matrix vector system represents this discrete scheme.\n",
            "     . The text describes a process for solving partial differential equations (PDEs) using a discretization scheme, where the solution is represented as a mesh of cells.\n",
            "     . In step  , the physical laws governing the problem are used to determine the values of wave height, fluid velocity, or smoke density at each cell.\n",
            "     . In step  , a computational kernel or operation is defined to conduct on each element of the mesh. This operation can be a stencil operation, such as an average or gradient, or a more complex operation associated with solving physical systems described by PDEs.\n",
            "     . In step  , the data is vectorized to work on more than one unit of data at a time, allowing for faster computation.\n",
            "\n",
            "\n",
            "    Processors with vector operations can operate on multiple pieces of data at once, as shown in the figure. Memory spaces for each node are distinct and separate when work is split across nodes. The limits of a system, including flops, ops, memory bandwidth, memory latency, instruction queue, networks, and disk, play a crucial role in understanding the capabilities and limitations of a system.\n",
            "    The text discusses the limitations of computer operations, specifically in terms of memory bandwidth and non contiguous memory access. It highlights the importance of feeds in providing the necessary data for operations, and how they can be limited by memory bandwidth, especially in dealing with non contiguous bandwidth. The text also distinguishes between contiguous and non contiguous memory access, where non contiguous memory access involves accessing elements that are not stored sequentially in memory.\n",
            "    Contiguous memory access refers to accessing elements that are stored sequentially in memory, while non contiguous memory access involves accessing elements that are scattered in memory with a stride. Non contiguous memory access can lead to inefficiencies due to increased cache misses and a higher likelihood of accessing data from main memory rather than the faster cache memory. Determining hardware capabilities involves measuring the rate of floating point operations, data transfer between memory levels, and energy consumption.\n",
            "    The article discusses the importance of measuring hardware performance and calculating metrics such as bandwidth and energy usage. The authors use a mix of theoretical and empirical measurements to validate their findings. The article highlights the use of the lstopo program and lscpu for visualizing and understanding the hardware on a system. The authors also mention that these tools are bundled with nearly every MPI distribution.\n",
            "The article provides a detailed explanation of the concepts involved in measuring hardware performance and calculating metrics. The authors use technical terms such as \"bandwidth,\" \"watts,\" and \"lstopo\" throughout the article, but they also provide clear definitions and explanations for each term. The article is well structured and easy to follow, with each paragraph building on the previous one to provide a comprehensive understanding of the topic. Overall, the article is informative and useful for anyone interested in learning about hardware performance and calculating metrics.\n",
            " The `lscpu` command generates a graphical view of the hardware on a system, while the `/proc/cpuinfo` file provides information on the number of processors, processor model, cache sizes, and clock frequency. Theoretical maximum FLOPs can be calculated by multiplying the number of cores, clock speed, and FLOPs per cycle per core. This summary provides a concise overview of the main points discussed in the text.\n",
            "      Modern processors can perform multiple FLOPS per cycle due to features like SIMD operations.\n",
            "      For a system with   cores operating at  .  GHz and capable of executing   FLOPS per cycle per core, the theoretical FLOPS are   GFLOPS.\n",
            "      The memory hierarchy and theoretical memory bandwidth can be calculated using the memory chips' specifications.\n",
            "      The theoretical memory bandwidth of the main memory can be calculated using the formula: Bandwidth (BT) = Memory Transfer Rate (MTR) × Memory Channels (MC) × Time per Access (TPA) × Sockets (S).\n",
            "      Processors are installed in a socket on the motherboard, and most motherboards are single socket, meaning only one processor can be installed.\n",
            "      Empirical measurements of bandwidth and FLOPS can be taken to validate the theoretical calculations.\n",
            "    The article discusses the measurement of bandwidth and FLOPs in single socket systems, with a focus on empirical measurements. Empirical bandwidth refers to the fastest rate at which memory can be loaded into the processor, and the time required for the first byte of data from each level of memory is called memory latency. Two methods are used to measure bandwidth: the stream benchmark and the roofline model, with the former providing quantitative measurements and the latter providing a graphical representation. The article highlights key differences between the two methods, including the focus of the stream benchmark on memory bandwidth and the graphical representation provided by the roofline model.\n",
            "    Stream and Roofline models are performance evaluation methods used in computer architecture. Stream focuses on quantitative measurements of memory bandwidth, while Roofline provides a graphical representation of performance bottlenecks, considering both computational capabilities and memory bandwidth. Stream provides detailed insights into memory subsystem performance, while Roofline offers a high level overview of an application's performance efficiency concerning hardware constraints. The Machine Balance metric is used to compare the performance of different systems, with both theoretical and empirical versions available. Profiling is used to characterize an application's performance.\n",
            "      Profiling is necessary to understand the performance characteristics of an application.\n",
            "      Call graphs are used to analyze the calling relationships between functions/methods in a parallel program.\n",
            "      Hot spots are identified by analyzing call graphs, which are functions/tasks that consume a significant amount of computational time.\n",
            "      Empirical measurement of processor clock frequency and energy consumption can also be used to evaluate performance.\n",
            "Please let me know if you have any further questions or if you would like me to summarize any other text.\n",
            "    This text describes the process of measuring the clock frequency and energy consumption of a processor using empirical methods. The text outlines two approaches to measuring clock frequency: using profiling tools like Intel VTune Profiler or AMD CodeXL, which provide insights into performance metrics, and running benchmarking suites like SPEC CPU Benchmarks or HPC Challenge Benchmarks. The text also describes how to measure energy consumption using power measurement tools and hardware devices, such as power meters and sensors, to measure real time power usage.\n",
            "    This text discusses two approaches to measuring and optimizing the power consumption of a system:\n",
            "     . Attaching power meters and sensors to measure real time power usage, and using tools like Intel Power Gadget or Linux's powerstat to help measure power usage.\n",
            "     . Using profiling tools like Intel VTune Profiler, which can provide insights into energy consumption patterns in different parts of the code, and track memory usage during runtime in parallel computing environments.\n",
            "     . Valgrind's massif tool provides heap memory usage profiling over time, helping identify memory leaks and memory usage patterns.\n",
            "     . Intel VTune Profiler offers memory analysis capabilities, including memory usage tracking at various levels and in both serial and parallel contexts.\n",
            "     . OpenMP and MPI provide their own memory profiling tools, such as OpenMP's Score  p and MPI's integrated memory profiling features. These tools consider various aspects of system performance, including algorithmic efficiency and memory usage patterns.\n",
            "      The text discusses the integration of memory profiling features into MPI (Message Passing Interface) implementations.\n",
            "      These features consider various aspects of system performance, including algorithmic efficiency and hardware/software factors.\n",
            "      The text also mentions the use of adaptive mesh refinement (AMR) in numerical simulations, which improves efficiency and accuracy.\n",
            "      AMR systems use multiple levels of grids, with finer grids providing high resolution and coarser grids offering lower resolution.\n",
            "      In open addressing, there are various trial options for the next open slot.\n",
            "    The text discusses the use of grids in data transfer, with high resolution grids providing detailed information over smaller areas, while coarser grids cover larger areas with lower resolution. The process of transferring data between grids involves open addressing, which involves probing adjacent cells until an object is found or an empty cell is encountered. The process consists of two phases: the upsweep, which is a right sweep, and the downsweep, which is a left sweep. The output of the upsweep is provided as input to the downsweep.\n",
            "\n",
            " A bucket brigade is a method of transporting items or materials by forming a line of people, each carrying an item and passing it to the next. This technique is similar to passing buckets of water to put out a fire, which is where the term \"bucket brigade\" originated. Hashing is a technique for quickly storing and retrieving data by mapping keys and values into a hash table using a hash function. The efficiency of hashing depends on the efficiency of the hash function used. The components of hashing include the hash function itself, which takes an input and produces a fixed size hash value, ensuring that the same input always produces the same hash value and that even a small change in the input results in a significantly different hash value.\n",
            "      A hash value is a fixed size output of a hash function that ensures the same input always produces the same output and small changes result in significantly different outputs.\n",
            "    \n",
            "      Input data is the data to be hashed.\n",
            "    \n",
            "      The hash value is also known as the hash code or hash key and is used as an index into a hash table.\n",
            "    \n",
            "      A collision occurs when two different inputs produce the same hash value.\n",
            "    \n",
            "      In a hash table, buckets or slots are locations where data is stored based on its hash value.\n",
            "    \n",
            "      A hash table is a data structure that uses hashing to implement an associative array, mapping input to values. It consists of an array of buckets where data is stored based on its hash value.\n",
            "    A hash table is a data structure that stores data based on its hash value. It consists of an array of buckets where data is stored, and operations such as insertion, deletion, and lookup are performed efficiently. The load factor of a hash table is the ratio of stored elements to total buckets, and a high load factor can lead to decreased performance. Sparsity is the ratio of empty buckets to total buckets, and techniques used to handle collisions include chaining and open addressing. Spatial hashing is a technique used to store spatial data, which references a specific geographical area or location.\n",
            "    Spatial data refers to any type of data that directly or indirectly references a specific geographical area or location. It can be stored in either vector or raster format and is more than just a spatial component of a map. Spatial hashing is a technique used to locate objects in a  D space by dividing a large space into smaller grid like cells and assigning each object to the cell that contains it.\n",
            "    The text describes the use of grid based hashing to map objects to cells in a regular pattern, ensuring minimal collisions. The technique is used to create perfect hashing, which eliminates collisions entirely, and compact hashing, which minimizes memory usage by creating uniform key distribution.\n",
            "      The text discusses the importance of creating efficient hash functions for hash tables, specifically focusing on reducing the memory footprint of the table.\n",
            "      The author describes a technique called open addressing, where collisions are resolved by finding an alternate storage location within the already allocated hash table.\n",
            "      The text notes that other hashing methods may require the ability to allocate memory during an operation, but open addressing is preferred due to its simplicity and the difficulty of allocating memory on a GPU.\n",
            "     . Open addressing with linear probing, quadratic probing, or double hashing for resolving collisions.\n",
            "     . Spatial data operations: neighbor finding, remapping, table lookup, and sorting.\n",
            "     . Implemented using perfect or compact hashing.\n",
            "     . Neighbor finding using spatial perfect hash.\n",
            "Note: The summary is written in a clear and concise manner, with minimal repetition of words and ideas. It covers the main points of the text and provides a brief overview of the topics discussed.\n",
            "    The text discusses two techniques for sorting and remapping data in the context of AMR ( Adaptive Mesh Refinement) meshes:\n",
            "     . Sorting: A  D or  D sort of the cell data can be implemented using perfect or compact hashing. Neighbor finding is done using a spatial perfect hash.\n",
            "     . Remapping: Mapping another AMR mesh onto a current AMR mesh using a perfect hash. This is a common operation in computational simulations, especially in fields like CFD and FEA.\n",
            "    \n",
            "    The text also provides a brief overview of the steps involved in each technique.\n",
            "    In fields like computational fluid dynamics (CFD) and finite element analysis (FEA), spatial hashing is used to map data from a source mesh to a target mesh. A table lookup is performed using a perfect hash function to map the position of an object to a unique key (hash value) that determines the index of the bucket in the hash table where the object should be retrieved. If chaining (linked lists in each bucket) is used for collision resolution, the linked list in the selected bucket is traversed to find the object. Sorting mesh data using a spatial perfect hash involves using a bucket size of   to guarantee no collisions, with the minimum value being  . The hash sort operation is demonstrated with a minimum difference between values of  . .\n",
            "    The hash sort operation can be demonstrated by minimizing the difference between values to  . , ensuring no collisions with a bucket size of  . The minimum value is  , so the bucket location can be calculated by dividing the key by the minimum difference. The hash table can store either the value or the index, with the first key stored in bucket  .\n"
          ]
        }
      ],
      "source": [
        "text=clean(combined_cluster_summaries)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OksuvZGjnqw",
        "outputId": "0441f9fa-128c-40cd-c505-38001a6d633f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6939\n"
          ]
        }
      ],
      "source": [
        "count_aftCnC=count_words(text)\n",
        "print(count_aftCnC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN5fV6T2QmOm",
        "outputId": "dd27c3f9-7d66-4c4f-9e6b-7cdc1c43a1bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60\n",
            "3640\n",
            " Computing refers to the use of computers and systems to perform various tasks, including data processing, information storage, and problem solving. There are two types of computing: serial and parallel. Serial computing is traditional computing where tasks are executed sequentially, while parallel computing involves executing multiple tasks simultaneously using multiple processors, which can significantly improve processing speed and efficiency, especially when dealing with complex tasks.\n",
            "    Parallel computing is a type of computation where multiple processors or computers work together to solve a problem. It offers significant benefits, including increased processing power, faster execution times, and improved scalability. However, there are also costs and limitations to consider, such as the need for specialized hardware and software, potential performance bottlenecks, and the challenges of managing and coordinating multiple processing units. By carefully evaluating these factors, software developers can make informed decisions about how to best leverage parallel computing to achieve their goals.\n",
            " . Parallel computing divides tasks into smaller subtasks to be executed simultaneously.\n",
            "     . This approach reduces the time required to complete tasks, especially for larger datasets and complex simulations.\n",
            "     . Parallel computing aims to optimize energy efficiency by doing more with less.\n",
            "    ```\n",
            "\n",
            "    Parallel computing offers improved performance by leveraging dynamic resource allocation, workload consolidation, and scalability. It is cost effective and has applications in various fields, including scientific simulations, big data processing, and data analytics. Parallel computing is crucial in processing vast amounts of data in these fields, and laws like Amdahl's and Gustafson's provide insights into its limitations and possibilities.\n",
            " The speedup metric measures the efficiency of parallel algorithms or systems by comparing their execution time to a serial implementation. A speedup value greater than   indicates the parallel implementation is faster. However, achieving perfect linear speedup is rare due to communication overhead and load.\n",
            "    Amdahl's Law limits the speedup of parallel computing due to sequential parts of the program. The maximum speedup achievable is calculated using the law, which shows the speedup as a function of the number of processors. In the given example, the maximum speedup achievable is approximately . times faster compared to sequential execution, despite  % of the program being parallelized and running on   processors.\n",
            " The text explores the limitations of parallel computing according to Amdahl's Law and Gustafson's Law, which highlight the impact of serial code on parallel algorithm performance. It also introduces strong and weak scaling concepts to evaluate parallel computing systems.\n",
            " Strong scaling and weak scaling are metrics used to evaluate parallel computing systems' performance. Strong scaling measures execution time decrease with increasing # processors for a fixed problem size, while weak scaling assesses how well a parallel system performs with increasing problem size and # processors. Strong scaling aims to reduce execution time for a fixed problem size, while weak scaling is applicable when problem size increases with # processors.\n",
            " Adding more processors to solve a problem faster is possible through weak scaling, which aims to maintain a constant workload per processor as the problem size and number of processors increase. Flynn's classification is crucial in parallel computing as it categorizes architectures based on instruction & data streams, highlighting the importance of parallelism in modern computing.\n",
            " SISD and SIMD are two computing architectures. SISD has a single processor executing a single instruction stream on a single data stream, while SIMD has a single processor executing the same instruction on multiple data streams in parallel. Parallel strategies are techniques used to improve performance by executing multiple tasks or processes simultaneously. Data parallel approach involves dividing data into smaller chunks and processing them simultaneously.\n",
            " Parallel computing and distributed systems are used to accelerate computations and process large data sets. Popular parallel strategies include data parallelism, task parallelism, and main worker approach. These strategies can be applied in sentiment analysis to improve efficiency.\n",
            " Task parallelism is a technique where independent tasks are executed concurrently without dependencies. In a task parallelism setup, each request is treated as an independent task that can be processed by multiple threads or processes, allowing the server to handle multiple requests simultaneously. This technique is commonly used in applications where tasks can be performed concurrently without dependencies. In parallel computing, bucket brigade parallelism is a variant of task parallelism where tasks are broken down into smaller subtasks that can be executed concurrently by multiple processors or threads, allowing for increased processing speed and efficiency.\n",
            " In parallel computing, bucket brigade parallelism breaks down tasks into smaller subtasks for efficient parallel processing. Two metrics, parallel speedup and comparative speedup, evaluate performance improvement.\n",
            "    Parallel computing is a technique that allows multiple processors to execute a program simultaneously, resulting in faster execution times. The parallel speedup is calculated by dividing the time taken to execute a program sequentially by the time taken to execute it parallelly. As a developer, it is important to choose the appropriate programming language and parallel software interfaces to optimize parallelization in applications. There are various models and paradigms that define how tasks are divided, coordinated, and executed in parallel systems, including hardware models and software models.\n",
            " Distributed memory architecture is a parallel computing method where nodes have private memory, while cross node parallel methods involve distributing tasks across multiple nodes. Shared memory architecture is an on node parallel method where multiple processors or cores share a single memory space. On node parallelism refers to parallel processing techniques that occur on a single computing node.\n",
            "    Parallelism in shared memory architecture involves multiple threads/processes running concurrently on the same node, accessing shared memory for computations. Accelerator devices are special purpose processors that offload tasks from the CPU and provide parallel processing capabilities. The general heterogeneous parallel architecture model combines multiple hardware architectures to provide a more efficient and powerful system.\n",
            " A heterogeneous parallel architecture is described, consisting of multiple CPUs and GPUs connected by a network. Efficient parallelization of tasks is achieved through message passing between processes, with the programmer responsible for exposing the parallelization, determining the best technique, and directing its operation safely and efficiently. Message passing is the most common technique for parallelization.\n",
            " Parallelization techniques include process based parallelization, thread based parallelization, and vectorization. These techniques are used in various systems, including distributed memory systems, multi core processors, and SMP systems.\n",
            " Vectorization is a technique for parallel computing that utilizes SIMD capabilities of modern processors to perform multiple operations with a single instruction. This approach enables efficient processing of data streams by offloading data and compute kernels to GPU's streaming multiprocessors. Techniques for improving performance include deploying multiple compute pathways, separating program instances, and offloading calculations to GPUs.\n",
            " . Offloading calculations to GPUs.\n",
            "     . Discretizing problems into smaller cells or elements.\n",
            "     . Using threading to operate multiple processing cores simultaneously.\n",
            "     . Splitting work between processors on two desktops, often called nodes in parallel processing.\n",
            "     . Understanding performance limits and profiling parallel programs to optimize execution.\n",
            " The article emphasizes the significance of profiling results in enhancing parallel application efficiency, with a focus on speedup and overall performance. It highlights the importance of analyzing call graphs in parallel programming and the potential for optimization opportunities. Additionally, the article touches on the use of dual socket motherboards and parallel algorithms in high performance computing systems.\n",
            " The article emphasizes the significance of algorithm analysis in parallel computing environments, highlighting the need to compare various algorithms and factors affecting their performance. It introduces parallel algorithms, which are designed to solve computational problems efficiently by utilizing multiple processing units simultaneously. Common types of parallel algorithms include parallel merge sort, parallel quicksort, and parallel global sum. These algorithms aim to break down tasks into smaller subtasks that can be processed independently and concurrently, leading to a significant speedup in overall computation time.\n",
            " Parallel global sum is a problem in parallel or distributed computing where elements are concurrently sorted and summed across multiple processors or nodes, ensuring minimal word repetition and brevity in summarizing the given text.\n",
            " . The process occurs sequentially, with each step waiting for the previous one to finish.\n",
            "     . Gustafson's law takes into account varying problem sizes.\n",
            "     . The problem is discretized into smaller cells or elements.\n",
            "     . Calculation is offloaded to GPUs, where vector length is larger than on CPUs.\n",
            "     . Array is divided into smaller subarrays, and maximum values are found in parallel.\n",
            "     . Maximum of partial maxima is selected as maximum element of the entire array.\n",
            "     . Algorithmic complexity is a measure of number of operations in an algorithm.\n",
            " The article discusses the use of asymptotic notation to describe the complexity of algorithms. It explains the three main types of asymptotic notation: big O notation, theta notation, and omega notation. Big O notation provides an approximation of an algorithm's running time, while theta notation provides a tight bound on the time complexity, characterizing the average case behavior of an algorithm.\n",
            "θ and ω notations are used to analyze the efficiency of algorithms in terms of time and space requirements as a function of input size. θ provides an upper bound on an algorithm's time complexity, while ω describes the lower bound or best case time complexity. Both notations are used in asymptotic notations like big O to characterize how an algorithm's performance scales as the input size grows towards infinity.\n",
            " The article discusses the performance of algorithms as inputs grow towards infinity, using the example of sum of elements in an array. The algorithm has linear time complexity but can be improved with parallelization techniques. Prefix sum operations are a fundamental parallel pattern in computer science and parallel computing, and there are two types: exclusive and inclusive.\n",
            "    Two types of parallel scan operations are described: inclusive prefix sum and exclusive prefix sum. These operations are efficient and can be performed in parallel. They are used in various applications, such as data compression and signal processing. The text does not provide detailed explanations or examples of these operations, nor does it discuss their theoretical foundations.\n",
            "    WORD COUNT:  \n",
            "🔍 Efficient parallel scan uses   sweeps, starting with last value set to  , followed by tree based sweep for final result. However, issues arise due to non associativity in finite precision arithmetic, and the addition of the last value might not change the overall sum. As problem size increases, these issues worsen, and adding almost identical values of different signs can result in subtraction rather than addition.\n",
            "    The article discusses the issue of catastrophic cancellation in floating point arithmetic and presents several techniques for addressing it, including the use of long double data types, pairwise summation, Kahan summation, Knuth summation, and quad precision summation. The easiest solution is to use long double data types on x architecture. Additionally, the article discusses the use of pairwise summation as an alternative method for improving the accuracy of summation operations in scientific computing and numerical analysis.\n",
            " Kahan summation is a method used to reduce numerical error in floating point summation by breaking down the process into smaller parts and using a compensating strategy. It involves three main steps: generating a list of elements, pairwise addition, and summing the intermediate sums. This technique was introduced by William Kahan to improve accuracy in large scale summation operations.\n",
            "    Kahan summation is an algorithm that improves the accuracy of floating point arithmetic operations by breaking down the process into smaller parts and using a compensating strategy. It maintains a compensation term to keep track of lost precision due to rounding errors and adjusts the sum accordingly. Quad precision summation refers to performing arithmetic operations with quadruple precision floating point numbers, providing greater precision than single or double precision formats.\n",
            " Quadruple precision is a higher precision format for performing arithmetic operations, offering better accuracy than single and double precision.\n",
            " The text discusses the process of sorting chunks of data and estimating energy consumption for an application, while also mentioning Flynn's taxonomy, a classification system that helps understand different types of computer architectures and their strengths and weaknesses. The text also briefly mentions the simplest type of computer architecture used in most traditional computers.\n",
            " The text discusses various computer architectures, their advantages, and disadvantages. Traditional computers are suitable for image and signal processing but limited for complex tasks. MISD and MIMD architectures are also discussed, with MIMD being commonly used in applications where different pieces of data can be processed independently. The text provides an example of using MIMD in a data analysis task on a large dataset of customer reviews.\n",
            " The text describes the process of sentiment analysis, breaking down a large dataset into smaller subsets, processing each subset, and combining the results to obtain the overall sentiment analysis. The process involves using a sentiment analysis model to analyze each subset simultaneously, storing the results temporarily until all processing units have finished, and then aggregating the results to obtain a comprehensive analysis of the entire dataset. The text also provides examples of different tasks that can be performed within the process, such as parsing requests, querying databases, and generating responses.\n",
            " The sentiment analysis process is flexible and adaptable, allowing different tasks to perform various operations without necessarily relying on the same data. A manufacturing assembly line for producing smartphones consists of three stages, each with specific tasks performed by workers, such as assembling components, installing software, and quality control.\n",
            " A processing scenario with multiple stages (A, B, and C) similar to a bucket brigade parallelism, where a compiler translates source code into hardware executable form, an OS manages execution on computer hardware, nodes communicate and coordinate through messages, each node operates independently with local memory, and data sharing is explicit through message passing. Coordination between nodes is crucial as tasks often depend on results or data computed on other nodes.\n",
            " Accelerators are specialized hardware components that speed up specific tasks or computations within a system. They work by processing data or performing tasks on a subset of the data or a specific portion of the computation, and communication and coordination between nodes are essential. Accelerators can be accessed and modified by any processor within the system and are useful in scenarios where the same operation needs to be performed on a large set of data elements. Discrete GPUs have a large number of streaming multiprocessors and their own DRAM, and accessing data on a discrete GPU requires communication over a PCI bus.\n",
            "    Accelerator devices are specialized hardware components designed to perform specific computations efficiently. Two nodes, each with multiple CPUs and GPUs, share the same DRAM memory but are located in different NUMA regions. Communication between processes is achieved through message passing, where data and instructions are exchanged between processes to synchronize and share information.\n",
            " The OS controls process and thread placement during runtime, sharing memory between them. SIMD accelerates computations with large datasets. Stream processing continuously processes data as it is generated or ingested, handling large volumes of real time data.\n",
            " Stream processing is useful for handling large volumes of real time data from various sources, such as sensors, social media, financial transactions, and IoT devices. It involves processing data as it arrives, rather than batch processing. The output of stream processing is transferred back to the CPU for file I/O or other work. An example application is modeling a volcanic plume, tsunami, or early detection of a volcanic eruption using machine learning. The process involves solving partial differential equations (PDEs) using a discretization scheme, where the solution is represented as a mesh of cells.\n",
            " . Determine values for wave height, fluid velocity, or smoke density using physical laws.\n",
            "     . Define computational kernel or operation to conduct on each element of mesh.\n",
            "     . Vectorize data for faster computation.\n",
            " Processors with vector operations can operate on multiple pieces of data at once, while memory spaces for each node are distinct and separate when work is split across nodes. Limits of a system, including flops, ops, memory bandwidth, memory latency, instruction queue, networks, and disk, impact the capabilities and limitations of a system. The text discusses limitations of computer operations, particularly in terms of memory bandwidth and non contiguous memory access, and highlights the importance of feeds in providing necessary data for operations. The text also distinguishes between contiguous and non contiguous memory access.\n",
            "    The article discusses the importance of measuring hardware performance in high performance computing (HPC) and the need to understand the capabilities of the hardware. The authors explain the difference between contiguous and non contiguous memory access and the impact of non contiguous access on hardware performance. They also provide an overview of tools used to measure hardware performance, including lstopo and lscpu.\n",
            " The article explains how to measure hardware performance and calculate metrics, using technical terms and definitions. The `lscpu` command and `/proc/cpuinfo` file provide information on hardware components, and theoretical maximum FLOPs can be calculated by multiplying core, clock speed, and FLOPs per cycle per core. The summary provides a concise overview of the main points discussed in the text.\n",
            " Modern processors can perform multiple FLOPS per cycle due to features like SIMD operations. Theoretical FLOPS can be calculated using the number of cores, clock speed, and memory hierarchy. Memory bandwidth can be calculated using the memory transfer rate, memory channels, time per access, and sockets. Empirical measurements can be taken to validate theoretical calculations.\n",
            " The article discusses empirical measurements of bandwidth and FLOPs in single socket systems, with a focus on validating theoretical calculations through empirical measurements. The article highlights two methods for measuring bandwidth: the stream benchmark and the roofline model, with the former providing quantitative measurements and the latter providing a graphical representation. Key differences between the two methods are discussed, including the focus of the stream benchmark on memory bandwidth and the graphical representation provided by the roofline model.\n",
            " Stream and Roofline are performance evaluation methods used in computer architecture. Stream focuses on memory bandwidth, while Roofline provides a graphical representation of performance bottlenecks. Stream provides detailed insights into memory subsystem performance, while Roofline offers a high level overview of application performance efficiency. Profiling is necessary to understand an application's performance characteristics, and call graphs are used to analyze the calling relationships between functions/methods in a parallel program. Hot spots are identified by analyzing call graphs, which are functions/tasks that consume a significant amount of computational time.\n",
            " The text describes two approaches to measuring and optimizing the performance of a processor: using profiling tools and running benchmarking suites. Additionally, the text discusses how to measure energy consumption using power measurement tools and hardware devices.\n",
            " This text discusses two approaches to measuring and optimizing power consumption in a system: attaching power meters and sensors for real time measurement, and using profiling tools like Intel VTune Profiler, Valgrind's massif, and OpenMP/MPI for in depth analysis. These tools provide insights into energy consumption patterns and help identify memory leaks and usage patterns.\n",
            " The text discusses the integration of memory profiling features into MPI implementations, focusing on system performance and using adaptive mesh refinement in numerical simulations.\n",
            " In open addressing, various trial options exist for the next open slot. Data is transferred using grids with varying resolutions, and open addressing involves probing adjacent cells until an object is found or an empty cell is encountered. The process consists of two phases: the upsweep and downsweep.\n",
            " A bucket brigade is a method of transporting items by forming a line of people, similar to passing buckets of water to put out a fire. Hashing is a technique for storing and retrieving data using a hash function, which produces a fixed size hash value based on an input. The efficiency of hashing depends on the efficiency of the hash function used.\n",
            "    Input data is hashed to generate a unique value, known as the hash code or hash key, which is used as an index into a hash table. Collisions occur when two different inputs produce the same hash value. Hash tables store data in buckets based on their hash values, allowing for efficient lookups and insertions.\n",
            " A hash table is a data structure that stores data based on its hash value, with operations performed efficiently. The load factor and sparsity are important metrics for hash tables. Spatial hashing is a technique used to locate objects in a D space by dividing a large space into smaller grid like cells. Spatial data refers to any type of data that directly or indirectly references a specific geographical area or location.\n",
            " The text discusses grid based hashing, a technique used to map objects to cells in a regular pattern to minimize collisions. The author highlights the importance of efficient hash functions for hash tables and describes open addressing, a method that resolves collisions by finding an alternate storage location within the table. The text also covers other hashing methods and their applications in spatial data operations.\n",
            " The text discusses two techniques for sorting and remapping data in AMR meshes using perfect or compact hashing. These techniques are used for neighbor finding, table lookup, and sorting. The text provides a brief overview of the steps involved in each technique and highlights the importance of these operations in computational simulations.\n",
            "    Techniques such as spatial hashing and finite element analysis use hash functions to map data from one mesh to another. In CFD and FEA, spatial hashing maps data from a source mesh to a target mesh using a perfect hash function and a table lookup. Collision resolution is achieved through chaining or linked lists in each bucket. Sorting mesh data using a spatial perfect hash involves selecting a bucket size that guarantees no collisions, with a minimum value of. The hash sort operation is demonstrated with a minimum difference between values of.\n",
            " The hash sort operation minimizes the difference between values to ensure no collisions with a bucket size of. The minimum value is used to calculate the bucket location by dividing the key by the minimum difference. The hash table can store either the value or the index, with the first key stored in bucket.\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "chunk_size=1084, chunk_overlap=150\n",
        ")\n",
        "chunks = text_splitter.split_text(text)\n",
        "torch.cuda.empty_cache()\n",
        "print(len(chunks))\n",
        "\n",
        "chunk_summaries = []\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'# Set the max_split_size_mb environment variable\n",
        "i=0\n",
        "for chunk in chunks:\n",
        "    summary = generate_summary(chunk)\n",
        "    torch.cuda.empty_cache()\n",
        "    chunk_summaries.append(summary)\n",
        "    i=i+1\n",
        "    print(i)\n",
        "combined_summary = \"\\n\".join(chunk_summaries)\n",
        "text=clean(combined_summary)\n",
        "print(count_words(text))\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ztUx6aiWtRTh"
      },
      "outputs": [],
      "source": [
        "def generate_summary(text_chunk):\n",
        "    # Defining the template to generate summary\n",
        "    template = \"\"\"\n",
        "    Summarize text very concisely in bullet points, avoiding word repetition for clarity\n",
        "    ```{text}```\n",
        "    SUMMARY:\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "    summary = llm_chain.run(text_chunk)\n",
        "    torch.cuda.empty_cache()\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3O6AOXnugCr",
        "outputId": "511519c9-755a-4669-fcb4-953cc16be3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6939\n"
          ]
        }
      ],
      "source": [
        "text=clean(combined_cluster_summaries)\n",
        "# print(text)\n",
        "print(count_words(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIVGSncRxpMh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dNp39JUuvk4",
        "outputId": "3aa54402-44ca-489b-8790-22157b5456d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60\n",
            "3794\n",
            "   Computing refers to using computers and systems for various tasks\n",
            "       There are two types of computing: serial and parallel\n",
            "       Serial computing is traditional computing with sequential tasks using a single processor\n",
            "       Parallel computing involves executing multiple tasks simultaneously using multiple processors\n",
            "       Parallel computing can significantly improve processing speed and efficiency for complex tasks\n",
            "       Examples of serial computing include sorting a large dataset of numbers in ascending order\n",
            "       Examples of parallel computing involve using multiple processors to perform multiple tasks simultaneously\n",
            "      Parallel computing is a type of computation where multiple processors or computers work together to solve a problem.\n",
            "      It allows for simultaneous processing of smaller sub tasks, leading to significant improvements in computational speed and efficiency.\n",
            "      Examples include sorting a large dataset by dividing it into smaller chunks and sorting each chunk independently by a separate processor.\n",
            "      Parallel computing offers significant benefits, including increased processing power, faster execution times, and improved scalability.\n",
            "      However, there are also costs and limitations to consider, such as the need for specialized hardware and software, potential performance bottlenecks, and the challenges of managing and coordinating multiple processing units.\n",
            "      Software developers must carefully evaluate these factors to make informed decisions about how to best leverage parallel computing to achieve their goals.\n",
            "    \n",
            "      Parallel computing divides tasks into smaller sub tasks to be executed simultaneously\n",
            "      Utilizes multiple cores to process data, reducing time required for tasks\n",
            "      Aims to optimize energy efficiency by doing more with less\n",
            "      Uses dynamic resource allocation and workload consolidation for efficient use of processors```\n",
            "Please let me know if you need any further assistance.\n",
            "      Parallel computing improves computational performance by leveraging dynamic resource allocation and workload consolidation.\n",
            "      Parallel computing is more cost effective than serial computing by easily scaling with more processors.\n",
            "      Parallel computing has various applications, including scientific simulations and big data processing.\n",
            "      Laws like Amdahl's and Gustafson's provide insights into the limitations and possibilities of parallel processing, including speedup.\n",
            "      Speedup metric measures how much faster parallel algo/sys can complete task compared to serial impl.\n",
            "      Speedup = parallel execution time / serial execution time\n",
            "      Speedup value >   indicates parallel impl is faster than serial impl.\n",
            "      Doubling # processors would halve execution time, resulting in a speedup of  .\n",
            "      Achieving perfect linear speedup is rare in real world scenarios due to factors such as comm. overhead and load.\n",
            "      Parallel computing is limited by communication overhead, load balancing issues, and synchronization constraints between processors.\n",
            "      Amdahl's Law states that the speedup of a parallel program is limited by the sequential fraction of the program.\n",
            "      The maximum speedup achievable is calculated using Amdahl's law, which shows the speedup as a function of the number of processors.\n",
            "      Even with multiple processors, there is a maximum speedup that can be achieved due to the presence of sequential parts of the program.\n",
            "      The maximum speedup achievable is approximately times faster compared to sequential execution.\n",
            "      Limitations of parallel computing according to Amdahl's Law and Gustafson's Law\n",
            "      Amdahl's Law: speedup limited by serial code fraction\n",
            "      Gustafson's Law: impact of parallelizable code increases with problem size\n",
            "      Strong scaling and weak scaling evaluation methods for parallel computing systems\n",
            "\n",
            "\n",
            "\n",
            "      Strong scaling measures execution time decrease as # processors increase for a fixed problem size.\n",
            "      Weak scaling assesses parallel system performance when problem size increases with # processors.\n",
            "      Strong scaling aims to reduce execution time for fixed problem size by utilizing more processors.\n",
            "      Weak scaling is applicable when problem size increases with # processors.\n",
            "      Additional processors can be employed to solve a problem faster.\n",
            "      Weak scaling measures how well a parallel system can handle larger workloads.\n",
            "      The goal of weak scaling is to maintain a constant workload per processor as the problem size grows.\n",
            "      Flynn's classification is crucial in parallel computing, categorizing architectures based on instruction & data streams.\n",
            "      It aids in designing & analyzing parallel processing systems.\n",
            "      SISD and SIMD are two computing architectures\n",
            "      SISD has a single processor, single instruction stream, and single data stream\n",
            "      SIMD has a single processor, same instruction, and multiple data streams in parallel\n",
            "      Parallel strategies improve efficiency and performance\n",
            "      Techniques include data parallel approach, dividing data into smaller chunks and processing them simultaneously\n",
            "      Parallel computing and distributed systems are used to speed up computations and handle large volumes of data\n",
            "    \n",
            "      Common parallel strategies include data parallelism, task parallelism, and main worker approach\n",
            "    \n",
            "      Data parallelism involves performing the same operation on multiple data elements simultaneously\n",
            "    \n",
            "      Task parallelism involves executing multiple independent tasks or processes in parallel\n",
            "    \n",
            "      These strategies can be applied in sentiment analysis to speed up the computationally intensive process\n",
            "      Task parallelism: Multiple tasks executed concurrently without dependencies\n",
            "      Incoming requests treated as independent tasks\n",
            "      Multiple threads or processes handle requests without waiting for completion\n",
            "      Used in applications with independent tasks\n",
            "      Bucket brigade parallelism: Tasks broken down into smaller sub tasks for concurrent processing\n",
            "      Increased processing speed and efficiency\n",
            "      Bucket brigade parallelism: breaking down tasks into smaller subtasks for efficient parallel processing.\n",
            "      Parallel speedup: measures how much faster a parallel algorithm runs compared to its sequential counterpart.\n",
            "      Comparative speedup: compares the performance of parallel and sequential algorithms on the same hardware.\n",
            "      Parallel computing: multiple processors execute program simultaneously, faster execution times.\n",
            "      Parallel speedup: time taken to execute sequentially / time taken to execute parallelly.\n",
            "      Comparative speedup: performance comparison between parallel implementations or hardware sets.\n",
            "      Developers responsible for programming language and parallel software interfaces.\n",
            "      Parallel computing approaches: hardware models (distributed memory, shared memory, vector units), software models (process based, message passing, thread based).\n",
            "      Models allow developers to optimize parallelization in applications.\n",
            "   Distributed memory architecture: multiple nodes with private memory\n",
            "       Cross node parallel: tasks distributed across nodes\n",
            "       Shared memory architecture: multiple processors share a single memory space\n",
            "       On node parallelism: parallel processing on a single node\n",
            "\n",
            "\n",
            "   Parallelism in shared memory architecture refers to parallel processing techniques on a single node.\n",
            "       Multiple threads/processes run concurrently on the same node, accessing shared memory for computations.\n",
            "       Accelerator devices are special purpose add on processors (e.g. GPUs) that offload tasks from CPU and provide parallel processing capabilities.\n",
            "       General heterogeneous parallel architecture model combines multiple hardware architectures (e.g. CPUs, dual core processors, accelerators) to provide more efficient and powerful system.\n",
            "   General model for heterogeneous parallel architecture\n",
            "       Multiple CPUs and GPUs connected by a network\n",
            "       Efficient parallelization of tasks through message passing\n",
            "       Programmer must expose parallelization, determine best technique, and direct its operation\n",
            "       Most common techniques for parallelization are process based parallelization, particularly through message passing\n",
            "🔍 Parallelization techniques:\n",
            "          Process based parallelization: divide task into multiple processes on separate nodes/cores\n",
            "          Thread based parallelization: divide task into multiple threads within a single process\n",
            "          Vectorization: perform multiple ops with one instruction\n",
            "          Used in: distributed memory systems, multi core processors, SMP systems\n",
            "   Vectorization is a parallel computing technique that leverages SIMD capabilities of modern processors.\n",
            "       It enables efficient and timely processing of data streams by offloading data and compute kernels to GPU's streaming multiprocessors.\n",
            "       Additional layers of parallelization can be applied on CPUs and GPUs, including vectorization, threads, and processes.\n",
            "       These layers allow for working on multiple units of data simultaneously, deploying more compute pathways, and separating program instances into separate memory spaces.\n",
            "       Techniques for improving performance include deploying multiple compute pathways, separating program instances, offloading calculation to GPUs, and discretizing the problem.\n",
            " . Offloading calculations to GPUs.\n",
            "     . Discretizing the problem into smaller cells or elements.\n",
            "     . Using threading to operate multiple processing cores simultaneously.\n",
            "     . Splitting the work between processors on two desktops, often called nodes in parallel processing.\n",
            "     . Understanding performance limits and profiling parallel programs to optimize execution.\n",
            "     . Leveraging profiling tools to enhance efficiency and improve speedup.\n",
            "      Profiling results enhance parallel app efficiency\n",
            "       Call graphs provide insights into program structure, dependencies, & optimization opportunities\n",
            "       Dual socket motherboards offer more processing cores & memory bandwidth\n",
            "       Parallel algorithms & patterns take advantage of parallel processing capabilities\n",
            "       Examples of effective parallel algorithms include \"divide and conquer\"\n",
            "      Algorithm analysis is crucial in parallel computing applications\n",
            "      Comparing different algorithms that solve the same problem is important\n",
            "      Factors affecting performance include hardware differences, compiler optimizations, and memory optimization\n",
            "      Parallel algorithms are designed to solve computational problems efficiently by utilizing multiple processing units simultaneously\n",
            "      Common types of parallel algorithms include parallel merge sort, parallel quicksort, and parallel global sum\n",
            "      These algorithms aim to break down tasks into smaller subtasks that can be processed independently and concurrently, leading to significant speedup in overall computation time.\n",
            "   Parallel global sum is a problem in parallel or distributed computing.\n",
            "       Elements are computed across multiple processors or nodes.\n",
            "       Sorts partitions are used to ensure minimal word repetition and brevity.\n",
            " . Process occurs sequentially, waiting for previous steps to finish.\n",
            "     . Unlike Amdahl's law, Gustafson's law considers varying problem sizes.\n",
            "     . Problem is discretized into smaller cells or elements.\n",
            "     . Calculation is offloaded to GPUs, where vector length is larger than on CPUs.\n",
            "     . Array is divided into smaller subarrays, and maximum values are found in parallel.\n",
            "     . Maximum of partial maxima is selected as maximum element of entire array.\n",
            "     . Algorithmic complexity is a measure of number of operations in an algorithm.\n",
            "\n",
            "       Asymptotic notation provides an upper limit on the number of basic operations an algorithm performs\n",
            "       Big O notation provides an approximation of how an algorithm's running time increases as the input size grows\n",
            "       Theta notation provides a tight bound on an algorithm's time complexity, characterizing the average case behavior of an algorithm\n",
            "   θ notation provides a tight bound on an algorithm's time complexity\n",
            "       ω notation describes the lower bound or best case time complexity of an algorithm\n",
            "       Both θ and ω notations are used in asymptotic notations like big O, θ, and ω to analyze the efficiency of algorithms\n",
            "       Framework is useful for characterizing how an algorithm's performance scales as the input size grows towards infinity\n",
            "\n",
            "   Algorithms performance with input size growth\n",
            "       Linear time complexity and constant space complexity\n",
            "       Parallelization techniques for large inputs\n",
            "       Prefix sum operation and its types (exclusive and inclusive)\n",
            "       Example input array and output for both types of prefix sum operations\n",
            "      Inclusive prefix sum operation: calculates the sum of all elements in an array, including the last element.\n",
            "      Exclusive prefix sum operation: calculates the sum of all elements in an array, excluding the last element.\n",
            "      Both types of operations are efficient and can be performed in parallel.\n",
            "      Used in various applications, such as data compression and signal processing.\n",
            "      No detailed explanations or examples provided in the text.\n",
            "      No discussion of theoretical foundations, such as computational complexity or convergence properties of algorithms.\n",
            "\n",
            "   Efficient parallel scan uses two sweeps through arrays\n",
            "       Last value set to zero for first sweep\n",
            "       Tree based sweep for final result\n",
            "       Changing order of additions affects answer in finite precision arithmetic\n",
            "       Non associativity of addition\n",
            "       Problem size increases, issues become more severe\n",
            "       Addition of last value might not change overall sum\n",
            "       Worst case issue with finite precision values (almost identical but opposite signs)\n",
            "\n",
            "\n",
            "      Discusses catastrophic cancellation in arithmetic operations, particularly in finite precision values\n",
            "      Highlights worst case scenario where adding almost identical but different sign values can result in significant error\n",
            "      Presents several techniques for addressing this issue, including long double data types, pairwise summation, Kahan summation, Knuth summation, and quad precision summation\n",
            "      Long double data type on x architecture is the easiest solution\n",
            "      Discusses use of long double data type in x architecture for improving floating point arithmetic precision\n",
            "      Pairwise summation method is an alternative technique for improving accuracy in scientific computing and numerical analysis\n",
            "\n",
            "❶ Kahan summation is a method to reduce numerical error in floating point summation.\n",
            "    ❷ It works by pairing numbers, adding them together, and then summing the intermediate sums using the same pairwise summation method.\n",
            "    ❸ The process involves three main steps: generating a list of elements, pairwise addition, and summing the intermediate sums.\n",
            "    ❹ Kahan summation improves accuracy in large scale summation operations by breaking down the process into smaller, more manageable parts and using a compensating strategy.\n",
            "   Kahan summation improves accuracy of floating point summation operations by breaking down process into smaller parts and using compensating strategy.\n",
            "       Kahan summation maintains compensation term to keep track of lost precision due to rounding errors.\n",
            "       Algorithm initializes sum and compensation term to zero, then iteratively adds numbers to sum and calculates difference between updated sum and original sum, adding to compensation term.\n",
            "       Final result is adjusted sum, which takes into account lost precision.\n",
            "       Quad precision summation refers to performing arithmetic operations with numbers represented in quadruple precision floating point format, providing greater precision than single or double precision formats.\n",
            "   Quadruple precision refers to a higher precision format for floating point numbers\n",
            "       Offers higher precision than single and double precision formats\n",
            "       Used for applications where high accuracy is required\n",
            "\n",
            "\n",
            "      Sorting chunks of data\n",
            "      Estimating energy consumption for an application\n",
            "      Use of weather forecasting and financial modeling\n",
            "      Flynn's taxonomy (a classification system)\n",
            "      Different types of computer architectures\n",
            "      Traditional computers (simplest type)\n",
            "      Computer architectures\n",
            "      Advantages and disadvantages of each architecture\n",
            "      Traditional architecture suitable for image and signal processing\n",
            "      MISD architecture can improve performance but is not commonly used\n",
            "      MIMD architecture used in applications where the same operation can be applied to different pieces of data independently\n",
            "      Example of using MIMD architecture in data analysis task on large dataset of customer reviews\n",
            "   Sentiment analysis involves splitting a large dataset into smaller subsets and analyzing each subset independently.\n",
            "       Each processing unit analyzes its assigned subset of reviews simultaneously.\n",
            "       Once all processing units have finished, the results are aggregated to produce a comprehensive analysis.\n",
            "       Examples of tasks within the process include parsing requests, querying databases, and generating responses.\n",
            "   Sentiment analysis process can adapt to different tasks and data\n",
            "       Tasks in sentiment analysis may not operate on same data\n",
            "       Smartphone assembly process consists of   stages\n",
            "       Workers in each stage perform specific tasks\n",
            "\n",
            "      Processing scenario with multiple stages (A, B, and C)\n",
            "      Stages similar to bucket brigade parallelism\n",
            "      Compiler translates source code for hardware execution\n",
            "      OS manages execution on computer hardware\n",
            "      Nodes connected via network communicate through messages\n",
            "      Each node operates independently with local memory\n",
            "      Data sharing through explicit message passing\n",
            "      Nodes work on subsets of data or specific portions of computation\n",
            "      Coordination between nodes essential for task completion\n",
            "      Accelerators are specialized hardware components for specific tasks/computations within a system\n",
            "      They process data or perform tasks on a subset of data or specific portion of computation\n",
            "      Communication and coordination between nodes are essential\n",
            "      Accelerators can be accessed and modified by any processor within the system\n",
            "      Useful in scenarios where same operation needs to be performed on large set of data elements\n",
            "      Discrete GPUs have many streaming multiprocessors and their own DRAM\n",
            "      Accessing data on a discrete GPU requires communication over PCI bus\n",
            "      Accelerator device: specialized hardware for efficient computation\n",
            "      Shared DRAM memory between nodes, but in different NUMA regions\n",
            "      Multi core CPUs with integrated and discrete GPUs\n",
            "      Communication through message passing (data and instructions exchange)\n",
            "      Processes spawned and placed on cores of two nodes by message passing library and OS\n",
            "      OS controls placement of processes and threads during runtime\n",
            "      Shared memory between threads\n",
            "      SIMD allows single instruction to operate on multiple data elements\n",
            "      Stream processing continuously processes data as it is generated\n",
            "      Useful for handling large volumes of real time data\n",
            "   Stream processing is useful for handling large volumes of real time data.\n",
            "       It involves processing data as it arrives rather than batch processing.\n",
            "       It is particularly useful for handling data from sources such as sensors, social media, financial transactions, and IoT devices.\n",
            "       The output of stream processing is transferred back to the CPU for file I/O or other work.\n",
            "       An example application is modeling a volcanic plume, tsunami, or early detection of a volcanic eruption using machine learning.\n",
            "       Calculation speed is critical for real time results.\n",
            "       The properties of wave height, fluid velocity, or smoke density are solved for according to physical laws in each cell of the computational domain.\n",
            "       A stencil operation or a matrix vector system represents this discrete scheme.\n",
            "      In step  , physical laws are used to determine values at each cell.\n",
            "    \n",
            "      In step  , a computational kernel or operation is defined for each element.\n",
            "    \n",
            "      In step  , data is vectorized for faster computation.\n",
            "      Processors with vector operations can operate on multiple pieces of data at once.\n",
            "      Memory spaces for each node are distinct and separate when work is split across nodes.\n",
            "      Limits of a system, including flops, ops, memory bandwidth, memory latency, instruction queue, networks, and disk, impact system capabilities and limitations.\n",
            "      Memory bandwidth and non contiguous memory access have limitations in computer operations.\n",
            "      Feeds provide necessary data for operations, but can be limited by memory bandwidth.\n",
            "      Non contiguous memory access involves accessing elements not stored sequentially in memory.\n",
            "      Contiguous memory access: sequential memory access\n",
            "      Non contiguous memory access: scattered memory access with a stride\n",
            "      Hardware capabilities: measuring performance, floating point ops, data transfer, energy consumption\n",
            "      Metrics: bandwidth, energy usage\n",
            "      Tools: lstopo, lscpu (bundled with MPI distributions)\n",
            "\n",
            "   Measuring hardware performance\n",
            "       Calculating metrics\n",
            "       Technical terms defined\n",
            "       Article structure and ease of follow\n",
            "       Usefulness for readers\n",
            "       `lscpu` command and `/proc/cpuinfo` file\n",
            "       Theoretical maximum FLOPs calculation\n",
            "This summary uses bullet points to highlight the main points discussed in the text, while avoiding repetition and using concise language for clarity.\n",
            "      Modern processors can perform multiple FLOPS per cycle due to features like SIMD operations.\n",
            "      Theoretical FLOPS can be calculated using the number of cores, clock speed, and FLOPS per cycle per core.\n",
            "      Memory hierarchy and bandwidth can be calculated using memory chips' specifications.\n",
            "      Memory bandwidth can be calculated using the formula: Bandwidth (BT) = Memory Transfer Rate (MTR) × Memory Channels (MC) × Time per Access (TPA) × Sockets (S).\n",
            "      Processors are installed in a socket on the motherboard, and most motherboards are single socket.\n",
            "      Empirical measurements can be taken to validate theoretical calculations.\n",
            "      Empirical measurements of bandwidth and FLOPS can be taken to validate theoretical calculations\n",
            "      Empirical bandwidth refers to the fastest rate at which memory can be loaded into the processor\n",
            "      Memory latency refers to the time required for the first byte of data from each level of memory\n",
            "      Two methods are used to measure bandwidth: the stream benchmark and the roofline model\n",
            "      The stream benchmark provides quantitative measurements of bandwidth\n",
            "      The roofline model provides a graphical representation of bandwidth\n",
            "      Key differences between the two methods include the focus of the stream benchmark on memory bandwidth and the graphical representation provided by the roofline model\n",
            "      Stream and Roofline are performance evaluation methods in computer architecture.\n",
            "      Stream focuses on memory bandwidth, while Roofline offers a high level overview of performance efficiency.\n",
            "      Machine Balance metric compares performance of different systems.\n",
            "      Profiling is necessary to understand application performance.\n",
            "      Call graphs are used to analyze calling relationships in parallel programs.\n",
            "      Hot spots are identified by analyzing call graphs.\n",
            "   Hot spots are identified by analyzing call graphs, which are functions/tasks that consume a significant amount of computational time.\n",
            "       Empirical measurement of processor clock frequency and energy consumption can be used to evaluate performance.\n",
            "       Two approaches to measuring clock frequency: using profiling tools like Intel VTune Profiler or AMD CodeXL, running benchmarking suites like SPEC CPU Benchmarks or HPC Challenge Benchmarks.\n",
            "       Two approaches to measuring energy consumption: using power measurement tools and hardware devices, such as power meters and sensors, to measure real time power usage.\n",
            "   Measure power consumption using power meters, sensors, and tools like Intel Power Gadget or Linux's powerstat.\n",
            "       Use profiling tools like Intel VTune Profiler for insights into energy consumption patterns and memory usage in different parts of the code.\n",
            "       Valgrind's massif tool provides heap memory usage profiling over time, helping identify memory leaks and memory usage patterns.\n",
            "       Intel VTune Profiler offers memory analysis capabilities, including memory usage tracking at various levels and in both serial and parallel contexts.\n",
            "       OpenMP and MPI provide their own memory profiling tools, such as OpenMP's Score p and MPI's integrated memory profiling features. These tools consider various aspects of system performance, including algorithmic efficiency and memory usage patterns.\n",
            "   Memory profiling features integrated into MPI\n",
            "       System performance considered, including algorithmic efficiency and hardware/software factors\n",
            "       Adaptive mesh refinement (AMR) used in numerical simulations\n",
            "       Multiple levels of grids for improved resolution and efficiency\n",
            "       Various trial options for open slots in open addressing\n",
            "      Open addressing involves various trial options for the next open slot.\n",
            "      Grids are used in data transfer, with high resolution grids providing detailed information over smaller areas, while coarser grids cover larger areas with lower resolution.\n",
            "      Data transfer between grids involves open addressing, which involves probing adjacent cells until an object is found or an empty cell is encountered.\n",
            "      The process consists of two phases: the upsweep, which is a right sweep, and the downsweep, which is a left sweep.\n",
            "      The output of the upsweep is provided as input to the downsweep.\n",
            "      Bucket brigade: a method of transporting items by forming a line of people.\n",
            "      Hashing: a technique for quickly storing and retrieving data.\n",
            "      Hash function: a function that takes an input and produces a fixed size hash value.\n",
            "      Hash value: a fixed size output of a hash function that ensures the same input always produces the same output.\n",
            "      Input data: the data to be hashed.\n",
            "        Input data is hashed to produce a unique value.\n",
            "        Hash value is also known as hash code or hash key.\n",
            "        Collisions occur when two different inputs produce the same hash value.\n",
            "        Hash table stores data in buckets based on hash value.\n",
            "        Hash table is an associative array that maps input to values using hashing.\n",
            "      Hash table stores data based on hash value\n",
            "      Operations performed efficiently\n",
            "      Load factor affects performance\n",
            "      Sparsity ratio affects performance\n",
            "      Techniques for handling collisions include chaining and open addressing\n",
            "      Spatial hashing technique used for spatial data\n",
            "      Spatial data references a specific geographical area or location\n",
            "      Can be stored in vector or raster format\n",
            "      More than just a spatial component of a map\n",
            "\n",
            "   Grid based hashing for minimal collisions\n",
            "       Perfect and compact hashing\n",
            "       Importance of efficient hash functions\n",
            "       Open addressing for resolving collisions\n",
            "       Linear probing, quadratic probing, and double hashing\n",
            "       Spatial data operations: neighbor finding, remapping, table lookup, and sorting\n",
            "   Spatial data operations: neighbor finding, remapping, table lookup, and sorting.\n",
            "       Implemented using perfect or compact hashing.\n",
            "       Neighbor finding using spatial perfect hash.\n",
            "       Sorting: A D or D sort of the cell data can be implemented using perfect or compact hashing.\n",
            "       Remapping: Mapping another AMR mesh onto a current AMR mesh using a perfect hash.\n",
            "       Steps involved: neighbor finding, remapping, table lookup, and sorting.\n",
            "\n",
            "   Spatial hashing is used in CFD and FEA to map data from a source mesh to a target mesh.\n",
            "       A perfect hash function is used for table lookup.\n",
            "       Chaining (linked lists in each bucket) is used for collision resolution.\n",
            "       Bucket size must be chosen to avoid collisions.\n",
            "       Minimum difference between values is demonstrated.\n",
            "\n",
            "      Minimize difference between values to ensure no collisions\n",
            "      Ensure no collisions with a bucket size of\n",
            "      Calculate bucket location by dividing key by minimum difference\n",
            "      Store value or index in hash table\n",
            "      First key stored in bucket\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "chunk_size=1084, chunk_overlap=150\n",
        ")\n",
        "chunks = text_splitter.split_text(text)\n",
        "torch.cuda.empty_cache()\n",
        "print(len(chunks))\n",
        "\n",
        "chunk_summaries = []\n",
        "torch.cuda.empty_cache()\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'# Set the max_split_size_mb environment variable\n",
        "i=0\n",
        "for chunk in chunks:\n",
        "    summary = generate_summary(chunk)\n",
        "    torch.cuda.empty_cache()\n",
        "    chunk_summaries.append(summary)\n",
        "    i=i+1\n",
        "    print(i)\n",
        "combined_summary = \"\\n\".join(chunk_summaries)\n",
        "text=clean(combined_summary)\n",
        "print(count_words(text))\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKioQnU8yg6I",
        "outputId": "7a9f9d1f-a2a4-4f6d-8a27-f8d1101c00fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3794\n",
            "33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33\n",
            "3160\n",
            "   Computing refers to using computers and systems for various tasks\n",
            "       There are two types of computing: serial and parallel\n",
            "       Serial computing is traditional computing with sequential tasks using a single processor\n",
            "       Parallel computing involves executing multiple tasks simultaneously using multiple processors\n",
            "       Parallel computing can significantly improve processing speed and efficiency for complex tasks\n",
            "       Examples of serial computing include sorting a large dataset of numbers in ascending order\n",
            "       Examples of parallel computing involve using multiple processors to perform multiple tasks simultaneously\n",
            "       Parallel computing is a type of computation where multiple processors or computers work together to solve a problem.\n",
            "       It allows for simultaneous processing of smaller sub tasks, leading to significant improvements in computational speed and efficiency.\n",
            "      Sorting large datasets by dividing them into smaller chunks and sorting each chunk independently\n",
            "      Increased processing power, faster execution times, and improved scalability\n",
            "      Specialized hardware and software, performance bottlenecks, and managing multiple processing units\n",
            "      Divides tasks into smaller sub tasks to be executed simultaneously\n",
            "      Utilizes multiple cores to process data, reducing time required for tasks\n",
            "      Aims to optimize energy efficiency by doing more with less\n",
            "      Uses dynamic resource allocation and workload consolidation for efficient use of processors\n",
            "```\n",
            "I hope this helps! Let me know if you have any questions or need further assistance.\n",
            "      Parallel computing improves computational performance by leveraging dynamic resource allocation and workload consolidation.\n",
            "      Parallel computing is more cost effective than serial computing by easily scaling with more processors.\n",
            "      Parallel computing has various applications, including scientific simulations and big data processing.\n",
            "      Laws like Amdahl's and Gustafson's provide insights into the limitations and possibilities of parallel processing, including speedup.\n",
            "      Speedup metric measures how much faster parallel algorithm/system can complete a task compared to serial implementation.\n",
            "      Speedup value >   indicates parallel implementation is faster than serial implementation.\n",
            "      Doubling number of processors would halve execution time, resulting in a speedup of .\n",
            "      Achieving perfect linear speedup is rare in real world scenarios due to factors such as communication overhead and load.\n",
            "   Achieving perfect linear speedup is rare in real world scenarios due to factors such as communication overhead and load.\n",
            "       Parallel computing is limited by sequential fraction of the program, Amdahl's Law, and synchronization constraints between processors.\n",
            "       Maximum speedup achievable is calculated using Amdahl's law and shows speedup as a function of number of processors.\n",
            "       Even with multiple processors, there is a maximum speedup that can be achieved due to sequential parts of the program.\n",
            "       Maximum speedup achievable is approximately times faster compared to sequential execution.\n",
            "       Limitations of parallel computing according to Amdahl's Law and Gustafson's Law.\n",
            "      Strong scaling measures execution time decrease as # processors increase for a fixed problem size\n",
            "      Weak scaling assesses parallel system performance when problem size increases with # processors\n",
            "      Strong scaling aims to reduce execution time for fixed problem size by utilizing more processors\n",
            "      Weak scaling is applicable when problem size increases with # processors\n",
            "      Additional processors can be employed to solve a problem faster\n",
            "      Weak scaling measures how well a parallel system can handle larger workloads\n",
            "      The goal of weak scaling is to maintain a constant workload per processor as the problem size grows\n",
            "      Flynn's classification is crucial in parallel computing, categorizing architectures based on instruction & data streams\n",
            "      SISD and SIMD are two computing architectures\n",
            "      SISD has a single processor, single instruction stream, and single data stream\n",
            "      SIMD has a single processor, same instruction, and multiple data streams in parallel\n",
            "      SIMD has a single processor, same instruction, and multiple data streams in parallel\n",
            "      Parallel strategies improve efficiency and performance\n",
            "      Techniques include data parallel approach, dividing data into smaller chunks and processing them simultaneously\n",
            "      Parallel computing and distributed systems are used to speed up computations and handle large volumes of data\n",
            "      Common parallel strategies include data parallelism, task parallelism, and main worker approach\n",
            "      These strategies can be applied in sentiment analysis to speed up the computationally intensive process\n",
            "      Task parallelism: Multiple tasks executed concurrently without dependencies\n",
            "      Incoming requests treated as independent tasks\n",
            "      Multiple threads or processes handle requests without waiting for completion\n",
            "      Multiple threads or processes handle requests without waiting for completion\n",
            "      Used in applications with independent tasks\n",
            "      Bucket brigade parallelism: breaking down tasks into smaller subtasks for efficient parallel processing\n",
            "      Increased processing speed and efficiency\n",
            "      Parallel speedup: measures how much faster a parallel algorithm runs compared to its sequential counterpart\n",
            "      Comparative speedup: compares the performance of parallel and sequential algorithms on the same hardware\n",
            "      Parallel computing: multiple processors execute program simultaneously, faster execution times\n",
            "      Parallel speedup: time taken to execute sequentially / time taken to execute parallelly\n",
            "      Comparative speedup: performance comparison between parallel implementations or hardware sets\n",
            "      Developers responsible for programming language and parallel software interfaces.\n",
            "   Developers responsible for programming language and parallel software interfaces.\n",
            "       Parallel computing approaches: hardware models (distributed memory, shared memory, vector units), software models (process based, message passing, thread based).\n",
            "       Models allow developers to optimize parallelization in applications.\n",
            "       Distributed memory architecture: multiple nodes with private memory.\n",
            "       Cross node parallel: tasks distributed across nodes.\n",
            "       Shared memory architecture: multiple processors share a single memory space.\n",
            "       On node parallelism: parallel processing on a single node.\n",
            "   Parallelism in shared memory architecture refers to parallel processing techniques on a single node.\n",
            "       Multiple threads/processes run concurrently on the same node, accessing shared memory for computations.\n",
            "       Accelerator devices are special purpose add on processors (e.g. GPUs) that offload tasks from CPU and provide parallel processing capabilities.\n",
            "       General heterogeneous parallel architecture model combines multiple hardware architectures (e.g. CPUs, dual core processors, accelerators) to provide more efficient and powerful system.\n",
            "       Most common techniques for parallelization are process based parallelization, particularly through message passing.\n",
            "\n",
            "🔍 Parallelization techniques:\n",
            "    🔍 Process based parallelization: divide task into multiple processes on separate nodes/cores\n",
            "    🔍 Thread based parallelization: divide task into multiple threads within a single process\n",
            "    🔍 Vectorization: perform multiple ops with one instruction\n",
            "    🔍 Used in: distributed memory systems, multi core processors, SMP systems\n",
            "    🔍 Vectorization is a parallel computing technique that leverages SIMD capabilities of modern processors.\n",
            "    🔍 Enables efficient and timely processing of data streams by offloading data and compute kernels to GPU's streaming multiprocessors.\n",
            "    🔍 Additional layers of parallelization can be applied on CPUs and GPUs, including vectorization, threads, and processes.\n",
            "    🔍 Allows for working on multiple units of data simultaneously, deploying more compute pathways, and separating program instances into separate memory spaces.\n",
            "   Deploy multiple compute pathways\n",
            "       Separate program instances\n",
            "       Offload calculations to GPUs\n",
            "       Discretize the problem\n",
            "       Use threading to operate multiple processing cores simultaneously\n",
            "       Split the work between processors on two desktops\n",
            "       Understand performance limits and profile parallel programs\n",
            "       Leverage profiling tools to enhance efficiency and improve speedup\n",
            "       Use call graphs to gain insights into program structure, dependencies, and optimization opportunities\n",
            "       Utilize dual socket motherboards for more processing cores and memory bandwidth\n",
            "       Employ effective parallel algorithms such as \"divide and conquer\"\n",
            "\n",
            "\n",
            "     . Divide and conquer algorithms are effective in parallel computing.\n",
            "     . Algorithm analysis is crucial for parallel computing applications.\n",
            "     . Comparing different algorithms that solve the same problem is important.\n",
            "     . Factors affecting performance include hardware differences, compiler optimizations, and memory optimization.\n",
            "     . Parallel algorithms are designed to solve computational problems efficiently by utilizing multiple processing units simultaneously.\n",
            "     . Common types of parallel algorithms include parallel merge sort, parallel quicksort, and parallel global sum.\n",
            "     . These algorithms aim to break down tasks into smaller subtasks that can be processed independently and concurrently, leading to significant speedup in overall computation time.\n",
            "     . Parallel global sum is a problem in parallel or distributed computing.\n",
            "     . Elements are computed across multiple processors or nodes.\n",
            "     . Sorts partitions are used to ensure minimal word repetition and brevity.\n",
            "     . Process occurs sequentially, waiting for previous steps to finish.\n",
            "\n",
            "   Sorts partitions ensure minimal word repetition and brevity.\n",
            "       Process occurs sequentially, waiting for previous steps to finish.\n",
            "       Problem discretization into smaller cells/elements.\n",
            "       Calculation offloaded to GPUs, where vector length is larger than on CPUs.\n",
            "       Array division into smaller subarrays, maximum values found in parallel.\n",
            "       Maximum of partial maxima selected as maximum element of entire array.\n",
            "       Algorithmic complexity measures number of operations in an algorithm.\n",
            "      Asymptotic notation provides an upper limit on the number of basic operations an algorithm performs\n",
            "      Big O notation provides an approximation of how an algorithm's running time increases as the input size grows\n",
            "      Theta notation provides a tight bound on an algorithm's time complexity, characterizing the average case behavior of an algorithm\n",
            "      θ notation provides a tight bound on an algorithm's time complexity\n",
            "      ω notation describes the lower bound or best case time complexity of an algorithm\n",
            "      Both θ and ω notations are used in asymptotic notations like big O, θ, and ω to analyze the efficiency of algorithms\n",
            "      Framework is useful for characterizing how an algorithm's performance scales as the input size grows towards infinity\n",
            "   Linear time complexity and constant space complexity\n",
            "       Parallelization techniques for large inputs\n",
            "       Prefix sum operation and its types (inclusive and exclusive)\n",
            "       Efficient and can be performed in parallel\n",
            "       Used in various applications\n",
            "       No detailed explanations or examples provided\n",
            "       No discussion of theoretical foundations\n",
            "\n",
            "\n",
            "      Efficient parallel scan uses   sweeps through arrays\n",
            "    \n",
            "      Last value set to zero for first sweep\n",
            "    \n",
            "      Tree based sweep for final result\n",
            "    \n",
            "      Changing order of additions affects answer in finite precision arithmetic\n",
            "    \n",
            "      Non associativity of addition\n",
            "    \n",
            "      Problem size increases, issues become more severe\n",
            "    \n",
            "      Addition of last value might not change overall sum\n",
            "    \n",
            "      Worst case issue with finite precision values (almost identical but opposite signs)\n",
            "\n",
            "\n",
            "   Catastrophic cancellation in arithmetic operations\n",
            "       Worst case scenario with almost identical but different sign values\n",
            "       Techniques for addressing this issue: long double data types, pairwise summation, Kahan summation, Knuth summation, quad precision summation\n",
            "       Long double data type on x architecture is easiest solution\n",
            "       Use of long double data type in x architecture for improving floating point arithmetic precision\n",
            "       Pairwise summation method as alternative technique for improving accuracy in scientific computing and numerical analysis\n",
            "\n",
            "\n",
            "      Kahan summation is a method to reduce numerical error in floating point summation.\n",
            "    \n",
            "      It works by pairing numbers, adding them together, and then summing the intermediate sums using the same pairwise summation method.\n",
            "    \n",
            "      The process involves three main steps: generating a list of elements, pairwise addition, and summing the intermediate sums.\n",
            "    \n",
            "      Kahan summation improves accuracy in large scale summation operations by breaking down the process into smaller, more manageable parts and using a compensating strategy.\n",
            "    \n",
            "      Kahan summation maintains a compensation term to keep track of lost precision due to rounding errors.\n",
            "    \n",
            "      The algorithm initializes the sum and compensation term to zero, then iteratively adds numbers to the sum and calculates the difference between the updated sum and the original sum, adding to the compensation term.\n",
            "    \n",
            "      The final result is the adjusted sum, which takes into account lost precision.\n",
            "   Final result is adjusted sum, taking into account lost precision\n",
            "       Quad precision summation refers to arithmetic operations with quadruple precision floating point numbers\n",
            "       Quadruple precision offers higher precision than single and double precision formats\n",
            "       Used for applications where high accuracy is required\n",
            "\n",
            "   Sorting chunks of data\n",
            "       Estimating energy consumption for an application\n",
            "       Use of weather forecasting and financial modeling\n",
            "       Flynn's taxonomy (a classification system)\n",
            "       Different types of computer architectures\n",
            "       Traditional computers (simplest type)\n",
            "       Computer architectures\n",
            "       Advantages and disadvantages of each architecture\n",
            "       Traditional architecture suitable for image and signal processing\n",
            "       MISD architecture can improve performance but is not commonly used\n",
            "       MIMD architecture used in applications where the same operation can be applied to different pieces of data independently\n",
            "       Example of using MIMD architecture in data analysis task on large dataset of customer reviews\n",
            "       Sentiment analysis involves splitting a large dataset into smaller subsets and analyzing each subset independently\n",
            "       Each processing unit analyzes its assigned subset of reviews simultaneously\n",
            "       Once all processing units have finished, the results are aggregated to produce a comprehensive analysis.\n",
            "   After processing units finish, results are combined for a complete analysis.\n",
            "       Examples of tasks in the process include parsing requests, querying databases, and generating responses.\n",
            "       The sentiment analysis process can adapt to different tasks and data.\n",
            "       Tasks in sentiment analysis may not operate on the same data.\n",
            "       The smartphone assembly process consists of several stages, with workers performing specific tasks in each stage.\n",
            "   Scenario with multiple stages (A, B, and C) similar to bucket brigade parallelism\n",
            "       Compiler translates source code for hardware execution\n",
            "       OS manages execution on computer hardware\n",
            "       Nodes connected via network communicate through messages\n",
            "       Each node operates independently with local memory\n",
            "       Data sharing through explicit message passing\n",
            "       Nodes work on subsets of data or specific portions of computation\n",
            "       Coordination between nodes essential for task completion\n",
            "       Accelerators are specialized hardware components for specific tasks/computations within a system\n",
            "       They process data or perform tasks on a subset of data or specific portion of computation\n",
            "       Communication and coordination between nodes are essential\n",
            "       Accelerators can be accessed and modified by any processor within the system\n",
            "       Useful in scenarios where same operation needs to be performed on large set of data elements\n",
            "       Discrete GPUs have many streaming multiprocessors and their own DRAM\n",
            "\n",
            "   Discrete GPUs have many streaming multiprocessors and DRAM\n",
            "       Accessing data on a discrete GPU requires communication over PCI bus\n",
            "       Accelerator device: specialized hardware for efficient computation\n",
            "       Shared DRAM memory between nodes, but in different NUMA regions\n",
            "       Multi core CPUs with integrated and discrete GPUs\n",
            "       Communication through message passing (data and instructions exchange)\n",
            "       Processes spawned and placed on cores of two nodes by message passing library and OS\n",
            "       OS controls placement of processes and threads during runtime\n",
            "       Shared memory between threads\n",
            "       SIMD allows single instruction to operate on multiple data elements\n",
            "       Stream processing continuously processes data as it is generated\n",
            "       Useful for handling large volumes of real time data\n",
            "\n",
            "\n",
            "      Stream processing involves processing data as it arrives, rather than in batches.\n",
            "    \n",
            "      Useful for handling data from sources like sensors, social media, financial transactions, and IoT devices.\n",
            "    \n",
            "      Output is transferred back to CPU for file I/O or other work.\n",
            "    \n",
            "      Examples include modeling volcanic plumes, tsunamis, and early detection of volcanic eruptions using machine learning.\n",
            "    \n",
            "      Calculation speed is critical for real time results.\n",
            "    \n",
            "      Discrete scheme represents physical laws using stencil operations or matrix vector systems.\n",
            "    \n",
            "      In step, physical laws are used to determine values at each cell.\n",
            "    \n",
            "      In step, a computational kernel or operation is defined for each element.\n",
            "    \n",
            "      In step, data is vectorized for faster computation.\n",
            "    \n",
            "      Processors with vector operations can operate on multiple pieces of data at once.\n",
            "   Data is vectorized for faster computation.\n",
            "       Processors can operate on multiple pieces of data at once.\n",
            "       Memory spaces are distinct and separate when work is split across nodes.\n",
            "       Limits of a system impact system capabilities and limitations.\n",
            "       Memory bandwidth and non contiguous memory access have limitations.\n",
            "       Feeds provide necessary data for operations, but can be limited by memory bandwidth.\n",
            "       Non contiguous memory access involves accessing elements not stored sequentially in memory.\n",
            "       Contiguous memory access: sequential memory access.\n",
            "       Non contiguous memory access: scattered memory access with a stride.\n",
            "       Hardware capabilities can be measured in terms of floating point ops, data transfer, and energy consumption.\n",
            "       Metrics can include bandwidth and energy usage.\n",
            "       Tools such as lstopo and lscpu can be used to measure and analyze system performance.\n",
            "   Calculating metrics\n",
            "       Technical terms defined\n",
            "       Article structure and ease of follow\n",
            "       Usefulness for readers\n",
            "       `lscpu` command and `/proc/cpuinfo` file\n",
            "       Theoretical maximum FLOPs calculation\n",
            "\n",
            "\n",
            "   Empirical measurements can validate theoretical calculations.\n",
            "       Empirical bandwidth refers to the fastest rate at which memory can be loaded into the processor.\n",
            "       Memory latency refers to the time required for the first byte of data from each level of memory.\n",
            "       Two methods are used to measure bandwidth: the stream benchmark and the roofline model.\n",
            "       Stream provides quantitative measurements of bandwidth, while Roofline offers a graphical representation.\n",
            "       Stream focuses on memory bandwidth, while Roofline offers a high level overview of performance efficiency.\n",
            "       Machine Balance metric compares performance of different systems.\n",
            "       Machine Balance metric compares performance of different systems.\n",
            "       Profiling is necessary to understand application performance.\n",
            "       Call graphs are used to analyze calling relationships in parallel programs.\n",
            "       Hot spots are identified by analyzing call graphs.\n",
            "       Hot spots are functions/tasks that consume a significant amount of computational time.\n",
            "       Performance can be evaluated using empirical measurement of processor clock frequency and energy consumption.\n",
            "       Two approaches to measuring clock frequency: using profiling tools or running benchmarking suites.\n",
            "       Two approaches to measuring energy consumption: using power measurement tools and hardware devices or using software tools like Intel Power Gadget or Linux's powerstat.\n",
            "   Measure power consumption using various tools and methods.\n",
            "       Use profiling tools for insights into energy consumption patterns and memory usage.\n",
            "       Utilize memory profiling features integrated into MPI and OpenMP.\n",
            "       Consider system performance, including algorithmic efficiency and hardware/software factors.\n",
            "       Use adaptive mesh refinement (AMR) in numerical simulations.\n",
            "      AMR is used in numerical simulations for improved resolution and efficiency\n",
            "       Open addressing involves various trial options for open slots\n",
            "       Grids are used in data transfer, with high resolution grids providing detailed information over smaller areas\n",
            "       Data transfer between grids involves open addressing\n",
            "       The process consists of two phases: the upsweep and the downsweep\n",
            "       Bucket brigade is a method of transporting items by forming a line of people\n",
            "       Hashing is a technique for quickly storing and retrieving data\n",
            "       Hash function is a function that takes an input and produces a fixed size hash value\n",
            "      A hash function is a function that takes an input and produces a fixed size hash value.\n",
            "    \n",
            "      The hash value is a unique output that ensures the same input always produces the same output.\n",
            "    \n",
            "      Input data is hashed to produce a unique value.\n",
            "    \n",
            "      The hash value is also known as a hash code or hash key.\n",
            "    \n",
            "      Collisions occur when two different inputs produce the same hash value.\n",
            "    \n",
            "      A hash table stores data in buckets based on the hash value.\n",
            "    \n",
            "      A hash table is an associative array that maps input to values using hashing.\n",
            "    \n",
            "      Operations performed efficiently.\n",
            "    \n",
            "      Load factor affects performance.\n",
            "    \n",
            "      Sparsity ratio affects performance.\n",
            "    \n",
            "      Techniques for handling collisions include chaining and open addressing.\n",
            "    \n",
            "      Spatial hashing technique used for spatial data.\n",
            "    \n",
            "      Spatial data references a specific geographical area or location.\n",
            "    \n",
            "      Can be stored in vector or raster format.\n",
            "    \n",
            "      More than just a spatial component of a map.\n",
            " . Grid based hashing for minimal collisions\n",
            "     . Perfect and compact hashing\n",
            "     . Importance of efficient hash functions\n",
            "     . Open addressing for resolving collisions\n",
            "     . Linear probing, quadratic probing, and double hashing\n",
            "     . Spatial data operations: neighbor finding, remapping, table lookup, and sorting\n",
            "     . Implemented using perfect or compact hashing\n",
            "     . Neighbor finding using spatial perfect hash\n",
            "     . Sorting: A D or D sort of the cell data can be implemented using perfect or compact hashing\n",
            "     . Remapping: Mapping another AMR mesh onto a current AMR mesh using a perfect hash\n",
            "     . Steps involved: neighbor finding, remapping, table lookup, and sorting\n",
            "\n",
            "   Spatial hashing is used in CFD and FEA to map data from a source mesh to a target mesh.\n",
            "       A perfect hash function is used for table lookup.\n",
            "       Chaining (linked lists in each bucket) is used for collision resolution.\n",
            "       Bucket size must be chosen to avoid collisions.\n",
            "       Minimum difference between values is demonstrated.\n",
            "       Bucket location is calculated by dividing key by minimum difference.\n",
            "       Values or indices are stored in the hash table.\n",
            "       The first key stored in a bucket is used.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "\n",
        "print(count_words(text))\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "chunk_size=1084, chunk_overlap=150\n",
        ")\n",
        "chunks = text_splitter.split_text(text)\n",
        "torch.cuda.empty_cache()\n",
        "print(len(chunks))\n",
        "\n",
        "chunk_summaries = []\n",
        "torch.cuda.empty_cacadhe()\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'# Set the max_split_size_mb environment variable\n",
        "i=0\n",
        "for chunk in chunks:\n",
        "    summary = generate_summary(chunk)\n",
        "    torch.cuda.empty_cache()\n",
        "    chunk_summaries.append(summary)\n",
        "    i=i+1\n",
        "    print(i)\n",
        "combined_summary = \"\\n\".join(chunk_summaries)\n",
        "text=clean(combined_summary)\n",
        "print(count_words(text))\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkzpjt3F0rkP",
        "outputId": "fdc92794-a987-4f2e-e874-f988c7f3c043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " - Computing refers to using computers and systems for various tasks\n",
            "     - There are two types of computing: serial and parallel\n",
            "     - Serial computing is traditional computing with sequential tasks using a single processor\n",
            "     - Parallel computing involves executing multiple tasks simultaneously using multiple processors\n",
            "     - Parallel computing can significantly improve processing speed and efficiency for complex tasks\n",
            "     - Examples of serial computing include sorting a large dataset of numbers in ascending order\n",
            "     - Examples of parallel computing involve using multiple processors to perform multiple tasks simultaneously\n",
            "     - Parallel computing is a type of computation where multiple processors or computers work together to solve a problem.\n",
            "     - It allows for simultaneous processing of smaller sub tasks, leading to significant improvements in computational speed and efficiency.\n",
            "\n",
            "    - Sorting large datasets by dividing them into smaller chunks and sorting each chunk independently\n",
            "    - Increased processing power, faster execution times, and improved scalability\n",
            "    - Specialized hardware and software, performance bottlenecks, and managing multiple processing units\n",
            "    - Divides tasks into smaller sub tasks to be executed simultaneously\n",
            "    - Utilizes multiple cores to process data, reducing time required for tasks\n",
            "    - Aims to optimize energy efficiency by doing more with less\n",
            "    - Uses dynamic resource allocation and workload consolidation for efficient use of processors\n",
            "```\n",
            "I hope this helps! Let me know if you have any questions or need further assistance.\n",
            "\n",
            "    • Parallel computing improves computational performance by leveraging dynamic resource allocation and workload consolidation.\n",
            "    • Parallel computing is more cost-effective than serial computing by easily scaling with more processors.\n",
            "    • Parallel computing has various applications, including scientific simulations and big data processing.\n",
            "    • Laws like Amdahl's and Gustafson's provide insights into the limitations and possibilities of parallel processing, including speedup.\n",
            "    • Speedup metric measures how much faster parallel algorithm/system can complete a task compared to serial implementation.\n",
            "    • Speedup value > 1 indicates parallel implementation is faster than serial implementation.\n",
            "    • Doubling number of processors would halve execution time, resulting in a speedup of .\n",
            "    • Achieving perfect linear speedup is rare in real-world scenarios due to factors such as communication overhead and load.\n",
            " • Achieving perfect linear speedup is rare in real-world scenarios due to factors such as communication overhead and load.\n",
            "     • Parallel computing is limited by sequential fraction of the program, Amdahl's Law, and synchronization constraints between processors.\n",
            "     • Maximum speedup achievable is calculated using Amdahl's law and shows speedup as a function of number of processors.\n",
            "     • Even with multiple processors, there is a maximum speedup that can be achieved due to sequential parts of the program.\n",
            "     • Maximum speedup achievable is approximately times faster compared to sequential execution.\n",
            "     • Limitations of parallel computing according to Amdahl's Law and Gustafson's Law.\n",
            "\n",
            "    - Strong scaling measures execution time decrease as # processors increase for a fixed problem size\n",
            "    - Weak scaling assesses parallel system performance when problem size increases with # processors\n",
            "    - Strong scaling aims to reduce execution time for fixed problem size by utilizing more processors\n",
            "    - Weak scaling is applicable when problem size increases with # processors\n",
            "    - Additional processors can be employed to solve a problem faster\n",
            "    - Weak scaling measures how well a parallel system can handle larger workloads\n",
            "    - The goal of weak scaling is to maintain a constant workload per processor as the problem size grows\n",
            "    - Flynn's classification is crucial in parallel computing, categorizing architectures based on instruction & data streams\n",
            "    - SISD and SIMD are two computing architectures\n",
            "    - SISD has a single processor, single instruction stream, and single data stream\n",
            "    - SIMD has a single processor, same instruction, and multiple data streams in parallel\n",
            "\n",
            "    * SIMD has a single processor, same instruction, and multiple data streams in parallel\n",
            "    * Parallel strategies improve efficiency and performance\n",
            "    * Techniques include data parallel approach, dividing data into smaller chunks and processing them simultaneously\n",
            "    * Parallel computing and distributed systems are used to speed up computations and handle large volumes of data\n",
            "    * Common parallel strategies include data parallelism, task parallelism, and main worker approach\n",
            "    * These strategies can be applied in sentiment analysis to speed up the computationally intensive process\n",
            "    * Task parallelism: Multiple tasks executed concurrently without dependencies\n",
            "    * Incoming requests treated as independent tasks\n",
            "    * Multiple threads or processes handle requests without waiting for completion\n",
            "\n",
            "    • Multiple threads or processes handle requests without waiting for completion\n",
            "    • Used in applications with independent tasks\n",
            "    • Bucket brigade parallelism: breaking down tasks into smaller subtasks for efficient parallel processing\n",
            "    • Increased processing speed and efficiency\n",
            "    • Parallel speedup: measures how much faster a parallel algorithm runs compared to its sequential counterpart\n",
            "    • Comparative speedup: compares the performance of parallel and sequential algorithms on the same hardware\n",
            "    • Parallel computing: multiple processors execute program simultaneously, faster execution times\n",
            "    • Parallel speedup: time taken to execute sequentially / time taken to execute parallelly\n",
            "    • Comparative speedup: performance comparison between parallel implementations or hardware sets\n",
            "    • Developers responsible for programming language and parallel software interfaces.\n",
            " • Developers responsible for programming language and parallel software interfaces.\n",
            "     • Parallel computing approaches: hardware models (distributed memory, shared memory, vector units), software models (process based, message passing, thread based).\n",
            "     • Models allow developers to optimize parallelization in applications.\n",
            "     • Distributed memory architecture: multiple nodes with private memory.\n",
            "     • Cross node parallel: tasks distributed across nodes.\n",
            "     • Shared memory architecture: multiple processors share a single memory space.\n",
            "     • On node parallelism: parallel processing on a single node.\n",
            " - Parallelism in shared memory architecture refers to parallel processing techniques on a single node.\n",
            "     - Multiple threads/processes run concurrently on the same node, accessing shared memory for computations.\n",
            "     - Accelerator devices are special purpose add on processors (e.g. GPUs) that offload tasks from CPU and provide parallel processing capabilities.\n",
            "     - General heterogeneous parallel architecture model combines multiple hardware architectures (e.g. CPUs, dual core processors, accelerators) to provide more efficient and powerful system.\n",
            "     - Most common techniques for parallelization are process based parallelization, particularly through message passing.\n",
            "\n",
            "\n",
            "\n",
            "🔍 Parallelization techniques:\n",
            "    🔍 Process-based parallelization: divide task into multiple processes on separate nodes/cores\n",
            "    🔍 Thread-based parallelization: divide task into multiple threads within a single process\n",
            "    🔍 Vectorization: perform multiple ops with one instruction\n",
            "    🔍 Used in: distributed memory systems, multi-core processors, SMP systems\n",
            "    🔍 Vectorization is a parallel computing technique that leverages SIMD capabilities of modern processors.\n",
            "    🔍 Enables efficient and timely processing of data streams by offloading data and compute kernels to GPU's streaming multiprocessors.\n",
            "    🔍 Additional layers of parallelization can be applied on CPUs and GPUs, including vectorization, threads, and processes.\n",
            "    🔍 Allows for working on multiple units of data simultaneously, deploying more compute pathways, and separating program instances into separate memory spaces.\n",
            " • Deploy multiple compute pathways\n",
            "     • Separate program instances\n",
            "     • Offload calculations to GPUs\n",
            "     • Discretize the problem\n",
            "     • Use threading to operate multiple processing cores simultaneously\n",
            "     • Split the work between processors on two desktops\n",
            "     • Understand performance limits and profile parallel programs\n",
            "     • Leverage profiling tools to enhance efficiency and improve speedup\n",
            "     • Use call graphs to gain insights into program structure, dependencies, and optimization opportunities\n",
            "     • Utilize dual socket motherboards for more processing cores and memory bandwidth\n",
            "     • Employ effective parallel algorithms such as \"divide and conquer\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    1. Divide and conquer algorithms are effective in parallel computing.\n",
            "    2. Algorithm analysis is crucial for parallel computing applications.\n",
            "    3. Comparing different algorithms that solve the same problem is important.\n",
            "    4. Factors affecting performance include hardware differences, compiler optimizations, and memory optimization.\n",
            "    5. Parallel algorithms are designed to solve computational problems efficiently by utilizing multiple processing units simultaneously.\n",
            "    6. Common types of parallel algorithms include parallel merge sort, parallel quicksort, and parallel global sum.\n",
            "    7. These algorithms aim to break down tasks into smaller subtasks that can be processed independently and concurrently, leading to significant speedup in overall computation time.\n",
            "    8. Parallel global sum is a problem in parallel or distributed computing.\n",
            "    9. Elements are computed across multiple processors or nodes.\n",
            "    10. Sorts partitions are used to ensure minimal word repetition and brevity.\n",
            "    11. Process occurs sequentially, waiting for previous steps to finish.\n",
            "\n",
            "\n",
            "\n",
            " • Sorts partitions ensure minimal word repetition and brevity.\n",
            "     • Process occurs sequentially, waiting for previous steps to finish.\n",
            "     • Problem discretization into smaller cells/elements.\n",
            "     • Calculation offloaded to GPUs, where vector length is larger than on CPUs.\n",
            "     • Array division into smaller subarrays, maximum values found in parallel.\n",
            "     • Maximum of partial maxima selected as maximum element of entire array.\n",
            "     • Algorithmic complexity measures number of operations in an algorithm.\n",
            "\n",
            "    • Asymptotic notation provides an upper limit on the number of basic operations an algorithm performs\n",
            "    • Big O notation provides an approximation of how an algorithm's running time increases as the input size grows\n",
            "    • Theta notation provides a tight bound on an algorithm's time complexity, characterizing the average case behavior of an algorithm\n",
            "    • θ notation provides a tight bound on an algorithm's time complexity\n",
            "    • ω notation describes the lower bound or best case time complexity of an algorithm\n",
            "    • Both θ and ω notations are used in asymptotic notations like big O, θ, and ω to analyze the efficiency of algorithms\n",
            "    • Framework is useful for characterizing how an algorithm's performance scales as the input size grows towards infinity\n",
            " • Linear time complexity and constant space complexity\n",
            "     • Parallelization techniques for large inputs\n",
            "     • Prefix sum operation and its types (inclusive and exclusive)\n",
            "     • Efficient and can be performed in parallel\n",
            "     • Used in various applications\n",
            "     • No detailed explanations or examples provided\n",
            "     • No discussion of theoretical foundations\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    - Efficient parallel scan uses 2 sweeps through arrays\n",
            "    \n",
            "    - Last value set to zero for first sweep\n",
            "    \n",
            "    - Tree based sweep for final result\n",
            "    \n",
            "    - Changing order of additions affects answer in finite precision arithmetic\n",
            "    \n",
            "    - Non associativity of addition\n",
            "    \n",
            "    - Problem size increases, issues become more severe\n",
            "    \n",
            "    - Addition of last value might not change overall sum\n",
            "    \n",
            "    - Worst case issue with finite precision values (almost identical but opposite signs)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " - Catastrophic cancellation in arithmetic operations\n",
            "     - Worst case scenario with almost identical but different sign values\n",
            "     - Techniques for addressing this issue: long double data types, pairwise summation, Kahan summation, Knuth summation, quad precision summation\n",
            "     - Long double data type on x architecture is easiest solution\n",
            "     - Use of long double data type in x architecture for improving floating point arithmetic precision\n",
            "     - Pairwise summation method as alternative technique for improving accuracy in scientific computing and numerical analysis\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    • Kahan summation is a method to reduce numerical error in floating point summation.\n",
            "    \n",
            "    • It works by pairing numbers, adding them together, and then summing the intermediate sums using the same pairwise summation method.\n",
            "    \n",
            "    • The process involves three main steps: generating a list of elements, pairwise addition, and summing the intermediate sums.\n",
            "    \n",
            "    • Kahan summation improves accuracy in large scale summation operations by breaking down the process into smaller, more manageable parts and using a compensating strategy.\n",
            "    \n",
            "    • Kahan summation maintains a compensation term to keep track of lost precision due to rounding errors.\n",
            "    \n",
            "    • The algorithm initializes the sum and compensation term to zero, then iteratively adds numbers to the sum and calculates the difference between the updated sum and the original sum, adding to the compensation term.\n",
            "    \n",
            "    • The final result is the adjusted sum, which takes into account lost precision.\n",
            " - Final result is adjusted sum, taking into account lost precision\n",
            "     - Quad precision summation refers to arithmetic operations with quadruple precision floating point numbers\n",
            "     - Quadruple precision offers higher precision than single and double precision formats\n",
            "     - Used for applications where high accuracy is required\n",
            "\n",
            "\n",
            "\n",
            " • Sorting chunks of data\n",
            "     • Estimating energy consumption for an application\n",
            "     • Use of weather forecasting and financial modeling\n",
            "     • Flynn's taxonomy (a classification system)\n",
            "     • Different types of computer architectures\n",
            "     • Traditional computers (simplest type)\n",
            "     • Computer architectures\n",
            "     • Advantages and disadvantages of each architecture\n",
            "     • Traditional architecture suitable for image and signal processing\n",
            "     • MISD architecture can improve performance but is not commonly used\n",
            "     • MIMD architecture used in applications where the same operation can be applied to different pieces of data independently\n",
            "     • Example of using MIMD architecture in data analysis task on large dataset of customer reviews\n",
            "     • Sentiment analysis involves splitting a large dataset into smaller subsets and analyzing each subset independently\n",
            "     • Each processing unit analyzes its assigned subset of reviews simultaneously\n",
            "     • Once all processing units have finished, the results are aggregated to produce a comprehensive analysis.\n",
            " • After processing units finish, results are combined for a complete analysis.\n",
            "     • Examples of tasks in the process include parsing requests, querying databases, and generating responses.\n",
            "     • The sentiment analysis process can adapt to different tasks and data.\n",
            "     • Tasks in sentiment analysis may not operate on the same data.\n",
            "     • The smartphone assembly process consists of several stages, with workers performing specific tasks in each stage.\n",
            " • Scenario with multiple stages (A, B, and C) similar to bucket brigade parallelism\n",
            "     • Compiler translates source code for hardware execution\n",
            "     • OS manages execution on computer hardware\n",
            "     • Nodes connected via network communicate through messages\n",
            "     • Each node operates independently with local memory\n",
            "     • Data sharing through explicit message passing\n",
            "     • Nodes work on subsets of data or specific portions of computation\n",
            "     • Coordination between nodes essential for task completion\n",
            "     • Accelerators are specialized hardware components for specific tasks/computations within a system\n",
            "     • They process data or perform tasks on a subset of data or specific portion of computation\n",
            "     • Communication and coordination between nodes are essential\n",
            "     • Accelerators can be accessed and modified by any processor within the system\n",
            "     • Useful in scenarios where same operation needs to be performed on large set of data elements\n",
            "     • Discrete GPUs have many streaming multiprocessors and their own DRAM\n",
            "\n",
            "\n",
            " • Discrete GPUs have many streaming multiprocessors and DRAM\n",
            "     • Accessing data on a discrete GPU requires communication over PCI bus\n",
            "     • Accelerator device: specialized hardware for efficient computation\n",
            "     • Shared DRAM memory between nodes, but in different NUMA regions\n",
            "     • Multi core CPUs with integrated and discrete GPUs\n",
            "     • Communication through message passing (data and instructions exchange)\n",
            "     • Processes spawned and placed on cores of two nodes by message passing library and OS\n",
            "     • OS controls placement of processes and threads during runtime\n",
            "     • Shared memory between threads\n",
            "     • SIMD allows single instruction to operate on multiple data elements\n",
            "     • Stream processing continuously processes data as it is generated\n",
            "     • Useful for handling large volumes of real time data\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "    • Stream processing involves processing data as it arrives, rather than in batches.\n",
            "    \n",
            "    • Useful for handling data from sources like sensors, social media, financial transactions, and IoT devices.\n",
            "    \n",
            "    • Output is transferred back to CPU for file I/O or other work.\n",
            "    \n",
            "    • Examples include modeling volcanic plumes, tsunamis, and early detection of volcanic eruptions using machine learning.\n",
            "    \n",
            "    • Calculation speed is critical for real-time results.\n",
            "    \n",
            "    • Discrete scheme represents physical laws using stencil operations or matrix vector systems.\n",
            "    \n",
            "    • In step, physical laws are used to determine values at each cell.\n",
            "    \n",
            "    • In step, a computational kernel or operation is defined for each element.\n",
            "    \n",
            "    • In step, data is vectorized for faster computation.\n",
            "    \n",
            "    • Processors with vector operations can operate on multiple pieces of data at once.\n",
            " • Data is vectorized for faster computation.\n",
            "     • Processors can operate on multiple pieces of data at once.\n",
            "     • Memory spaces are distinct and separate when work is split across nodes.\n",
            "     • Limits of a system impact system capabilities and limitations.\n",
            "     • Memory bandwidth and non-contiguous memory access have limitations.\n",
            "     • Feeds provide necessary data for operations, but can be limited by memory bandwidth.\n",
            "     • Non-contiguous memory access involves accessing elements not stored sequentially in memory.\n",
            "     • Contiguous memory access: sequential memory access.\n",
            "     • Non-contiguous memory access: scattered memory access with a stride.\n",
            "     • Hardware capabilities can be measured in terms of floating point ops, data transfer, and energy consumption.\n",
            "     • Metrics can include bandwidth and energy usage.\n",
            "     • Tools such as lstopo and lscpu can be used to measure and analyze system performance.\n",
            " • Calculating metrics\n",
            "     • Technical terms defined\n",
            "     • Article structure and ease of follow\n",
            "     • Usefulness for readers\n",
            "     • `lscpu` command and `/proc/cpuinfo` file\n",
            "     • Theoretical maximum FLOPs calculation\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " • Empirical measurements can validate theoretical calculations.\n",
            "     • Empirical bandwidth refers to the fastest rate at which memory can be loaded into the processor.\n",
            "     • Memory latency refers to the time required for the first byte of data from each level of memory.\n",
            "     • Two methods are used to measure bandwidth: the stream benchmark and the roofline model.\n",
            "     • Stream provides quantitative measurements of bandwidth, while Roofline offers a graphical representation.\n",
            "     • Stream focuses on memory bandwidth, while Roofline offers a high-level overview of performance efficiency.\n",
            "     • Machine Balance metric compares performance of different systems.\n",
            "\n",
            "     • Machine Balance metric compares performance of different systems.\n",
            "     • Profiling is necessary to understand application performance.\n",
            "     • Call graphs are used to analyze calling relationships in parallel programs.\n",
            "     • Hot spots are identified by analyzing call graphs.\n",
            "     • Hot spots are functions/tasks that consume a significant amount of computational time.\n",
            "     • Performance can be evaluated using empirical measurement of processor clock frequency and energy consumption.\n",
            "     • Two approaches to measuring clock frequency: using profiling tools or running benchmarking suites.\n",
            "     • Two approaches to measuring energy consumption: using power measurement tools and hardware devices or using software tools like Intel Power Gadget or Linux's powerstat.\n",
            " • Measure power consumption using various tools and methods.\n",
            "     • Use profiling tools for insights into energy consumption patterns and memory usage.\n",
            "     • Utilize memory profiling features integrated into MPI and OpenMP.\n",
            "     • Consider system performance, including algorithmic efficiency and hardware/software factors.\n",
            "     • Use adaptive mesh refinement (AMR) in numerical simulations.\n",
            "\n",
            "    • AMR is used in numerical simulations for improved resolution and efficiency\n",
            "     • Open addressing involves various trial options for open slots\n",
            "     • Grids are used in data transfer, with high resolution grids providing detailed information over smaller areas\n",
            "     • Data transfer between grids involves open addressing\n",
            "     • The process consists of two phases: the upsweep and the downsweep\n",
            "     • Bucket brigade is a method of transporting items by forming a line of people\n",
            "     • Hashing is a technique for quickly storing and retrieving data\n",
            "     • Hash function is a function that takes an input and produces a fixed size hash value\n",
            "\n",
            "    - A hash function is a function that takes an input and produces a fixed size hash value.\n",
            "    \n",
            "    - The hash value is a unique output that ensures the same input always produces the same output.\n",
            "    \n",
            "    - Input data is hashed to produce a unique value.\n",
            "    \n",
            "    - The hash value is also known as a hash code or hash key.\n",
            "    \n",
            "    - Collisions occur when two different inputs produce the same hash value.\n",
            "    \n",
            "    - A hash table stores data in buckets based on the hash value.\n",
            "    \n",
            "    - A hash table is an associative array that maps input to values using hashing.\n",
            "    \n",
            "    - Operations performed efficiently.\n",
            "    \n",
            "    - Load factor affects performance.\n",
            "    \n",
            "    - Sparsity ratio affects performance.\n",
            "    \n",
            "    - Techniques for handling collisions include chaining and open addressing.\n",
            "    \n",
            "    - Spatial hashing technique used for spatial data.\n",
            "    \n",
            "    - Spatial data references a specific geographical area or location.\n",
            "    \n",
            "    - Can be stored in vector or raster format.\n",
            "    \n",
            "    - More than just a spatial component of a map.\n",
            "1. Grid-based hashing for minimal collisions\n",
            "    2. Perfect and compact hashing\n",
            "    3. Importance of efficient hash functions\n",
            "    4. Open addressing for resolving collisions\n",
            "    5. Linear probing, quadratic probing, and double hashing\n",
            "    6. Spatial data operations: neighbor finding, remapping, table lookup, and sorting\n",
            "    7. Implemented using perfect or compact hashing\n",
            "    8. Neighbor finding using spatial perfect hash\n",
            "    9. Sorting: A D or D sort of the cell data can be implemented using perfect or compact hashing\n",
            "    10. Remapping: Mapping another AMR mesh onto a current AMR mesh using a perfect hash\n",
            "    11. Steps involved: neighbor finding, remapping, table lookup, and sorting\n",
            "\n",
            "\n",
            "\n",
            " - Spatial hashing is used in CFD and FEA to map data from a source mesh to a target mesh.\n",
            "     - A perfect hash function is used for table lookup.\n",
            "     - Chaining (linked lists in each bucket) is used for collision resolution.\n",
            "     - Bucket size must be chosen to avoid collisions.\n",
            "     - Minimum difference between values is demonstrated.\n",
            "     - Bucket location is calculated by dividing key by minimum difference.\n",
            "     - Values or indices are stored in the hash table.\n",
            "     - The first key stored in a bucket is used.\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(combined_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " NLTK can be used to tokenize a sentence, identify parts of speech, and perform sentiment analysis on a given text.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ": LLAMA, mentioned in your code, is a specific type of LLM designed for natural language generation tasks. It can generate coherent and contextually relevant text based on prompts or templates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0067cbd3212d4982b282b60e9595d55c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00c6b66c662747d58915fcfbf73ecc28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65d30454942d407196207f7047d067a3",
            "placeholder": "​",
            "style": "IPY_MODEL_ca2807f3b1b9479396ddc103fb9f0817",
            "value": " 2/2 [01:56&lt;00:00, 52.85s/it]"
          }
        },
        "010d425aeacf48958321d247b012b9e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01c5b920f9e6402393421b742d292511": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01f6627d81b34cc490be8715a87b6052": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "045cd436d6684e488b4f2ea68fdcb505": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0485883bf3734f76b6d74cfd394a839d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05d7934a2664489d962df8817f4b62ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06ef2c252438449786e6e53bb0ff71af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07b5cf374657429698e0571fa531afe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_415636cea443480ca587bdb0f14cadbd",
              "IPY_MODEL_4eb4c79a5b33405b86b036f73b97e5e5",
              "IPY_MODEL_69df3ec03b894e24964d7b026a05e333"
            ],
            "layout": "IPY_MODEL_70b6de45fd814c78a8fa5aed4ef25c71"
          }
        },
        "089cca1722c44e79ade5511e18d6cb3f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f543a57dc2485aa06b90d6c7916fd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08fc40c0f03243cab947a7cef7eff9bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bafd751df08478798ccc1f83129adee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0da253259aa143b3989b9d2b35b5256d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0da90438c34340d9bc244432a263b775": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5b04591614a45baa61c99fffeaee9ea",
              "IPY_MODEL_1c5256ca7bdf49db9d319630deb8afe6",
              "IPY_MODEL_f9c756dcb961421281b5fbda0dcf710c"
            ],
            "layout": "IPY_MODEL_010d425aeacf48958321d247b012b9e5"
          }
        },
        "0e0838df15cf45c7833d673d29005d92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eac531a8d76444ea6080cf431c95c70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f406c11440947ab9d61ef69edcce4ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fc765c2ad074c23bf983778536e6ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_880fc387eb0545daacefd2ddccbc142e",
            "placeholder": "​",
            "style": "IPY_MODEL_c2b8d9bcefb24e5581b88a6da7670ca2",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 86.0kB/s]"
          }
        },
        "10cfcad40a324b5e8627b9cf49362f9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10df2301d7404c679519fcb2683aafb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "121b55d7fff541adb2339fb9dba18ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130a979d290f4c73ae5e6159709fad30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "134dd13cbdf34877b5c762058438e407": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1398db51216d4c0694e5c8f5605cf821": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f1ac6bad7e2425cb91a86255bb9c973",
            "placeholder": "​",
            "style": "IPY_MODEL_86f57bc710464b1f980e41525d3b7602",
            "value": " 614/614 [00:00&lt;00:00, 7.69kB/s]"
          }
        },
        "13a53cbf190b416aa389679b3cf99d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1440ac4325d04b67a2f0cca0bf707052": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_406e17f1717642f3b1de316d149db4b7",
              "IPY_MODEL_55fb87a785344c2e87e9415fe6b5971d",
              "IPY_MODEL_3c346f1e5e0b44bb901384f512472a77"
            ],
            "layout": "IPY_MODEL_cebca57b98024b82bd44d6b87357e6c6"
          }
        },
        "151cb3174e764d22b756df1a8cfebab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7209cf92aa4e457a8b088d5156dfd148",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_045cd436d6684e488b4f2ea68fdcb505",
            "value": 349
          }
        },
        "15918e6cc16f48d0aef9e0338ba9414b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d820ae5158de4add8287ef107bd147c9",
            "placeholder": "​",
            "style": "IPY_MODEL_cc7f42eadca64700bdc6442df1795a55",
            "value": ".gitattributes: 100%"
          }
        },
        "166215b9186b48c7bc2e81633034eb07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f934488f864bfc888dfd2e3dbaf63d",
            "placeholder": "​",
            "style": "IPY_MODEL_401cd6c92be449b6a4ca9d3896a120bc",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 7.32MB/s]"
          }
        },
        "17b1f4e2ca844c8098976d89efc73ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f406c11440947ab9d61ef69edcce4ae",
            "placeholder": "​",
            "style": "IPY_MODEL_6079e41ab2f84152914b6705288758b1",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "1b70cc83eb444f70b72383181702720b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c37f663fe1341a7b8f0264d702ed458": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b04bb0dbe8a34a3a9ecfd3af6137b4aa",
            "placeholder": "​",
            "style": "IPY_MODEL_330644e2c3f64d89b2d4c215731ba220",
            "value": "train_script.py: 100%"
          }
        },
        "1c45f801d2264ab19e8b4668aef7a883": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ac787938e3142268aa87002d353fd6e",
              "IPY_MODEL_ec86ae2ba7284a0582175b0211195e35",
              "IPY_MODEL_7244f69df3954192852a6f779ee694f9"
            ],
            "layout": "IPY_MODEL_5647e09dfc6341308f6d368e4f9d1245"
          }
        },
        "1c5256ca7bdf49db9d319630deb8afe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08f543a57dc2485aa06b90d6c7916fd1",
            "max": 188,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f8496d18f3b4d20bd1a506dc5286f1a",
            "value": 188
          }
        },
        "1e1ca2e880ca48338f43b398d7b7fe75": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fd3f0531553439980b28c41ddb12bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_927a2735ea124d329c940928fe9036cd",
              "IPY_MODEL_259068f072bd44ecadcbf9f5dd2caa38",
              "IPY_MODEL_ecb2939ca02b496bac01bfed46f093e6"
            ],
            "layout": "IPY_MODEL_0eac531a8d76444ea6080cf431c95c70"
          }
        },
        "207c4020a0854f9d9d5347a3e5a70700": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20a5ff7425ea4398bd59f2e3d3c1665a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a318fd36784acabdbfddeb08aefbe9",
            "placeholder": "​",
            "style": "IPY_MODEL_f6475ea2117c4d318099aa428711e207",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 712kB/s]"
          }
        },
        "2123d9ee8e2d442c96768ade7b3c3e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2419d5d90d6b4175b18fb4044559a71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e39daa366c9347c794609473f36c48a1",
            "placeholder": "​",
            "style": "IPY_MODEL_827b9be43d8647c380d395c89127a0b6",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "2436c5504d7a48ec9c956ee4f853d0c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "246622d76a864d69b6415aae695c4676": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29595bfa29ee41759cd0c324da256324",
            "placeholder": "​",
            "style": "IPY_MODEL_56d3be2c83a0462db1aba0ae90dbcf25",
            "value": "data_config.json: 100%"
          }
        },
        "25765a2f8c5f46eeb1a1935be9489942": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f09e71b69414d5ea8d33775b0e49afb",
              "IPY_MODEL_389a979e51fa4e80b73fe0ea91f4d44a",
              "IPY_MODEL_6699b8425e0840f5b056560a53f8d264"
            ],
            "layout": "IPY_MODEL_7289bf695a444051a3df4c774f9e89e6"
          }
        },
        "25809a66c2e6451bae6a41c0e79589ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "259068f072bd44ecadcbf9f5dd2caa38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b612e4df05d34f8a8777de1df96385e2",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8e9b59c0852462b9189700249b7f53a",
            "value": 1618
          }
        },
        "25bca1cd81404e9f891d2fb03b6252cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "273d2d2efbe44670a912503be3ccab32": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab37193e0554b868d8cd2c89c1a75b6",
            "placeholder": "​",
            "style": "IPY_MODEL_af3bf8baa629407db57e30d859a14dc9",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.08kB/s]"
          }
        },
        "277a8d7e669e4a419308845cd212bc3c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28a9394b23544f06b7fb67fb1c22b6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2419d5d90d6b4175b18fb4044559a71b",
              "IPY_MODEL_d219ff58b2ac4759b95c424feb68c2ff",
              "IPY_MODEL_630f349b9728467bb80d22019d0cb9c8"
            ],
            "layout": "IPY_MODEL_089cca1722c44e79ade5511e18d6cb3f"
          }
        },
        "29595bfa29ee41759cd0c324da256324": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bac1dae5e99478e943233a378b692ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25bca1cd81404e9f891d2fb03b6252cb",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10df2301d7404c679519fcb2683aafb6",
            "value": 116
          }
        },
        "2c7b7026e9fa4b3ab605b9a69492b795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e1e8db1efac4c66b36a59e0cce7ae6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f1ac6bad7e2425cb91a86255bb9c973": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a9903b92fd4898a912a85197a0fa8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3185bd21d3ac45a68cd4a4fadd4b4cbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31d55b73c00346048c15b13b553aa004": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_386d337704554263bf5a6e6c6ef9040b",
            "max": 9976576152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a881c3671f8e4da1872ded5855d259f5",
            "value": 9976576152
          }
        },
        "330644e2c3f64d89b2d4c215731ba220": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33871bff4a434e4eb4878577dda7d87f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "341e2e7960ca42978f36f7e6f7e2ba0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "348fff0007cf4a4d9d79b4042280fc93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "357c701d705947bdb3277d79baf65b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d981f65753bf4ce9aab6e48c12dbbebf",
              "IPY_MODEL_b310fa65d16f470f9aab3e2b378b35c6",
              "IPY_MODEL_273d2d2efbe44670a912503be3ccab32"
            ],
            "layout": "IPY_MODEL_121b55d7fff541adb2339fb9dba18ad5"
          }
        },
        "3613369f598141408176e59870880b92": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d796832ca14bc4ab65d5e6c6d7718a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e1ca2e880ca48338f43b398d7b7fe75",
            "max": 26788,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6863be341b224208911150937cf6d49b",
            "value": 26788
          }
        },
        "3752af9de83f4fb3983b1498f67c14ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37b3bdd457704d1d9be66a4b043cdca3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37f4c5b6a30f40d69d330f5981ca24ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "383723ff9b8c49bdac77ba69452e976d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05d7934a2664489d962df8817f4b62ba",
            "placeholder": "​",
            "style": "IPY_MODEL_2123d9ee8e2d442c96768ade7b3c3e7a",
            "value": " 116/116 [00:00&lt;00:00, 8.43kB/s]"
          }
        },
        "386d337704554263bf5a6e6c6ef9040b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "389a979e51fa4e80b73fe0ea91f4d44a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33871bff4a434e4eb4878577dda7d87f",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e46fc4ba9e6340f39a2d65bc9169f721",
            "value": 231508
          }
        },
        "3912d48a397d4aeab0bea005bf133b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c346f1e5e0b44bb901384f512472a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e4f5802a99148c9b18fef97e5ffb41a",
            "placeholder": "​",
            "style": "IPY_MODEL_e4b7590ef96e4fd292f24360575e6fe8",
            "value": " 190/190 [00:00&lt;00:00, 15.5kB/s]"
          }
        },
        "3fd004e8f1ca419d94e2c14b080f6591": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3fd468616b23429dac88ad2364621eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75ff2d0080954489acf1eb0f0bd15287",
              "IPY_MODEL_e403799d08624226b7d4ffd4daff5872",
              "IPY_MODEL_81834d69630d459cb63cb15e3df59c3b"
            ],
            "layout": "IPY_MODEL_5334617c720a41848c29a2ca6a306267"
          }
        },
        "401cd6c92be449b6a4ca9d3896a120bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "406e17f1717642f3b1de316d149db4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79754fb19a944d679aff7d1d41bee382",
            "placeholder": "​",
            "style": "IPY_MODEL_30a9903b92fd4898a912a85197a0fa8c",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "40d54a26f1184bd3bbf4a4610ce9c5f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "415636cea443480ca587bdb0f14cadbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10cfcad40a324b5e8627b9cf49362f9b",
            "placeholder": "​",
            "style": "IPY_MODEL_3912d48a397d4aeab0bea005bf133b47",
            "value": "README.md: 100%"
          }
        },
        "41f934488f864bfc888dfd2e3dbaf63d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c017acda504247b7019a7e4ed7cfa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a598bf4e25a457d850c9ab8f61a3d7f",
              "IPY_MODEL_b3be2a94140f4585b66bed0dc8354d82",
              "IPY_MODEL_8c5b5229b356441096571548ba1b238f"
            ],
            "layout": "IPY_MODEL_277a8d7e669e4a419308845cd212bc3c"
          }
        },
        "440f71e8aba644718bbf5a9d36b2dfa9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "443b7a09cff64df1908b45aa7db424bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "453a3077a6424e36b24bc5392c5bda55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc54cb4131be4e00a93da43cc2347e5d",
            "placeholder": "​",
            "style": "IPY_MODEL_1b70cc83eb444f70b72383181702720b",
            "value": "tokenizer.json: 100%"
          }
        },
        "46a7db90c743450da24cefc1301d6e9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4800262b66bf4211ba9dbe6b63a1fb85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49b3758e6aae4266b27358dafa67a472": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a8abe70e81b41b6b906d7f589c96e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b26f5f9c1e14e1b87a46df9ecdc8ce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15918e6cc16f48d0aef9e0338ba9414b",
              "IPY_MODEL_afc13d22546d49beac0e07cee8594884",
              "IPY_MODEL_0fc765c2ad074c23bf983778536e6ae3"
            ],
            "layout": "IPY_MODEL_c92c1a4c2106446dade797d455248402"
          }
        },
        "4ba75d56b0b64550b45ddb887f1e7487": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd6322456434347a1fbf28c0eb449db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d6cf7fd213a4fcf9d4b0bb8a7353925": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eb4c79a5b33405b86b036f73b97e5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3613369f598141408176e59870880b92",
            "max": 10610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dedabd3236e84164b3682929b0dec997",
            "value": 10610
          }
        },
        "4f8496d18f3b4d20bd1a506dc5286f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5334617c720a41848c29a2ca6a306267": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5370bb0d91254e6b80067f58338769ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cffe6b9f73ea43c59b04dc2f5bd43f3d",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3185bd21d3ac45a68cd4a4fadd4b4cbf",
            "value": 13156
          }
        },
        "54acc001f90a4396940365689f3073f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55646c5d5b4b4d6f88a571a01be7646c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55fb87a785344c2e87e9415fe6b5971d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90003a4b4a634e11a771f78099479270",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3752af9de83f4fb3983b1498f67c14ad",
            "value": 190
          }
        },
        "5647e09dfc6341308f6d368e4f9d1245": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56afaf64fa044ee29f72a85b54e83817": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56d3be2c83a0462db1aba0ae90dbcf25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "592b84c8aee24de0ae9415a35bf4b2c3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a1e0f3563c240f492d87faa3cdebc35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a2be7be86b342329b06f945d2f0c16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25809a66c2e6451bae6a41c0e79589ca",
            "placeholder": "​",
            "style": "IPY_MODEL_4800262b66bf4211ba9dbe6b63a1fb85",
            "value": " 26.8k/26.8k [00:00&lt;00:00, 349kB/s]"
          }
        },
        "5aaf0d68a737490b8580034fbb539e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08fc40c0f03243cab947a7cef7eff9bc",
            "placeholder": "​",
            "style": "IPY_MODEL_bd54092c9f0c4878b58884079d0c590e",
            "value": " 2/2 [01:06&lt;00:00, 30.57s/it]"
          }
        },
        "5ab4a0e86caa4b989e36afc4f58262f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8e8d1e0b5854ffeabb9cc84f6e1f42e",
            "placeholder": "​",
            "style": "IPY_MODEL_e117cce7b8a1414fb21c7bb0174ce546",
            "value": "tokenizer.model: 100%"
          }
        },
        "5ad1e2bb92674c7bbe126c22ed44f4ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c24226a6f1f4f9a94f62f42abfccf5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8700e39b542343ab8dec520f85851962",
            "placeholder": "​",
            "style": "IPY_MODEL_4a8abe70e81b41b6b906d7f589c96e70",
            "value": " 500k/500k [00:00&lt;00:00, 3.22MB/s]"
          }
        },
        "5e4f5802a99148c9b18fef97e5ffb41a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e87fcf97ea8488b8c0e5c628fe1c15b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eabf59e514849aaa89e645ba831df1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6079e41ab2f84152914b6705288758b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60a318fd36784acabdbfddeb08aefbe9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61447be61da040f783db02af4a1c3f00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "630f349b9728467bb80d22019d0cb9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61447be61da040f783db02af4a1c3f00",
            "placeholder": "​",
            "style": "IPY_MODEL_4d6cf7fd213a4fcf9d4b0bb8a7353925",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 166MB/s]"
          }
        },
        "63373723871c4a779f828a072a8a7a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a53b9945064e9386cf4a9b43b6bde0",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72f06ba39b1b4646849799523603c35b",
            "value": 614
          }
        },
        "65b30315baa243289cff793a390256a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97cc701da613482b8116cb8d7a918235",
              "IPY_MODEL_2bac1dae5e99478e943233a378b692ad",
              "IPY_MODEL_383723ff9b8c49bdac77ba69452e976d"
            ],
            "layout": "IPY_MODEL_5e87fcf97ea8488b8c0e5c628fe1c15b"
          }
        },
        "65d30454942d407196207f7047d067a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6699b8425e0840f5b056560a53f8d264": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98721607aae74677927ed54e4ec71ea4",
            "placeholder": "​",
            "style": "IPY_MODEL_5a1e0f3563c240f492d87faa3cdebc35",
            "value": " 232k/232k [00:00&lt;00:00, 15.5MB/s]"
          }
        },
        "66ea718ce1b04801964b40b1c9ed5f67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67c1e871843044e886536f82f6bf4386": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c37f663fe1341a7b8f0264d702ed458",
              "IPY_MODEL_5370bb0d91254e6b80067f58338769ca",
              "IPY_MODEL_20a5ff7425ea4398bd59f2e3d3c1665a"
            ],
            "layout": "IPY_MODEL_f3b8c0f0e53740698ff48b2ec0faf5f5"
          }
        },
        "684d428d881142ae86d719d473f97079": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6863be341b224208911150937cf6d49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "69d5be3efd434cf3bc831379102525a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69df3ec03b894e24964d7b026a05e333": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b62a050fa13b4b4cb94821c34a781982",
            "placeholder": "​",
            "style": "IPY_MODEL_aa980ba89ff2416f971f799367ff808c",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 595kB/s]"
          }
        },
        "6dd8cb25887b4a2ca45f0385904fd8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b33038646f6d4e95ba809c59102d7659",
              "IPY_MODEL_6f60bbe7c613405ebb1e160583288b86",
              "IPY_MODEL_72c3a43cd32f4019b5068f00bb853795"
            ],
            "layout": "IPY_MODEL_eba3e75c58124a07a593b5ef8ae37142"
          }
        },
        "6de86a633fa144d284d217d2b7e1e956": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e12c540d1eb483ba34688727431d2ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f2f10a55a9148b89c05247dd171d430": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bd6322456434347a1fbf28c0eb449db",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc33eaa3b3d64cd38280ac4a12236945",
            "value": 2
          }
        },
        "6f60bbe7c613405ebb1e160583288b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5f85d2dd8684c9789dc8ccbe8a2b7a8",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d75c8412c8bf42f1811c5db9f84351d9",
            "value": 466247
          }
        },
        "6f88f589da5746b0b3012cab3f04fd2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70b6de45fd814c78a8fa5aed4ef25c71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7209cf92aa4e457a8b088d5156dfd148": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7244f69df3954192852a6f779ee694f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ad1e2bb92674c7bbe126c22ed44f4ac",
            "placeholder": "​",
            "style": "IPY_MODEL_fc3a18476f25474d8be79a6e76ea73d4",
            "value": " 112/112 [00:00&lt;00:00, 9.31kB/s]"
          }
        },
        "7289bf695a444051a3df4c774f9e89e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72c3a43cd32f4019b5068f00bb853795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9b18b6b0a2f47be8f6f4a7312b70169",
            "placeholder": "​",
            "style": "IPY_MODEL_130a979d290f4c73ae5e6159709fad30",
            "value": " 466k/466k [00:00&lt;00:00, 16.1MB/s]"
          }
        },
        "72f06ba39b1b4646849799523603c35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "754fda1ec6b14bc784ea7a863c9856c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75ff2d0080954489acf1eb0f0bd15287": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2436c5504d7a48ec9c956ee4f853d0c4",
            "placeholder": "​",
            "style": "IPY_MODEL_54acc001f90a4396940365689f3073f4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "79754fb19a944d679aff7d1d41bee382": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a9dd9effebb4d7f80fb87a64a0c7524": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ab37193e0554b868d8cd2c89c1a75b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac787938e3142268aa87002d353fd6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a7db90c743450da24cefc1301d6e9f",
            "placeholder": "​",
            "style": "IPY_MODEL_134dd13cbdf34877b5c762058438e407",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "7c9798b46b434698b110bdca48e0a679": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d2eccb895bf423898000a429bb22829": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e495eb59998460da7c11313fd7308d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17b1f4e2ca844c8098976d89efc73ccb",
              "IPY_MODEL_e42bdcbe90954392802f25a8ebe49488",
              "IPY_MODEL_f17d7d344987445e9ea416f678081f01"
            ],
            "layout": "IPY_MODEL_0e0838df15cf45c7833d673d29005d92"
          }
        },
        "7f09e71b69414d5ea8d33775b0e49afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13a53cbf190b416aa389679b3cf99d2c",
            "placeholder": "​",
            "style": "IPY_MODEL_e1cb224eb69b44cbad98971c199b51b2",
            "value": "vocab.txt: 100%"
          }
        },
        "7f97dee86c594c1b9df74979381ddfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6938629282b4bc49ce99bc5969112ec",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c035d61a7b2746ccb7068044e77e00b7",
            "value": 39265
          }
        },
        "802aab6089df4829b5e995a70744aea2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817088136b6a4a7c95fb20afcc340a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d98add0b681845cdb31406350427a10a",
              "IPY_MODEL_8be01a0b885843db8dbcfc6e18d8f857",
              "IPY_MODEL_ad4be70826ab47b8b2ec837dae7d49d1"
            ],
            "layout": "IPY_MODEL_6f88f589da5746b0b3012cab3f04fd2b"
          }
        },
        "81834d69630d459cb63cb15e3df59c3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_348fff0007cf4a4d9d79b4042280fc93",
            "placeholder": "​",
            "style": "IPY_MODEL_4ba75d56b0b64550b45ddb887f1e7487",
            "value": " 414/414 [00:00&lt;00:00, 5.44kB/s]"
          }
        },
        "827b9be43d8647c380d395c89127a0b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8337c953ff04444183492b842caef018": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e82f01bb4646049f18cfb40ae2f8c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "865d00ea545749cdaa75b8dc51ebce01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86f57bc710464b1f980e41525d3b7602": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8700e39b542343ab8dec520f85851962": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "880fc387eb0545daacefd2ddccbc142e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a598bf4e25a457d850c9ab8f61a3d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1884fe85d9544d695a4cda6ff625d1d",
            "placeholder": "​",
            "style": "IPY_MODEL_f97ddf3c70e04b38a5f5dbf8519338bf",
            "value": "config.json: 100%"
          }
        },
        "8be01a0b885843db8dbcfc6e18d8f857": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b411dfa24ec342be82fc54d133d4fca8",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7f83bf7b5654fd3bd43aea45cecc99f",
            "value": 350
          }
        },
        "8c5b5229b356441096571548ba1b238f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e793f20e6c444bd8bdea619444d73775",
            "placeholder": "​",
            "style": "IPY_MODEL_e2d8a2ee0591448980c5c0c4128f927c",
            "value": " 612/612 [00:00&lt;00:00, 38.3kB/s]"
          }
        },
        "8f564c9cb8c741ff867fd66a916c5d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90003a4b4a634e11a771f78099479270": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927a2735ea124d329c940928fe9036cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_592b84c8aee24de0ae9415a35bf4b2c3",
            "placeholder": "​",
            "style": "IPY_MODEL_b1af8f61aea547c2bfd4409b6e6ca774",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "92a23b4920084407af1decd6ff0fc8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97cc701da613482b8116cb8d7a918235": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_207c4020a0854f9d9d5347a3e5a70700",
            "placeholder": "​",
            "style": "IPY_MODEL_3fd004e8f1ca419d94e2c14b080f6591",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "98721607aae74677927ed54e4ec71ea4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "995f8c3da9a44cd7bbc12552c8dfffda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ab4a0e86caa4b989e36afc4f58262f6",
              "IPY_MODEL_bc3dc2eecafb4f4c82258db7483f2fd5",
              "IPY_MODEL_5c24226a6f1f4f9a94f62f42abfccf5f"
            ],
            "layout": "IPY_MODEL_5eabf59e514849aaa89e645ba831df1c"
          }
        },
        "9b8172fa894d463fa646905a47504f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8337c953ff04444183492b842caef018",
            "placeholder": "​",
            "style": "IPY_MODEL_faeff528fddc42618ca7ef6c46df66ce",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 2.48MB/s]"
          }
        },
        "9b8ffe7803b0458dbbdc2ec1c57049d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a4edbe7102604a33a314989daef49c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a864278de4af450d8ba5f88e35bc4cff",
              "IPY_MODEL_bfc2ae82f1e64aa1bf53e10a65ddecec",
              "IPY_MODEL_5aaf0d68a737490b8580034fbb539e7a"
            ],
            "layout": "IPY_MODEL_e659a998dc394d35bbf62c5220c9ae43"
          }
        },
        "a5f85d2dd8684c9789dc8ccbe8a2b7a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a63067e77e1f4b9085d5643324bf4188": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8ad9c374727475996a71894a0ac1bcd",
              "IPY_MODEL_6f2f10a55a9148b89c05247dd171d430",
              "IPY_MODEL_00c6b66c662747d58915fcfbf73ecc28"
            ],
            "layout": "IPY_MODEL_e389376124014ea991f961013e720342"
          }
        },
        "a75fbdfb58014986b5541370e1eeb341": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d54a26f1184bd3bbf4a4610ce9c5f4",
            "placeholder": "​",
            "style": "IPY_MODEL_b82ebd31ad50448f8ed9ee69790bbbea",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "a7c4fb1c78104eb3b132083e8dfbb2f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef02edc83b284f73b3cf83d8196c3095",
            "placeholder": "​",
            "style": "IPY_MODEL_e9f3b6e600d04116a51953300826b7dc",
            "value": "modules.json: 100%"
          }
        },
        "a864278de4af450d8ba5f88e35bc4cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d88260045e6a450e987247c2e82f63d9",
            "placeholder": "​",
            "style": "IPY_MODEL_acc0e3b453454610be3b097e01b5a930",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a881c3671f8e4da1872ded5855d259f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a94e33aa900042d39ec3f1ad1f31fd60": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66ea718ce1b04801964b40b1c9ed5f67",
            "placeholder": "​",
            "style": "IPY_MODEL_865d00ea545749cdaa75b8dc51ebce01",
            "value": " 9.98G/9.98G [01:29&lt;00:00, 115MB/s]"
          }
        },
        "a9f0f228532e4d7c814f43b93150740b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa980ba89ff2416f971f799367ff808c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abb316aafa7e4142896cd5be79add40b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acc0e3b453454610be3b097e01b5a930": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad4be70826ab47b8b2ec837dae7d49d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e37563ca76c8444e80217363894035e7",
            "placeholder": "​",
            "style": "IPY_MODEL_2e1e8db1efac4c66b36a59e0cce7ae6a",
            "value": " 350/350 [00:00&lt;00:00, 9.93kB/s]"
          }
        },
        "ae5ac601e6a94265a5078bd82918a979": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af3bf8baa629407db57e30d859a14dc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afab86c6094043b6ad0caba746274a3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a75fbdfb58014986b5541370e1eeb341",
              "IPY_MODEL_31d55b73c00346048c15b13b553aa004",
              "IPY_MODEL_a94e33aa900042d39ec3f1ad1f31fd60"
            ],
            "layout": "IPY_MODEL_440f71e8aba644718bbf5a9d36b2dfa9"
          }
        },
        "afc13d22546d49beac0e07cee8594884": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c42d121fb7a046bc8f0dfad002448a51",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2c1e790804e46dfab61bb6482f0b800",
            "value": 1175
          }
        },
        "b035483702824f5b872543de152792f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7c4fb1c78104eb3b132083e8dfbb2f5",
              "IPY_MODEL_151cb3174e764d22b756df1a8cfebab3",
              "IPY_MODEL_f28c4f7c30224cb6a898a2a726ae5963"
            ],
            "layout": "IPY_MODEL_6de86a633fa144d284d217d2b7e1e956"
          }
        },
        "b04bb0dbe8a34a3a9ecfd3af6137b4aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1af8f61aea547c2bfd4409b6e6ca774": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b310fa65d16f470f9aab3e2b378b35c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56afaf64fa044ee29f72a85b54e83817",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae5ac601e6a94265a5078bd82918a979",
            "value": 53
          }
        },
        "b33038646f6d4e95ba809c59102d7659": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b3758e6aae4266b27358dafa67a472",
            "placeholder": "​",
            "style": "IPY_MODEL_01c5b920f9e6402393421b742d292511",
            "value": "tokenizer.json: 100%"
          }
        },
        "b3be2a94140f4585b66bed0dc8354d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abb316aafa7e4142896cd5be79add40b",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06ef2c252438449786e6e53bb0ff71af",
            "value": 612
          }
        },
        "b411dfa24ec342be82fc54d133d4fca8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b612e4df05d34f8a8777de1df96385e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b62a050fa13b4b4cb94821c34a781982": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b82ebd31ad50448f8ed9ee69790bbbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b83381e6efc34f35ae16743ea58621cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc33eaa3b3d64cd38280ac4a12236945": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc3dc2eecafb4f4c82258db7483f2fd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_341e2e7960ca42978f36f7e6f7e2ba0e",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b83381e6efc34f35ae16743ea58621cc",
            "value": 499723
          }
        },
        "bc565c6e32a445d2955aecdba3aadad8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd54092c9f0c4878b58884079d0c590e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb6c5a35bfe4ef5a130372c8b87ca6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_684d428d881142ae86d719d473f97079",
            "placeholder": "​",
            "style": "IPY_MODEL_92a23b4920084407af1decd6ff0fc8a2",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "bfc2ae82f1e64aa1bf53e10a65ddecec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca9b4b60af6a49bd85f68379a2672535",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b8ffe7803b0458dbbdc2ec1c57049d2",
            "value": 2
          }
        },
        "c035d61a7b2746ccb7068044e77e00b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2b8d9bcefb24e5581b88a6da7670ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2c1e790804e46dfab61bb6482f0b800": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c42d121fb7a046bc8f0dfad002448a51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c682b70874c641b0aeacc875d77f52c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8bac774c5864d1babebaa4903acdd68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9141e8889a84996be82c5278f2a7256": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c92c1a4c2106446dade797d455248402": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c96d70c291874ca58794ae248e9ed5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_453a3077a6424e36b24bc5392c5bda55",
              "IPY_MODEL_fa71edfddd2248ae911649a17d52440b",
              "IPY_MODEL_166215b9186b48c7bc2e81633034eb07"
            ],
            "layout": "IPY_MODEL_443b7a09cff64df1908b45aa7db424bd"
          }
        },
        "ca2807f3b1b9479396ddc103fb9f0817": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca9b4b60af6a49bd85f68379a2672535": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc7f42eadca64700bdc6442df1795a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cebca57b98024b82bd44d6b87357e6c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cffe6b9f73ea43c59b04dc2f5bd43f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d07513b491e340fe9f173dbed9bf109f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1b2e4262698480a84084d29782a24be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d219ff58b2ac4759b95c424feb68c2ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffad9115768e48f49f7005185556a9d0",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bafd751df08478798ccc1f83129adee",
            "value": 90888945
          }
        },
        "d75c8412c8bf42f1811c5db9f84351d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7f83bf7b5654fd3bd43aea45cecc99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d820ae5158de4add8287ef107bd147c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88260045e6a450e987247c2e82f63d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d90ee37a3be9444eaca328ca34660733": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d981f65753bf4ce9aab6e48c12dbbebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a9dd9effebb4d7f80fb87a64a0c7524",
            "placeholder": "​",
            "style": "IPY_MODEL_c9141e8889a84996be82c5278f2a7256",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "d98add0b681845cdb31406350427a10a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37b3bdd457704d1d9be66a4b043cdca3",
            "placeholder": "​",
            "style": "IPY_MODEL_d1b2e4262698480a84084d29782a24be",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d9b18b6b0a2f47be8f6f4a7312b70169": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc54cb4131be4e00a93da43cc2347e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dedabd3236e84164b3682929b0dec997": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e117cce7b8a1414fb21c7bb0174ce546": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1884fe85d9544d695a4cda6ff625d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1cb224eb69b44cbad98971c199b51b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2d8a2ee0591448980c5c0c4128f927c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e37563ca76c8444e80217363894035e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e389376124014ea991f961013e720342": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e39daa366c9347c794609473f36c48a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e403799d08624226b7d4ffd4daff5872": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90ee37a3be9444eaca328ca34660733",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc30da70bac8455ca0763df819834119",
            "value": 414
          }
        },
        "e42bdcbe90954392802f25a8ebe49488": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9f0f228532e4d7c814f43b93150740b",
            "max": 3500296424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85e82f01bb4646049f18cfb40ae2f8c0",
            "value": 3500296424
          }
        },
        "e46fc4ba9e6340f39a2d65bc9169f721": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4b7590ef96e4fd292f24360575e6fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e659a998dc394d35bbf62c5220c9ae43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6938629282b4bc49ce99bc5969112ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e793f20e6c444bd8bdea619444d73775": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ad9c374727475996a71894a0ac1bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d2eccb895bf423898000a429bb22829",
            "placeholder": "​",
            "style": "IPY_MODEL_2c7b7026e9fa4b3ab605b9a69492b795",
            "value": "Downloading shards: 100%"
          }
        },
        "e9f3b6e600d04116a51953300826b7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eba3e75c58124a07a593b5ef8ae37142": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec86ae2ba7284a0582175b0211195e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_802aab6089df4829b5e995a70744aea2",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55646c5d5b4b4d6f88a571a01be7646c",
            "value": 112
          }
        },
        "ecb2939ca02b496bac01bfed46f093e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc565c6e32a445d2955aecdba3aadad8",
            "placeholder": "​",
            "style": "IPY_MODEL_37f4c5b6a30f40d69d330f5981ca24ec",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 16.9kB/s]"
          }
        },
        "ed2b2dc791264b959471f3611010efd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef02edc83b284f73b3cf83d8196c3095": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05bfa06beca4fe19ae4d026565c4bee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_246622d76a864d69b6415aae695c4676",
              "IPY_MODEL_7f97dee86c594c1b9df74979381ddfbf",
              "IPY_MODEL_9b8172fa894d463fa646905a47504f69"
            ],
            "layout": "IPY_MODEL_0067cbd3212d4982b282b60e9595d55c"
          }
        },
        "f17d7d344987445e9ea416f678081f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d07513b491e340fe9f173dbed9bf109f",
            "placeholder": "​",
            "style": "IPY_MODEL_8f564c9cb8c741ff867fd66a916c5d4c",
            "value": " 3.50G/3.50G [00:27&lt;00:00, 203MB/s]"
          }
        },
        "f28c4f7c30224cb6a898a2a726ae5963": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0da253259aa143b3989b9d2b35b5256d",
            "placeholder": "​",
            "style": "IPY_MODEL_754fda1ec6b14bc784ea7a863c9856c3",
            "value": " 349/349 [00:00&lt;00:00, 15.8kB/s]"
          }
        },
        "f3b8c0f0e53740698ff48b2ec0faf5f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4534d0465de4078a5f9345295784661": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_beb6c5a35bfe4ef5a130372c8b87ca6a",
              "IPY_MODEL_36d796832ca14bc4ab65d5e6c6d7718a",
              "IPY_MODEL_5a2be7be86b342329b06f945d2f0c16b"
            ],
            "layout": "IPY_MODEL_69d5be3efd434cf3bc831379102525a5"
          }
        },
        "f5b04591614a45baa61c99fffeaee9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01f6627d81b34cc490be8715a87b6052",
            "placeholder": "​",
            "style": "IPY_MODEL_c682b70874c641b0aeacc875d77f52c5",
            "value": "generation_config.json: 100%"
          }
        },
        "f6475ea2117c4d318099aa428711e207": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6a53b9945064e9386cf4a9b43b6bde0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8e8d1e0b5854ffeabb9cc84f6e1f42e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8e9b59c0852462b9189700249b7f53a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9564b2cfe214f19a97aecab114ebb5a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f97ddf3c70e04b38a5f5dbf8519338bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9c756dcb961421281b5fbda0dcf710c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed2b2dc791264b959471f3611010efd4",
            "placeholder": "​",
            "style": "IPY_MODEL_6e12c540d1eb483ba34688727431d2ff",
            "value": " 188/188 [00:00&lt;00:00, 11.9kB/s]"
          }
        },
        "fa71edfddd2248ae911649a17d52440b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9564b2cfe214f19a97aecab114ebb5a",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fef37280e7f74fcdbec5610d604a0d09",
            "value": 1842767
          }
        },
        "faeff528fddc42618ca7ef6c46df66ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc30da70bac8455ca0763df819834119": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc3a18476f25474d8be79a6e76ea73d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd927eb564434ee1876df34dd55d9155": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff7a85e1262b4ae1a998c63db51467d9",
              "IPY_MODEL_63373723871c4a779f828a072a8a7a1c",
              "IPY_MODEL_1398db51216d4c0694e5c8f5605cf821"
            ],
            "layout": "IPY_MODEL_c8bac774c5864d1babebaa4903acdd68"
          }
        },
        "fef37280e7f74fcdbec5610d604a0d09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ff7a85e1262b4ae1a998c63db51467d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0485883bf3734f76b6d74cfd394a839d",
            "placeholder": "​",
            "style": "IPY_MODEL_7c9798b46b434698b110bdca48e0a679",
            "value": "config.json: 100%"
          }
        },
        "ffad9115768e48f49f7005185556a9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
